epoch 0  training loss: 51.37923812866211
epoch 0  clean testing loss: 49.23060989379883
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise5.00e+00_invop1 ...
  0%|                                                                                      | 9/300000 [00:00<5:24:24, 15.41it/s]
epoch 100  training loss: 5.155135631561279

  0%|                                                                                    | 167/300000 [00:02<1:03:55, 78.17it/s]
epoch 200  training loss: 4.539883136749268
epoch 200  clean testing loss: 0.3042123019695282
epoch 300  training loss: 4.36074161529541

  0%|                                                                                    | 329/300000 [00:04<1:04:01, 78.02it/s]
epoch 400  training loss: 4.21567440032959
epoch 400  clean testing loss: 0.4339764416217804
epoch 500  training loss: 4.044798851013184

  0%|▏                                                                                   | 473/300000 [00:06<1:06:14, 75.35it/s]
epoch 600  training loss: 3.907827615737915

  0%|▏                                                                                   | 625/300000 [00:08<1:08:51, 72.46it/s]
epoch 700  training loss: 3.796070098876953
epoch 700  clean testing loss: 0.7509174346923828
epoch 800  training loss: 3.6431431770324707

  0%|▏                                                                                   | 769/300000 [00:10<1:09:44, 71.51it/s]
epoch 900  training loss: 3.4843344688415527

  0%|▎                                                                                   | 913/300000 [00:12<1:09:17, 71.93it/s]
epoch 1000  training loss: 3.3434031009674072
epoch 1000  clean testing loss: 1.2016792297363281

  0%|▎                                                                                  | 1045/300000 [00:14<1:15:49, 65.71it/s]
epoch 1100  training loss: 3.278047561645508

  0%|▎                                                                                  | 1178/300000 [00:16<1:15:47, 65.72it/s]
epoch 1200  training loss: 3.186239242553711
epoch 1200  clean testing loss: 1.651837706565857
epoch 1300  training loss: 3.1665992736816406

  0%|▎                                                                                  | 1311/300000 [00:18<1:15:20, 66.07it/s]
epoch 1400  training loss: 2.9692299365997314


  1%|▍                                                                                  | 1634/300000 [00:23<1:16:31, 64.98it/s]
epoch 1500  training loss: 2.9099230766296387
epoch 1500  clean testing loss: 1.9076566696166992
epoch 1600  training loss: 2.7518911361694336

  1%|▍                                                                                  | 1767/300000 [00:25<1:16:18, 65.14it/s]
epoch 1700  training loss: 2.6584296226501465

  1%|▌                                                                                  | 1893/300000 [00:27<1:16:06, 65.28it/s]
epoch 1800  training loss: 2.6468114852905273

  1%|▌                                                                                  | 2026/300000 [00:29<1:17:03, 64.45it/s]
epoch 1900  training loss: 2.546147346496582
epoch 1900  clean testing loss: 2.52465558052063
epoch 2000  training loss: 2.3734233379364014
epoch 2000  clean testing loss: 2.578646421432495
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise5.00e+00_invop1 ...
epoch 2100  training loss: 2.4586119651794434

  1%|▌                                                                                  | 2159/300000 [00:31<1:16:13, 65.12it/s]
epoch 2200  training loss: 2.9104697704315186

  1%|▋                                                                                  | 2292/300000 [00:33<1:15:55, 65.34it/s]
epoch 2300  training loss: 2.7632088661193848
epoch 2300  clean testing loss: 2.081913948059082
epoch 2400  training loss: 2.5598623752593994


  1%|▋                                                                                  | 2551/300000 [00:37<1:14:24, 66.62it/s]
epoch 2500  training loss: 2.4142744541168213

  1%|▋                                                                                  | 2684/300000 [00:39<1:15:17, 65.82it/s]
epoch 2600  training loss: 2.246049404144287

  1%|▊                                                                                  | 2817/300000 [00:41<1:14:56, 66.09it/s]
epoch 2700  training loss: 3.0394392013549805
epoch 2700  clean testing loss: 2.141339063644409
epoch 2800  training loss: 2.5777153968811035

  1%|▊                                                                                  | 2950/300000 [00:43<1:16:32, 64.68it/s]
epoch 2900  training loss: 2.345676898956299
epoch 2900  clean testing loss: 2.4550046920776367
epoch 3000  training loss: 2.316831111907959
epoch 3000  clean testing loss: 2.831904411315918


  1%|▉                                                                                  | 3216/300000 [00:47<1:14:29, 66.40it/s]
epoch 3100  training loss: 2.3374907970428467
epoch 3100  clean testing loss: 3.2523412704467773
epoch 3200  training loss: 2.151186227798462
epoch 3200  clean testing loss: 3.230544090270996
epoch 3300  training loss: 2.0814096927642822

  1%|▉                                                                                  | 3349/300000 [00:49<1:14:59, 65.93it/s]
epoch 3400  training loss: 2.0886361598968506

  1%|▉                                                                                  | 3482/300000 [00:51<1:15:06, 65.80it/s]
epoch 3500  training loss: 1.924485683441162
epoch 3500  clean testing loss: 3.4273452758789062
epoch 3600  training loss: 2.0162386894226074

  1%|█                                                                                  | 3615/300000 [00:53<1:15:47, 65.18it/s]
epoch 3700  training loss: 2.179309606552124


  1%|█                                                                                  | 3874/300000 [00:57<1:14:54, 65.89it/s]
epoch 3800  training loss: 2.032407760620117
epoch 3800  clean testing loss: 3.3411989212036133
epoch 3900  training loss: 1.9140599966049194
epoch 3900  clean testing loss: 3.178159236907959
epoch 4000  training loss: 1.907123327255249
epoch 4000  clean testing loss: 3.2716856002807617
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise5.00e+00_invop1 ...
epoch 4100  training loss: 1.9438531398773193


  1%|█▏                                                                                 | 4246/300000 [01:05<1:15:16, 65.48it/s]
epoch 4200  training loss: 1.8707327842712402

  1%|█▏                                                                                 | 4379/300000 [01:07<1:14:31, 66.12it/s]
epoch 4300  training loss: 2.0910768508911133

  2%|█▏                                                                                 | 4512/300000 [01:09<1:14:40, 65.96it/s]
epoch 4400  training loss: 1.9524798393249512
epoch 4400  clean testing loss: 3.6359055042266846
epoch 4500  training loss: 1.9357247352600098

  2%|█▎                                                                                 | 4645/300000 [01:11<1:14:50, 65.77it/s]
epoch 4600  training loss: 1.8348482847213745

  2%|█▎                                                                                 | 4778/300000 [01:13<1:14:14, 66.28it/s]
epoch 4700  training loss: 1.8882310390472412

  2%|█▎                                                                                 | 4911/300000 [01:15<1:14:14, 66.25it/s]
epoch 4800  training loss: 1.8313692808151245

  2%|█▍                                                                                 | 5037/300000 [01:17<1:14:00, 66.43it/s]
epoch 4900  training loss: 1.8309388160705566
epoch 4900  clean testing loss: 3.9351515769958496
epoch 5000  training loss: 1.7335513830184937
epoch 5000  clean testing loss: 3.9542393684387207

  2%|█▍                                                                                 | 5170/300000 [01:19<1:14:48, 65.68it/s]
epoch 5100  training loss: 1.680589199066162

  2%|█▍                                                                                 | 5303/300000 [01:21<1:14:35, 65.84it/s]
epoch 5200  training loss: 1.6010913848876953

  2%|█▌                                                                                 | 5436/300000 [01:23<1:14:34, 65.83it/s]
epoch 5300  training loss: 1.6706302165985107
epoch 5300  clean testing loss: 4.243347644805908
epoch 5400  training loss: 1.6582167148590088

  2%|█▌                                                                                 | 5569/300000 [01:25<1:13:47, 66.51it/s]
epoch 5500  training loss: 1.6372169256210327

  2%|█▌                                                                                 | 5702/300000 [01:27<1:14:09, 66.14it/s]
epoch 5600  training loss: 1.643334150314331

  2%|█▌                                                                                 | 5835/300000 [01:29<1:13:46, 66.45it/s]
epoch 5700  training loss: 1.6213597059249878
epoch 5700  clean testing loss: 4.3582377433776855
epoch 5800  training loss: 2.3064188957214355

  2%|█▋                                                                                 | 5968/300000 [01:31<1:13:46, 66.42it/s]
epoch 5900  training loss: 2.223280668258667

  2%|█▋                                                                                 | 6094/300000 [01:33<1:14:12, 66.00it/s]
epoch 6000  training loss: 2.095560073852539
epoch 6000  clean testing loss: 3.570249319076538

  2%|█▋                                                                                 | 6227/300000 [01:35<1:14:52, 65.39it/s]
epoch 6100  training loss: 1.8496557474136353
epoch 6100  clean testing loss: 3.578514814376831
epoch 6200  training loss: 1.8698651790618896

  2%|█▊                                                                                 | 6360/300000 [01:37<1:14:38, 65.57it/s]
epoch 6300  training loss: 1.8323001861572266

  2%|█▊                                                                                 | 6493/300000 [01:39<1:14:08, 65.98it/s]
epoch 6400  training loss: 1.849354863166809

  2%|█▊                                                                                 | 6626/300000 [01:41<1:13:44, 66.31it/s]
epoch 6500  training loss: 1.775003433227539
epoch 6500  clean testing loss: 3.8739562034606934
epoch 6600  training loss: 1.7114795446395874

  2%|█▊                                                                                 | 6759/300000 [01:43<1:13:55, 66.11it/s]
epoch 6700  training loss: 1.7021570205688477

  2%|█▉                                                                                 | 6892/300000 [01:45<1:13:32, 66.43it/s]
epoch 6800  training loss: 1.8090835809707642

  2%|█▉                                                                                 | 6997/300000 [01:47<1:12:43, 67.14it/s]
epoch 6900  training loss: 1.7096260786056519
epoch 6900  clean testing loss: 4.240833282470703
epoch 7000  training loss: 1.7880586385726929
epoch 7000  clean testing loss: 4.346490383148193


  2%|██                                                                                 | 7242/300000 [01:51<1:13:50, 66.08it/s]
epoch 7100  training loss: 1.7409907579421997
epoch 7100  clean testing loss: 4.328660011291504
epoch 7200  training loss: 1.7123396396636963

  2%|██                                                                                 | 7380/300000 [01:53<1:13:41, 66.18it/s]
epoch 7300  training loss: 1.676242709159851

  3%|██                                                                                 | 7513/300000 [01:55<1:14:24, 65.51it/s]
epoch 7400  training loss: 1.5835367441177368

  3%|██                                                                                 | 7646/300000 [01:57<1:13:09, 66.61it/s]
epoch 7500  training loss: 1.5644880533218384
epoch 7500  clean testing loss: 4.155388832092285
epoch 7600  training loss: 1.365373969078064

  3%|██▏                                                                                | 7779/300000 [01:59<1:13:40, 66.11it/s]
epoch 7700  training loss: 1.544977068901062

  3%|██▏                                                                                | 7905/300000 [02:01<1:14:37, 65.24it/s]
epoch 7800  training loss: 1.7824946641921997

  3%|██▏                                                                                | 7975/300000 [02:02<1:13:45, 65.99it/s]
epoch 7900  training loss: 1.9915053844451904

  3%|██▏                                                                                | 8094/300000 [02:05<1:16:10, 63.87it/s]
epoch 8000  training loss: 2.102830410003662
epoch 8000  clean testing loss: 3.551668167114258
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise5.00e+00_invop1 ...
epoch 8100  training loss: 1.9087915420532227

  3%|██▎                                                                                | 8227/300000 [02:07<1:12:43, 66.86it/s]
epoch 8200  training loss: 1.721555233001709

  3%|██▎                                                                                | 8367/300000 [02:09<1:13:03, 66.54it/s]
epoch 8300  training loss: 1.8013707399368286

  3%|██▎                                                                                | 8493/300000 [02:11<1:13:48, 65.82it/s]
epoch 8400  training loss: 1.6080442667007446
epoch 8400  clean testing loss: 4.415211200714111
epoch 8500  training loss: 1.5932676792144775

  3%|██▍                                                                                | 8626/300000 [02:13<1:13:26, 66.12it/s]
epoch 8600  training loss: 1.534423828125

  3%|██▍                                                                                | 8759/300000 [02:15<1:13:52, 65.70it/s]
epoch 8700  training loss: 1.4842228889465332

  3%|██▍                                                                                | 8892/300000 [02:17<1:12:58, 66.48it/s]
epoch 8800  training loss: 1.6235042810440063
epoch 8800  clean testing loss: 4.560827255249023
epoch 8900  training loss: 1.6463117599487305

  3%|██▍                                                                                | 9025/300000 [02:19<1:13:32, 65.94it/s]
epoch 9000  training loss: 1.7102385759353638
epoch 9000  clean testing loss: 4.832767963409424

  3%|██▌                                                                                | 9158/300000 [02:21<1:13:35, 65.87it/s]
epoch 9100  training loss: 1.5922801494598389

  3%|██▌                                                                                | 9291/300000 [02:23<1:14:46, 64.80it/s]
epoch 9200  training loss: 1.5068806409835815
epoch 9200  clean testing loss: 4.884652614593506
epoch 9300  training loss: 1.2945724725723267

  3%|██▌                                                                                | 9424/300000 [02:26<1:13:40, 65.74it/s]
epoch 9400  training loss: 1.4811404943466187

  3%|██▋                                                                                | 9557/300000 [02:28<1:13:01, 66.29it/s]
epoch 9500  training loss: 1.297961711883545
epoch 9500  clean testing loss: 5.342813968658447
epoch 9600  training loss: 1.4210532903671265
epoch 9600  clean testing loss: 5.030614376068115
epoch 9700  training loss: 1.5493364334106445
epoch 9700  clean testing loss: 4.97764253616333
epoch 9800  training loss: 1.629804015159607
epoch 9800  clean testing loss: 4.8978753089904785
epoch 9900  training loss: 1.5123425722122192

  3%|██▊                                                                                | 9949/300000 [02:33<1:13:30, 65.76it/s]
epoch 10000  training loss: 1.659485936164856
epoch 10000  clean testing loss: 5.296818733215332

  3%|██▊                                                                               | 10082/300000 [02:36<1:13:02, 66.16it/s]
epoch 10100  training loss: 1.5791667699813843
epoch 10100  clean testing loss: 5.108051776885986
epoch 10200  training loss: 1.6057887077331543

  3%|██▊                                                                               | 10215/300000 [02:38<1:14:17, 65.01it/s]
epoch 10300  training loss: 1.4423571825027466

  3%|██▊                                                                               | 10348/300000 [02:40<1:12:46, 66.34it/s]
epoch 10400  training loss: 1.4807580709457397

  3%|██▊                                                                               | 10481/300000 [02:42<1:12:54, 66.18it/s]
epoch 10500  training loss: 1.4554030895233154
epoch 10500  clean testing loss: 5.229094982147217
epoch 10600  training loss: 1.5777212381362915

  4%|██▉                                                                               | 10614/300000 [02:44<1:13:42, 65.43it/s]
epoch 10700  training loss: 1.3545465469360352

  4%|██▉                                                                               | 10747/300000 [02:46<1:13:22, 65.71it/s]
epoch 10800  training loss: 1.4892266988754272

  4%|██▉                                                                               | 10880/300000 [02:48<1:12:14, 66.70it/s]
epoch 10900  training loss: 1.343379020690918
epoch 10900  clean testing loss: 5.368340492248535
epoch 11000  training loss: 1.6466660499572754
epoch 11000  clean testing loss: 5.033127307891846

  4%|███                                                                               | 11006/300000 [02:50<1:14:19, 64.80it/s]
epoch 11100  training loss: 1.5928083658218384

  4%|███                                                                               | 11139/300000 [02:52<1:13:17, 65.69it/s]
epoch 11200  training loss: 1.326614499092102

  4%|███                                                                               | 11272/300000 [02:54<1:12:22, 66.49it/s]
epoch 11300  training loss: 1.4686528444290161
epoch 11300  clean testing loss: 5.278600692749023
epoch 11400  training loss: 1.3388524055480957

  4%|███                                                                               | 11405/300000 [02:56<1:13:25, 65.51it/s]
epoch 11500  training loss: 1.2152084112167358
epoch 11500  clean testing loss: 5.3285369873046875
epoch 11600  training loss: 1.2687811851501465


  4%|███▏                                                                              | 11776/300000 [03:01<1:12:15, 66.48it/s]
epoch 11700  training loss: 1.5052908658981323
epoch 11700  clean testing loss: 5.176478862762451
epoch 11800  training loss: 1.4574390649795532

  4%|███▎                                                                              | 11902/300000 [03:03<1:13:05, 65.69it/s]
epoch 11900  training loss: 1.732450008392334
epoch 11900  clean testing loss: 4.7588324546813965
epoch 12000  training loss: 2.060535430908203
epoch 12000  clean testing loss: 4.750747203826904

  4%|███▎                                                                              | 12035/300000 [03:05<1:13:28, 65.31it/s]
epoch 12100  training loss: 1.9585953950881958

  4%|███▎                                                                              | 12168/300000 [03:07<1:12:07, 66.52it/s]
epoch 12200  training loss: 1.9391734600067139

  4%|███▎                                                                              | 12301/300000 [03:09<1:12:31, 66.12it/s]
epoch 12300  training loss: 1.6865872144699097
epoch 12300  clean testing loss: 4.751594066619873
epoch 12400  training loss: 1.6867854595184326


  4%|███▍                                                                              | 12567/300000 [03:13<1:12:43, 65.87it/s]
epoch 12500  training loss: 1.5954309701919556

  4%|███▍                                                                              | 12700/300000 [03:15<1:12:41, 65.87it/s]
epoch 12600  training loss: 1.7027724981307983

  4%|███▌                                                                              | 12833/300000 [03:17<1:12:22, 66.12it/s]
epoch 12700  training loss: 1.7212649583816528
epoch 12700  clean testing loss: 5.222232818603516
epoch 12800  training loss: 1.7158788442611694

  4%|███▌                                                                              | 12966/300000 [03:19<1:12:06, 66.34it/s]
epoch 12900  training loss: 1.6147993803024292

  4%|███▌                                                                              | 13099/300000 [03:21<1:12:25, 66.02it/s]
epoch 13000  training loss: 1.7328346967697144
epoch 13000  clean testing loss: 4.963611602783203

  4%|███▌                                                                              | 13232/300000 [03:23<1:12:30, 65.91it/s]
epoch 13100  training loss: 1.5737625360488892
epoch 13100  clean testing loss: 4.981020927429199
epoch 13200  training loss: 1.5871309041976929

  4%|███▋                                                                              | 13358/300000 [03:25<1:11:07, 67.17it/s]
epoch 13300  training loss: 1.7377914190292358

  4%|███▋                                                                              | 13491/300000 [03:27<1:13:29, 64.97it/s]
epoch 13400  training loss: 1.8964321613311768

  5%|███▋                                                                              | 13624/300000 [03:29<1:13:29, 64.94it/s]
epoch 13500  training loss: 1.836137056350708
epoch 13500  clean testing loss: 4.910356521606445
epoch 13600  training loss: 1.8423676490783691

  5%|███▊                                                                              | 13757/300000 [03:31<1:11:48, 66.43it/s]
epoch 13700  training loss: 1.9069126844406128

  5%|███▊                                                                              | 13890/300000 [03:33<1:13:11, 65.15it/s]
epoch 13800  training loss: 1.7034685611724854

  5%|███▊                                                                              | 13997/300000 [03:35<1:07:00, 71.14it/s]
epoch 13900  training loss: 1.7329999208450317
epoch 13900  clean testing loss: 5.137505054473877
epoch 14000  training loss: 1.7634276151657104
epoch 14000  clean testing loss: 4.9854607582092285
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise5.00e+00_invop1 ...
epoch 14100  training loss: 1.6602020263671875


  5%|███▉                                                                              | 14243/300000 [03:48<1:11:23, 66.71it/s]
epoch 14200  training loss: 1.637141466140747

  5%|███▉                                                                              | 14376/300000 [03:50<1:12:59, 65.22it/s]
epoch 14300  training loss: 1.5916634798049927

  5%|███▉                                                                              | 14509/300000 [03:52<1:12:03, 66.03it/s]
epoch 14400  training loss: 1.5825741291046143

  5%|████                                                                              | 14642/300000 [03:54<1:12:16, 65.80it/s]
epoch 14500  training loss: 1.6079388856887817
epoch 14500  clean testing loss: 5.521958827972412
epoch 14600  training loss: 1.6896657943725586

  5%|████                                                                              | 14775/300000 [03:56<1:12:19, 65.72it/s]
epoch 14700  training loss: 1.6145575046539307

  5%|████                                                                              | 14901/300000 [03:58<1:12:10, 65.83it/s]
epoch 14800  training loss: 1.5076419115066528

  5%|████                                                                              | 14999/300000 [04:00<1:11:15, 66.65it/s]
epoch 14900  training loss: 1.4848262071609497
epoch 14900  clean testing loss: 5.600454807281494
epoch 15000  training loss: 1.5430501699447632
epoch 15000  clean testing loss: 5.3822340965271

  5%|████▏                                                                             | 15111/300000 [04:03<1:13:58, 64.19it/s]
epoch 15100  training loss: 1.6068650484085083

  5%|████▏                                                                             | 15244/300000 [04:05<1:11:50, 66.06it/s]
epoch 15200  training loss: 1.5292737483978271

  5%|████▏                                                                             | 15377/300000 [04:07<1:12:23, 65.52it/s]
epoch 15300  training loss: 1.5122466087341309

  5%|████▏                                                                             | 15510/300000 [04:09<1:12:02, 65.82it/s]
epoch 15400  training loss: 1.4615817070007324
epoch 15400  clean testing loss: 4.994194507598877
epoch 15500  training loss: 1.5107934474945068
epoch 15500  clean testing loss: 5.186427116394043
epoch 15600  training loss: 1.51578688621521

  5%|████▎                                                                             | 15643/300000 [04:11<1:12:35, 65.28it/s]
epoch 15700  training loss: 1.61858069896698

  5%|████▎                                                                             | 15776/300000 [04:13<1:11:23, 66.36it/s]
epoch 15800  training loss: 1.4096935987472534
epoch 15800  clean testing loss: 5.337833404541016
epoch 15900  training loss: 1.478751301765442

  5%|████▎                                                                             | 15902/300000 [04:15<1:12:49, 65.02it/s]
epoch 16000  training loss: 1.4568171501159668
epoch 16000  clean testing loss: 5.255998611450195
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise5.00e+00_invop1 ...
epoch 16100  training loss: 1.4851341247558594

  5%|████▍                                                                             | 16120/300000 [04:20<1:12:11, 65.54it/s]
epoch 16200  training loss: 1.4214900732040405

  5%|████▍                                                                             | 16253/300000 [04:22<1:11:34, 66.07it/s]
epoch 16300  training loss: 1.6120760440826416

  5%|████▍                                                                             | 16386/300000 [04:24<1:12:11, 65.48it/s]
epoch 16400  training loss: 1.5730212926864624

  6%|████▌                                                                             | 16519/300000 [04:26<1:11:29, 66.08it/s]
epoch 16500  training loss: 1.4393237829208374
epoch 16500  clean testing loss: 5.23805570602417
epoch 16600  training loss: 1.4118809700012207

  6%|████▌                                                                             | 16652/300000 [04:28<1:10:26, 67.05it/s]
epoch 16700  training loss: 1.4391897916793823

  6%|████▌                                                                             | 16778/300000 [04:30<1:11:44, 65.80it/s]
epoch 16800  training loss: 1.5182584524154663

  6%|████▌                                                                             | 16911/300000 [04:32<1:11:38, 65.86it/s]
epoch 16900  training loss: 1.5132185220718384
epoch 16900  clean testing loss: 5.206629276275635
epoch 17000  training loss: 1.511005163192749
epoch 17000  clean testing loss: 5.306097984313965

  6%|████▋                                                                             | 17044/300000 [04:34<1:12:18, 65.21it/s]
epoch 17100  training loss: 1.4887239933013916

  6%|████▋                                                                             | 17177/300000 [04:36<1:10:58, 66.42it/s]
epoch 17200  training loss: 1.3200279474258423

  6%|████▋                                                                             | 17310/300000 [04:38<1:11:07, 66.24it/s]
epoch 17300  training loss: 1.4162864685058594
epoch 17300  clean testing loss: 5.215601921081543
epoch 17400  training loss: 1.3425043821334839

  6%|████▊                                                                             | 17443/300000 [04:40<1:11:09, 66.18it/s]
epoch 17500  training loss: 1.3673362731933594

  6%|████▊                                                                             | 17576/300000 [04:42<1:10:32, 66.73it/s]
epoch 17600  training loss: 1.3194060325622559

  6%|████▊                                                                             | 17709/300000 [04:44<1:11:16, 66.01it/s]
epoch 17700  training loss: 1.4415138959884644
epoch 17700  clean testing loss: 5.323156356811523
epoch 17800  training loss: 1.329986810684204

  6%|████▉                                                                             | 17842/300000 [04:46<1:11:33, 65.72it/s]
epoch 17900  training loss: 1.402732491493225

  6%|████▉                                                                             | 17975/300000 [04:49<1:11:24, 65.83it/s]
epoch 18000  training loss: 1.411618709564209
epoch 18000  clean testing loss: 5.353202819824219

  6%|████▉                                                                             | 18108/300000 [04:51<1:10:27, 66.68it/s]
epoch 18100  training loss: 1.2114052772521973
epoch 18100  clean testing loss: 5.347951412200928
epoch 18200  training loss: 1.2149105072021484

  6%|████▉                                                                             | 18241/300000 [04:53<1:11:34, 65.61it/s]
epoch 18300  training loss: 1.3568170070648193

  6%|█████                                                                             | 18374/300000 [04:55<1:11:05, 66.03it/s]
epoch 18400  training loss: 1.289516568183899

  6%|█████                                                                             | 18500/300000 [04:56<1:10:17, 66.75it/s]
epoch 18500  training loss: 1.3254492282867432
epoch 18500  clean testing loss: 5.61356782913208
epoch 18600  training loss: 1.327539324760437

  6%|█████                                                                             | 18633/300000 [04:58<1:10:38, 66.39it/s]
epoch 18700  training loss: 1.265838384628296


  6%|█████▏                                                                            | 19110/300000 [05:11<1:14:51, 62.54it/s]
epoch 18800  training loss: 1.224437952041626
epoch 18800  clean testing loss: 5.602148532867432
epoch 18900  training loss: 1.1948169469833374
epoch 18900  clean testing loss: 5.729459285736084
epoch 19000  training loss: 1.2827560901641846
epoch 19000  clean testing loss: 5.887076377868652
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise5.00e+00_invop1 ...
epoch 19100  training loss: 1.2201555967330933
epoch 19100  clean testing loss: 6.06915807723999
epoch 19200  training loss: 1.0746018886566162

  6%|█████▎                                                                            | 19243/300000 [05:13<1:10:23, 66.48it/s]
epoch 19300  training loss: 1.1348122358322144

  6%|█████▎                                                                            | 19376/300000 [05:15<1:10:32, 66.31it/s]
epoch 19400  training loss: 1.1771247386932373
epoch 19400  clean testing loss: 6.061343193054199
epoch 19500  training loss: 1.0171897411346436

  7%|█████▎                                                                            | 19509/300000 [05:17<1:10:39, 66.17it/s]
epoch 19600  training loss: 1.1579605340957642

  7%|█████▎                                                                            | 19642/300000 [05:19<1:10:39, 66.13it/s]
epoch 19700  training loss: 1.2713453769683838

  7%|█████▍                                                                            | 19775/300000 [05:21<1:09:43, 66.99it/s]
epoch 19800  training loss: 1.2677236795425415
epoch 19800  clean testing loss: 5.808351993560791
epoch 19900  training loss: 1.2029328346252441

  7%|█████▍                                                                            | 19908/300000 [05:23<1:10:27, 66.26it/s]
epoch 20000  training loss: 1.0787023305892944
epoch 20000  clean testing loss: 5.751346111297607

  7%|█████▍                                                                            | 19999/300000 [05:24<1:10:53, 65.84it/s]
epoch 20100  training loss: 1.2304564714431763

  7%|█████▍                                                                            | 20118/300000 [05:27<1:11:19, 65.41it/s]
epoch 20200  training loss: 1.1050468683242798

  7%|█████▌                                                                            | 20251/300000 [05:29<1:10:25, 66.21it/s]
epoch 20300  training loss: 1.318324089050293

  7%|█████▌                                                                            | 20384/300000 [05:31<1:10:21, 66.23it/s]
epoch 20400  training loss: 1.1392720937728882
epoch 20400  clean testing loss: 5.487464427947998
epoch 20500  training loss: 1.155992865562439

  7%|█████▌                                                                            | 20517/300000 [05:33<1:10:30, 66.07it/s]
epoch 20600  training loss: 1.15567147731781

  7%|█████▋                                                                            | 20650/300000 [05:35<1:10:06, 66.40it/s]
epoch 20700  training loss: 1.2628840208053589

  7%|█████▋                                                                            | 20783/300000 [05:37<1:10:31, 65.98it/s]
epoch 20800  training loss: 1.270151972770691
epoch 20800  clean testing loss: 5.617169380187988
epoch 20900  training loss: 1.2595804929733276

  7%|█████▋                                                                            | 20916/300000 [05:39<1:09:46, 66.67it/s]
epoch 21000  training loss: 1.1565383672714233
epoch 21000  clean testing loss: 5.411801338195801

  7%|█████▊                                                                            | 21049/300000 [05:41<1:10:49, 65.64it/s]
epoch 21100  training loss: 1.2778435945510864

  7%|█████▊                                                                            | 21182/300000 [05:43<1:10:26, 65.96it/s]
epoch 21200  training loss: 1.163681149482727
epoch 21200  clean testing loss: 5.022605895996094
epoch 21300  training loss: 1.259822130203247

  7%|█████▊                                                                            | 21315/300000 [05:45<1:10:22, 66.00it/s]
epoch 21400  training loss: 1.2595914602279663

  7%|█████▊                                                                            | 21448/300000 [05:47<1:11:21, 65.06it/s]
epoch 21500  training loss: 1.363526463508606

  7%|█████▉                                                                            | 21581/300000 [05:49<1:09:00, 67.24it/s]
epoch 21600  training loss: 1.4342660903930664
epoch 21600  clean testing loss: 4.772061824798584
epoch 21700  training loss: 1.486761212348938
epoch 21700  clean testing loss: 4.8387861251831055
epoch 21800  training loss: 1.5484343767166138
epoch 21800  clean testing loss: 5.009751796722412
epoch 21900  training loss: 1.5220683813095093
epoch 21900  clean testing loss: 5.030717849731445
epoch 22000  training loss: 1.5096962451934814
epoch 22000  clean testing loss: 4.9858078956604

  7%|██████                                                                            | 22043/300000 [05:56<1:09:30, 66.65it/s]
epoch 22100  training loss: 1.4708962440490723

  7%|██████                                                                            | 22176/300000 [05:58<1:09:26, 66.68it/s]
epoch 22200  training loss: 1.5312869548797607
epoch 22200  clean testing loss: 4.614048480987549
epoch 22300  training loss: 1.6004170179367065

  7%|██████                                                                            | 22309/300000 [06:00<1:11:08, 65.06it/s]
epoch 22400  training loss: 1.5780279636383057

  7%|██████▏                                                                           | 22442/300000 [06:02<1:09:48, 66.27it/s]
epoch 22500  training loss: 1.694141149520874
epoch 22500  clean testing loss: 4.9644856452941895
epoch 22600  training loss: 1.5720292329788208
epoch 22600  clean testing loss: 5.135003089904785
epoch 22700  training loss: 1.6112478971481323
epoch 22700  clean testing loss: 5.072705268859863
epoch 22800  training loss: 1.590240240097046
epoch 22800  clean testing loss: 5.1739583015441895
epoch 22900  training loss: 1.5346300601959229

  8%|██████▎                                                                           | 22939/300000 [06:10<1:09:47, 66.16it/s]
epoch 23000  training loss: 1.701395034790039
epoch 23000  clean testing loss: 4.535100936889648

  8%|██████▎                                                                           | 23072/300000 [06:12<1:09:22, 66.52it/s]
epoch 23100  training loss: 1.6870660781860352
epoch 23100  clean testing loss: 4.934568405151367
epoch 23200  training loss: 1.7001234292984009

  8%|██████▎                                                                           | 23205/300000 [06:14<1:09:48, 66.08it/s]
epoch 23300  training loss: 1.65664803981781

  8%|██████▍                                                                           | 23338/300000 [06:16<1:10:26, 65.46it/s]
epoch 23400  training loss: 1.550256371498108

  8%|██████▍                                                                           | 23471/300000 [06:18<1:09:31, 66.29it/s]
epoch 23500  training loss: 1.5896238088607788
epoch 23500  clean testing loss: 5.182033538818359
epoch 23600  training loss: 1.6766763925552368

  8%|██████▍                                                                           | 23597/300000 [06:20<1:09:40, 66.13it/s]
epoch 23700  training loss: 1.386447787284851

  8%|██████▍                                                                           | 23730/300000 [06:22<1:09:22, 66.36it/s]
epoch 23800  training loss: 1.5948143005371094

  8%|██████▌                                                                           | 23863/300000 [06:24<1:09:58, 65.77it/s]
epoch 23900  training loss: 1.654147982597351

  8%|██████▌                                                                           | 23996/300000 [06:26<1:09:22, 66.31it/s]
epoch 24000  training loss: 1.351544737815857
epoch 24000  clean testing loss: 4.883448600769043
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise5.00e+00_invop1 ...
epoch 24100  training loss: 1.488777756690979

  8%|██████▌                                                                           | 24129/300000 [06:28<1:09:47, 65.88it/s]
epoch 24200  training loss: 1.3508143424987793

  8%|██████▋                                                                           | 24262/300000 [06:30<1:09:47, 65.85it/s]
epoch 24300  training loss: 1.4060994386672974

  8%|██████▋                                                                           | 24395/300000 [06:32<1:09:22, 66.22it/s]
epoch 24400  training loss: 1.402674913406372
epoch 24400  clean testing loss: 5.133233070373535
epoch 24500  training loss: 1.4875959157943726

  8%|██████▋                                                                           | 24528/300000 [06:34<1:09:25, 66.13it/s]
epoch 24600  training loss: 1.482052206993103

  8%|██████▋                                                                           | 24654/300000 [06:36<1:12:19, 63.44it/s]
epoch 24700  training loss: 1.5419201850891113

  8%|██████▊                                                                           | 24787/300000 [06:38<1:09:52, 65.65it/s]
epoch 24800  training loss: 1.507413387298584
epoch 24800  clean testing loss: 5.541955471038818
epoch 24900  training loss: 1.7152245044708252

  8%|██████▊                                                                           | 24920/300000 [06:40<1:10:35, 64.94it/s]
epoch 25000  training loss: 1.4735537767410278
epoch 25000  clean testing loss: 5.44876766204834

  8%|██████▊                                                                           | 25053/300000 [06:42<1:09:31, 65.91it/s]
epoch 25100  training loss: 1.6044683456420898

  8%|██████▉                                                                           | 25179/300000 [06:44<1:09:24, 66.00it/s]
epoch 25200  training loss: 1.6350840330123901
epoch 25200  clean testing loss: 5.359586238861084
epoch 25300  training loss: 1.568136215209961


  8%|██████▉                                                                           | 25445/300000 [06:48<1:09:33, 65.78it/s]
epoch 25400  training loss: 1.602988600730896

  9%|██████▉                                                                           | 25578/300000 [06:50<1:09:18, 66.00it/s]
epoch 25500  training loss: 1.6453402042388916

  9%|███████                                                                           | 25711/300000 [06:52<1:09:04, 66.19it/s]
epoch 25600  training loss: 1.5762004852294922
epoch 25600  clean testing loss: 5.4583740234375
epoch 25700  training loss: 1.354589581489563

  9%|███████                                                                           | 25844/300000 [06:54<1:08:53, 66.32it/s]
epoch 25800  training loss: 1.4989292621612549

  9%|███████                                                                           | 25970/300000 [06:56<1:09:50, 65.40it/s]
epoch 25900  training loss: 1.513112187385559

  9%|███████▏                                                                          | 26068/300000 [06:57<1:09:05, 66.09it/s]
epoch 26000  training loss: 1.4032890796661377
epoch 26000  clean testing loss: 5.512200355529785
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise5.00e+00_invop1 ...
epoch 26100  training loss: 1.466446042060852

  9%|███████▏                                                                          | 26202/300000 [06:59<1:10:13, 64.98it/s]
epoch 26200  training loss: 1.2524628639221191

  9%|███████▏                                                                          | 26335/300000 [07:01<1:08:50, 66.26it/s]
epoch 26300  training loss: 1.4019957780838013

  9%|███████▏                                                                          | 26468/300000 [07:03<1:09:13, 65.85it/s]
epoch 26400  training loss: 1.3557701110839844

  9%|███████▎                                                                          | 26601/300000 [07:06<1:08:40, 66.35it/s]
epoch 26500  training loss: 1.2925548553466797
epoch 26500  clean testing loss: 5.621547222137451
epoch 26600  training loss: 1.3009312152862549

  9%|███████▎                                                                          | 26734/300000 [07:08<1:08:23, 66.59it/s]
epoch 26700  training loss: 1.3520110845565796

  9%|███████▎                                                                          | 26860/300000 [07:09<1:09:31, 65.48it/s]
epoch 26800  training loss: 1.3253560066223145

  9%|███████▍                                                                          | 26993/300000 [07:11<1:08:55, 66.02it/s]
epoch 26900  training loss: 1.3165745735168457
epoch 26900  clean testing loss: 5.7541117668151855
epoch 27000  training loss: 1.3019908666610718
epoch 27000  clean testing loss: 5.785566329956055

  9%|███████▍                                                                          | 27126/300000 [07:13<1:08:19, 66.56it/s]
epoch 27100  training loss: 1.3128273487091064

  9%|███████▍                                                                          | 27259/300000 [07:16<1:08:23, 66.47it/s]
epoch 27200  training loss: 1.156356930732727

  9%|███████▍                                                                          | 27392/300000 [07:18<1:08:37, 66.20it/s]
epoch 27300  training loss: 1.2425364255905151
epoch 27300  clean testing loss: 5.59736442565918
epoch 27400  training loss: 1.175072193145752

  9%|███████▌                                                                          | 27525/300000 [07:20<1:08:17, 66.50it/s]
epoch 27500  training loss: 1.3066291809082031

  9%|███████▌                                                                          | 27658/300000 [07:22<1:08:29, 66.28it/s]
epoch 27600  training loss: 1.1403425931930542

  9%|███████▋                                                                          | 28100/300000 [07:32<1:14:18, 60.99it/s]
epoch 27700  training loss: 1.0974470376968384
epoch 27700  clean testing loss: 5.374120712280273
epoch 27800  training loss: 1.2612391710281372
epoch 27800  clean testing loss: 5.42263126373291
epoch 27900  training loss: 1.258704662322998
epoch 27900  clean testing loss: 5.28117036819458
epoch 28000  training loss: 1.1453025341033936
epoch 28000  clean testing loss: 5.379767894744873
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise5.00e+00_invop1 ...
epoch 28100  training loss: 1.3574334383010864

  9%|███████▋                                                                          | 28233/300000 [07:34<1:09:01, 65.62it/s]
epoch 28200  training loss: 1.2433816194534302

  9%|███████▊                                                                          | 28366/300000 [07:36<1:08:26, 66.14it/s]
epoch 28300  training loss: 1.1259863376617432

  9%|███████▊                                                                          | 28499/300000 [07:38<1:09:50, 64.79it/s]
epoch 28400  training loss: 1.2666834592819214
epoch 28400  clean testing loss: 5.419394016265869
epoch 28500  training loss: 1.2563204765319824

 10%|███████▊                                                                          | 28632/300000 [07:40<1:08:30, 66.01it/s]
epoch 28600  training loss: 1.1588950157165527

 10%|███████▊                                                                          | 28765/300000 [07:42<1:08:44, 65.77it/s]
epoch 28700  training loss: 1.2684295177459717

 10%|███████▉                                                                          | 28898/300000 [07:44<1:08:17, 66.17it/s]
epoch 28800  training loss: 1.3616479635238647
epoch 28800  clean testing loss: 5.544545650482178
epoch 28900  training loss: 1.3097203969955444

 10%|███████▉                                                                          | 29031/300000 [07:46<1:08:00, 66.40it/s]
epoch 29000  training loss: 1.3273316621780396
epoch 29000  clean testing loss: 5.478428840637207

 10%|███████▉                                                                          | 29164/300000 [07:48<1:07:44, 66.63it/s]
epoch 29100  training loss: 1.3201600313186646

 10%|████████                                                                          | 29297/300000 [07:50<1:08:13, 66.13it/s]
epoch 29200  training loss: 1.1463958024978638
epoch 29200  clean testing loss: 5.443789482116699
epoch 29300  training loss: 1.3139660358428955

 10%|████████                                                                          | 29423/300000 [07:52<1:08:41, 65.64it/s]
epoch 29400  training loss: 1.110970377922058

 10%|████████                                                                          | 29556/300000 [07:54<1:07:31, 66.75it/s]
epoch 29500  training loss: 1.2727110385894775

 10%|████████                                                                          | 29689/300000 [07:56<1:08:43, 65.55it/s]
epoch 29600  training loss: 1.1259205341339111
epoch 29600  clean testing loss: 5.757030487060547
epoch 29700  training loss: 0.9869368076324463

 10%|████████▏                                                                         | 29822/300000 [07:58<1:07:23, 66.81it/s]
epoch 29800  training loss: 1.1546231508255005

 10%|████████▏                                                                         | 29955/300000 [08:00<1:08:11, 66.00it/s]
epoch 29900  training loss: 1.196115493774414

 10%|████████▏                                                                         | 30109/300000 [08:11<1:17:03, 58.37it/s]
epoch 30000  training loss: 1.1797922849655151
epoch 30000  clean testing loss: 5.671112537384033
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise5.00e+00_invop1 ...
epoch 30100  training loss: 1.1924302577972412

 10%|████████▎                                                                         | 30242/300000 [08:13<1:07:30, 66.59it/s]
epoch 30200  training loss: 1.261747121810913

 10%|████████▎                                                                         | 30375/300000 [08:15<1:08:46, 65.33it/s]
epoch 30300  training loss: 1.2708799839019775

 10%|████████▎                                                                         | 30508/300000 [08:17<1:08:05, 65.96it/s]
epoch 30400  training loss: 1.3302210569381714
epoch 30400  clean testing loss: 5.88677453994751
epoch 30500  training loss: 1.2032922506332397

 10%|████████▍                                                                         | 30641/300000 [08:19<1:07:57, 66.05it/s]
epoch 30600  training loss: 1.2927571535110474

 10%|████████▍                                                                         | 30774/300000 [08:21<1:07:22, 66.60it/s]
epoch 30700  training loss: 1.1766198873519897

 10%|████████▍                                                                         | 30900/300000 [08:23<1:07:57, 66.00it/s]
epoch 30800  training loss: 1.1591113805770874
epoch 30800  clean testing loss: 5.9259209632873535
epoch 30900  training loss: 1.2435956001281738
epoch 30900  clean testing loss: 5.703738212585449
epoch 31000  training loss: 1.2120327949523926
epoch 31000  clean testing loss: 5.833047389984131
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise5.00e+00_invop1 ...
epoch 31100  training loss: 1.2597020864486694

 10%|████████▌                                                                         | 31124/300000 [08:30<1:08:42, 65.22it/s]
epoch 31200  training loss: 1.1995893716812134

 10%|████████▌                                                                         | 31257/300000 [08:32<1:08:09, 65.72it/s]
epoch 31300  training loss: 1.2620285749435425

 10%|████████▌                                                                         | 31390/300000 [08:34<1:08:04, 65.77it/s]
epoch 31400  training loss: 1.315691590309143
epoch 31400  clean testing loss: 5.521065711975098
epoch 31500  training loss: 1.4651092290878296

 11%|████████▌                                                                         | 31523/300000 [08:36<1:07:36, 66.18it/s]
epoch 31600  training loss: 1.5666027069091797

 11%|████████▋                                                                         | 31656/300000 [08:38<1:07:38, 66.12it/s]
epoch 31700  training loss: 1.4550114870071411

 11%|████████▋                                                                         | 31789/300000 [08:40<1:07:26, 66.28it/s]
epoch 31800  training loss: 1.551915168762207
epoch 31800  clean testing loss: 5.6288957595825195
epoch 31900  training loss: 1.5070152282714844

 11%|████████▋                                                                         | 31922/300000 [08:42<1:07:57, 65.74it/s]
epoch 32000  training loss: 1.6870802640914917
epoch 32000  clean testing loss: 5.61877965927124

 11%|████████▊                                                                         | 32055/300000 [08:44<1:07:22, 66.28it/s]
epoch 32100  training loss: 1.436577320098877

 11%|████████▊                                                                         | 32188/300000 [08:46<1:07:21, 66.27it/s]
epoch 32200  training loss: 1.6993433237075806

 11%|████████▊                                                                         | 32321/300000 [08:48<1:08:06, 65.50it/s]
epoch 32300  training loss: 1.7913269996643066
epoch 32300  clean testing loss: 5.572235584259033
epoch 32400  training loss: 1.7073755264282227

 11%|████████▊                                                                         | 32454/300000 [08:50<1:06:46, 66.77it/s]
epoch 32500  training loss: 1.7045133113861084

 11%|████████▉                                                                         | 32587/300000 [08:52<1:07:04, 66.44it/s]
epoch 32600  training loss: 1.611654281616211

 11%|████████▉                                                                         | 32713/300000 [08:54<1:07:10, 66.31it/s]
epoch 32700  training loss: 1.9182686805725098
epoch 32700  clean testing loss: 5.940236568450928
epoch 32800  training loss: 1.9405006170272827
epoch 32800  clean testing loss: 5.62681245803833
epoch 32900  training loss: 1.620396614074707
epoch 32900  clean testing loss: 5.420493125915527
epoch 33000  training loss: 1.6470754146575928
epoch 33000  clean testing loss: 5.723087310791016
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise5.00e+00_invop1 ...
epoch 33100  training loss: 1.6137720346450806

 11%|█████████                                                                         | 33113/300000 [09:07<1:12:18, 61.51it/s]
epoch 33200  training loss: 1.648553490638733

 11%|█████████                                                                         | 33246/300000 [09:09<1:06:45, 66.59it/s]
epoch 33300  training loss: 1.543147087097168

 11%|█████████                                                                         | 33379/300000 [09:11<1:06:54, 66.42it/s]
epoch 33400  training loss: 1.6791101694107056
epoch 33400  clean testing loss: 5.803886413574219
epoch 33500  training loss: 1.5739418268203735

 11%|█████████▏                                                                        | 33512/300000 [09:13<1:07:06, 66.19it/s]
epoch 33600  training loss: 1.567463755607605

 11%|█████████▏                                                                        | 33645/300000 [09:15<1:06:47, 66.46it/s]
epoch 33700  training loss: 1.7321480512619019

 11%|█████████▏                                                                        | 33778/300000 [09:17<1:07:14, 65.99it/s]
epoch 33800  training loss: 1.6630698442459106
epoch 33800  clean testing loss: 5.628532409667969
epoch 33900  training loss: 1.5471208095550537

 11%|█████████▎                                                                        | 33911/300000 [09:19<1:06:53, 66.29it/s]
epoch 34000  training loss: 1.639273762702942
epoch 34000  clean testing loss: 5.831164360046387

 11%|█████████▎                                                                        | 34044/300000 [09:21<1:07:08, 66.02it/s]
epoch 34100  training loss: 1.6111611127853394

 11%|█████████▎                                                                        | 34177/300000 [09:23<1:06:34, 66.54it/s]
epoch 34200  training loss: 1.59649658203125
epoch 34200  clean testing loss: 6.000885963439941
epoch 34300  training loss: 1.539060115814209

 11%|█████████▍                                                                        | 34310/300000 [09:25<1:07:21, 65.73it/s]
epoch 34400  training loss: 1.541367530822754

 11%|█████████▍                                                                        | 34443/300000 [09:27<1:06:43, 66.33it/s]
epoch 34500  training loss: 1.5107836723327637

 12%|█████████▍                                                                        | 34576/300000 [09:29<1:06:26, 66.58it/s]
epoch 34600  training loss: 1.4985074996948242
epoch 34600  clean testing loss: 6.188043117523193
epoch 34700  training loss: 1.546195387840271

 12%|█████████▍                                                                        | 34709/300000 [09:31<1:07:03, 65.94it/s]
epoch 34800  training loss: 1.4394809007644653

 12%|█████████▌                                                                        | 34835/300000 [09:33<1:07:19, 65.64it/s]
epoch 34900  training loss: 1.5448150634765625

 12%|█████████▌                                                                        | 34968/300000 [09:35<1:05:49, 67.10it/s]
epoch 35000  training loss: 1.5823465585708618
epoch 35000  clean testing loss: 6.274496078491211
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise5.00e+00_invop1 ...
epoch 35100  training loss: 1.6025196313858032

 12%|█████████▌                                                                        | 35130/300000 [09:39<1:09:13, 63.76it/s]
epoch 35200  training loss: 1.549403429031372


 12%|█████████▋                                                                        | 35396/300000 [09:44<1:06:49, 66.00it/s]
epoch 35300  training loss: 1.616768717765808
epoch 35300  clean testing loss: 6.385890960693359
epoch 35400  training loss: 1.455365777015686

 12%|█████████▋                                                                        | 35494/300000 [09:45<1:06:29, 66.31it/s]
epoch 35500  training loss: 1.6205605268478394
epoch 35500  clean testing loss: 6.5731048583984375
epoch 35600  training loss: 1.492417573928833


 12%|█████████▊                                                                        | 35760/300000 [09:49<1:06:45, 65.96it/s]
epoch 35700  training loss: 1.5648951530456543
epoch 35700  clean testing loss: 6.5071516036987305
epoch 35800  training loss: 1.5966721773147583


 12%|█████████▊                                                                        | 36019/300000 [09:53<1:06:37, 66.03it/s]
epoch 35900  training loss: 1.4996682405471802
epoch 35900  clean testing loss: 6.160282611846924
epoch 36000  training loss: 1.50644850730896
epoch 36000  clean testing loss: 6.258953094482422

 12%|█████████▉                                                                        | 36152/300000 [09:55<1:06:55, 65.71it/s]
epoch 36100  training loss: 1.6428093910217285

 12%|█████████▉                                                                        | 36285/300000 [09:57<1:06:15, 66.33it/s]
epoch 36200  training loss: 1.5643337965011597

 12%|█████████▉                                                                        | 36418/300000 [09:59<1:06:16, 66.29it/s]
epoch 36300  training loss: 1.341017723083496
epoch 36300  clean testing loss: 6.3398027420043945
epoch 36400  training loss: 1.5188210010528564

 12%|█████████▉                                                                        | 36551/300000 [10:01<1:06:33, 65.97it/s]
epoch 36500  training loss: 1.4459036588668823

 12%|██████████                                                                        | 36684/300000 [10:03<1:06:21, 66.13it/s]
epoch 36600  training loss: 1.3375669717788696

 12%|██████████▏                                                                       | 37105/300000 [10:10<1:07:15, 65.15it/s]
epoch 36700  training loss: 1.414313793182373
epoch 36700  clean testing loss: 6.268599033355713
epoch 36800  training loss: 1.4396202564239502
epoch 36800  clean testing loss: 6.428696632385254
epoch 36900  training loss: 1.4322071075439453
epoch 36900  clean testing loss: 6.492396831512451
epoch 37000  training loss: 1.3724089860916138
epoch 37000  clean testing loss: 6.392406463623047
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise5.00e+00_invop1 ...
epoch 37100  training loss: 1.3683726787567139

 12%|██████████▏                                                                       | 37238/300000 [10:12<1:06:14, 66.12it/s]
epoch 37200  training loss: 1.4573487043380737

 12%|██████████▏                                                                       | 37371/300000 [10:14<1:06:07, 66.20it/s]
epoch 37300  training loss: 1.4018255472183228

 13%|██████████▎                                                                       | 37504/300000 [10:16<1:06:33, 65.73it/s]
epoch 37400  training loss: 1.3771212100982666
epoch 37400  clean testing loss: 6.362415313720703
epoch 37500  training loss: 1.3274952173233032

 13%|██████████▍                                                                       | 38099/300000 [10:25<1:05:30, 66.64it/s]
epoch 37600  training loss: 1.390354037284851
epoch 37600  clean testing loss: 6.300096035003662
epoch 37700  training loss: 1.46554696559906
epoch 37700  clean testing loss: 6.607798099517822
epoch 37800  training loss: 1.3841383457183838
epoch 37800  clean testing loss: 6.524383544921875
epoch 37900  training loss: 1.3506462574005127
epoch 37900  clean testing loss: 6.561070442199707
epoch 38000  training loss: 1.400592565536499
epoch 38000  clean testing loss: 6.49543571472168
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise5.00e+00_invop1 ...
epoch 38100  training loss: 1.4818921089172363

 13%|██████████▍                                                                       | 38232/300000 [10:27<1:06:42, 65.40it/s]
epoch 38200  training loss: 1.2613357305526733

 13%|██████████▍                                                                       | 38365/300000 [10:29<1:05:31, 66.55it/s]
epoch 38300  training loss: 1.3657974004745483

 13%|██████████▌                                                                       | 38498/300000 [10:31<1:05:42, 66.32it/s]
epoch 38400  training loss: 1.370504379272461

 13%|██████████▌                                                                       | 38631/300000 [10:33<1:05:21, 66.65it/s]
epoch 38500  training loss: 1.4450664520263672
epoch 38500  clean testing loss: 6.295182228088379
epoch 38600  training loss: 1.446362853050232

 13%|██████████▌                                                                       | 38764/300000 [10:35<1:05:34, 66.40it/s]
epoch 38700  training loss: 1.2581654787063599

 13%|██████████▋                                                                       | 38897/300000 [10:37<1:05:55, 66.01it/s]
epoch 38800  training loss: 1.3615134954452515

 13%|██████████▋                                                                       | 39107/300000 [10:43<1:09:34, 62.50it/s]
epoch 38900  training loss: 1.3169183731079102
epoch 38900  clean testing loss: 6.467561721801758
epoch 39000  training loss: 1.3898305892944336
epoch 39000  clean testing loss: 6.4429144859313965
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise5.00e+00_invop1 ...
epoch 39100  training loss: 1.353856086730957

 13%|██████████▋                                                                       | 39240/300000 [10:45<1:05:22, 66.49it/s]
epoch 39200  training loss: 1.2973992824554443

 13%|██████████▊                                                                       | 39373/300000 [10:47<1:05:52, 65.94it/s]
epoch 39300  training loss: 1.3329682350158691

 13%|██████████▊                                                                       | 39506/300000 [10:49<1:05:31, 66.25it/s]
epoch 39400  training loss: 1.265559434890747
epoch 39400  clean testing loss: 6.256234169006348
epoch 39500  training loss: 1.364181637763977

 13%|██████████▊                                                                       | 39639/300000 [10:51<1:06:26, 65.30it/s]
epoch 39600  training loss: 1.3941500186920166

 13%|██████████▊                                                                       | 39772/300000 [10:53<1:04:57, 66.76it/s]
epoch 39700  training loss: 1.3094505071640015

 13%|██████████▉                                                                       | 39905/300000 [10:55<1:06:04, 65.61it/s]
epoch 39800  training loss: 1.3903393745422363
epoch 39800  clean testing loss: 6.573637008666992
epoch 39900  training loss: 1.4674460887908936

 13%|██████████▉                                                                       | 40038/300000 [10:57<1:05:57, 65.68it/s]
epoch 40000  training loss: 1.3656052350997925
epoch 40000  clean testing loss: 6.575441837310791

 13%|██████████▉                                                                       | 40171/300000 [10:59<1:05:12, 66.41it/s]
epoch 40100  training loss: 1.2844746112823486

 13%|███████████                                                                       | 40304/300000 [11:01<1:05:43, 65.85it/s]
epoch 40200  training loss: 1.240614652633667

 13%|███████████                                                                       | 40437/300000 [11:03<1:05:47, 65.76it/s]
epoch 40300  training loss: 1.2710658311843872
epoch 40300  clean testing loss: 6.473682880401611
epoch 40400  training loss: 1.35110342502594

 14%|███████████                                                                       | 40563/300000 [11:05<1:05:21, 66.15it/s]
epoch 40500  training loss: 1.2834049463272095

 14%|███████████                                                                       | 40696/300000 [11:07<1:05:25, 66.06it/s]
epoch 40600  training loss: 1.3338682651519775

 14%|███████████▏                                                                      | 40829/300000 [11:09<1:05:14, 66.21it/s]
epoch 40700  training loss: 1.2826215028762817
epoch 40700  clean testing loss: 6.459852695465088
epoch 40800  training loss: 1.2255100011825562

 14%|███████████▏                                                                      | 40962/300000 [11:11<1:06:12, 65.21it/s]
epoch 40900  training loss: 1.1961077451705933

 14%|███████████▏                                                                      | 41095/300000 [11:13<1:05:06, 66.27it/s]
epoch 41000  training loss: 1.2723346948623657
epoch 41000  clean testing loss: 6.451751232147217

 14%|███████████▎                                                                      | 41228/300000 [11:15<1:05:37, 65.71it/s]
epoch 41100  training loss: 1.2550383806228638
epoch 41100  clean testing loss: 6.598631381988525
epoch 41200  training loss: 1.3748350143432617

 14%|███████████▎                                                                      | 41361/300000 [11:17<1:06:05, 65.22it/s]
epoch 41300  training loss: 1.2309529781341553

 14%|███████████▎                                                                      | 41494/300000 [11:19<1:04:51, 66.43it/s]
epoch 41400  training loss: 1.287000298500061

 14%|███████████▍                                                                      | 41627/300000 [11:21<1:05:16, 65.97it/s]
epoch 41500  training loss: 1.3162641525268555
epoch 41500  clean testing loss: 6.50229549407959
epoch 41600  training loss: 1.3379749059677124

 14%|███████████▍                                                                      | 41760/300000 [11:23<1:05:09, 66.05it/s]
epoch 41700  training loss: 1.314894437789917

 14%|███████████▍                                                                      | 41893/300000 [11:25<1:04:35, 66.60it/s]
epoch 41800  training loss: 1.2250936031341553

 14%|███████████▍                                                                      | 42026/300000 [11:27<1:05:04, 66.07it/s]
epoch 41900  training loss: 1.367302656173706
epoch 41900  clean testing loss: 6.469271659851074
epoch 42000  training loss: 1.3206392526626587
epoch 42000  clean testing loss: 6.368979454040527

 14%|███████████▌                                                                      | 42159/300000 [11:29<1:05:35, 65.52it/s]
epoch 42100  training loss: 1.346500277519226

 14%|███████████▌                                                                      | 42292/300000 [11:31<1:04:45, 66.32it/s]
epoch 42200  training loss: 1.2857762575149536

 14%|███████████▌                                                                      | 42425/300000 [11:33<1:04:49, 66.22it/s]
epoch 42300  training loss: 1.363269567489624
epoch 42300  clean testing loss: 6.491252422332764
epoch 42400  training loss: 1.2991782426834106

 14%|███████████▋                                                                      | 42558/300000 [11:35<1:04:40, 66.34it/s]
epoch 42500  training loss: 1.1968914270401

 14%|███████████▋                                                                      | 42684/300000 [11:37<1:04:44, 66.25it/s]
epoch 42600  training loss: 1.2386919260025024
epoch 42600  clean testing loss: 6.476622581481934
epoch 42700  training loss: 1.2494182586669922
epoch 42700  clean testing loss: 6.303961277008057
epoch 42800  training loss: 1.2123401165008545
epoch 42800  clean testing loss: 6.31764030456543
epoch 42900  training loss: 1.188416600227356
epoch 42900  clean testing loss: 6.242415428161621
epoch 43000  training loss: 1.1988872289657593
epoch 43000  clean testing loss: 6.24004602432251

 14%|███████████▊                                                                      | 43048/300000 [11:43<1:04:30, 66.38it/s]
epoch 43100  training loss: 1.2629544734954834

 14%|███████████▊                                                                      | 43181/300000 [11:45<1:05:16, 65.58it/s]
epoch 43200  training loss: 1.1340476274490356
epoch 43200  clean testing loss: 6.3707966804504395
epoch 43300  training loss: 1.3414993286132812

 14%|███████████▊                                                                      | 43314/300000 [11:47<1:05:23, 65.43it/s]
epoch 43400  training loss: 1.2249791622161865

 14%|███████████▉                                                                      | 43447/300000 [11:49<1:04:31, 66.26it/s]
epoch 43500  training loss: 1.3357014656066895

 15%|███████████▉                                                                      | 43580/300000 [11:51<1:04:46, 65.97it/s]
epoch 43600  training loss: 1.1724646091461182

 15%|███████████▉                                                                      | 43713/300000 [11:53<1:04:21, 66.37it/s]
epoch 43700  training loss: 1.2341960668563843
epoch 43700  clean testing loss: 6.4332075119018555
epoch 43800  training loss: 1.270043969154358

 15%|███████████▉                                                                      | 43846/300000 [11:55<1:05:31, 65.16it/s]
epoch 43900  training loss: 1.2459096908569336

 15%|████████████                                                                      | 43979/300000 [11:57<1:04:26, 66.22it/s]
epoch 44000  training loss: 1.223084568977356
epoch 44000  clean testing loss: 6.412623882293701

 15%|████████████                                                                      | 44112/300000 [11:59<1:05:17, 65.32it/s]
epoch 44100  training loss: 1.3083429336547852
epoch 44100  clean testing loss: 6.3132853507995605
epoch 44200  training loss: 1.233679175376892

 15%|████████████                                                                      | 44245/300000 [12:01<1:04:30, 66.08it/s]
epoch 44300  training loss: 1.3357890844345093

 15%|████████████▏                                                                     | 44371/300000 [12:03<1:05:07, 65.41it/s]
epoch 44400  training loss: 1.2832200527191162

 15%|████████████▏                                                                     | 44504/300000 [12:05<1:05:06, 65.40it/s]
epoch 44500  training loss: 1.1476253271102905
epoch 44500  clean testing loss: 6.212998867034912
epoch 44600  training loss: 1.1467173099517822

 15%|████████████▏                                                                     | 44637/300000 [12:07<1:04:21, 66.14it/s]
epoch 44700  training loss: 1.1683251857757568

 15%|████████████▏                                                                     | 44770/300000 [12:09<1:03:59, 66.48it/s]
epoch 44800  training loss: 1.2560447454452515
epoch 44800  clean testing loss: 6.3127121925354
epoch 44900  training loss: 1.3645291328430176
epoch 44900  clean testing loss: 6.25026273727417
epoch 45000  training loss: 1.1629693508148193
epoch 45000  clean testing loss: 6.216520309448242

 15%|████████████▎                                                                     | 45071/300000 [12:14<1:04:20, 66.04it/s]
epoch 45100  training loss: 1.229569435119629
epoch 45100  clean testing loss: 6.194733619689941
epoch 45200  training loss: 1.2092783451080322

 15%|████████████▎                                                                     | 45204/300000 [12:16<1:03:41, 66.67it/s]
epoch 45300  training loss: 1.2180145978927612

 15%|████████████▍                                                                     | 45337/300000 [12:18<1:05:09, 65.14it/s]
epoch 45400  training loss: 1.219582200050354

 15%|████████████▍                                                                     | 45470/300000 [12:20<1:04:07, 66.16it/s]
epoch 45500  training loss: 1.2902241945266724
epoch 45500  clean testing loss: 6.222736835479736
epoch 45600  training loss: 1.2390464544296265

 15%|████████████▍                                                                     | 45603/300000 [12:22<1:05:15, 64.97it/s]
epoch 45700  training loss: 1.1609466075897217

 15%|████████████▍                                                                     | 45729/300000 [12:24<1:03:52, 66.34it/s]
epoch 45800  training loss: 1.2327488660812378

 15%|████████████▌                                                                     | 45862/300000 [12:26<1:04:16, 65.90it/s]
epoch 45900  training loss: 1.3241907358169556
epoch 45900  clean testing loss: 6.147757053375244
epoch 46000  training loss: 1.2024695873260498
epoch 46000  clean testing loss: 6.1394219398498535

 15%|████████████▌                                                                     | 45995/300000 [12:28<1:03:46, 66.38it/s]
epoch 46100  training loss: 1.1818406581878662

 15%|████████████▌                                                                     | 46128/300000 [12:30<1:03:26, 66.70it/s]
epoch 46200  training loss: 1.286375641822815

 15%|████████████▋                                                                     | 46261/300000 [12:32<1:04:11, 65.88it/s]
epoch 46300  training loss: 1.2617535591125488
epoch 46300  clean testing loss: 6.056543350219727
epoch 46400  training loss: 1.2347590923309326

 15%|████████████▋                                                                     | 46394/300000 [12:34<1:03:53, 66.16it/s]
epoch 46500  training loss: 1.124093770980835

 16%|████████████▋                                                                     | 46527/300000 [12:36<1:03:43, 66.30it/s]
epoch 46600  training loss: 1.1499526500701904

 16%|████████████▊                                                                     | 46660/300000 [12:38<1:03:35, 66.39it/s]
epoch 46700  training loss: 1.2010382413864136
epoch 46700  clean testing loss: 6.086635112762451
epoch 46800  training loss: 1.1014817953109741

 16%|████████████▊                                                                     | 46793/300000 [12:40<1:03:48, 66.13it/s]
epoch 46900  training loss: 1.161354422569275

 16%|████████████▊                                                                     | 46926/300000 [12:42<1:03:39, 66.26it/s]
epoch 47000  training loss: 1.178375005722046
epoch 47000  clean testing loss: 6.074484825134277

 16%|████████████▊                                                                     | 47059/300000 [12:44<1:04:14, 65.63it/s]
epoch 47100  training loss: 1.2397029399871826
epoch 47100  clean testing loss: 6.073519706726074
epoch 47200  training loss: 1.0915049314498901

 16%|████████████▉                                                                     | 47192/300000 [12:46<1:03:45, 66.09it/s]
epoch 47300  training loss: 1.1273621320724487

 16%|████████████▉                                                                     | 47325/300000 [12:48<1:03:20, 66.48it/s]
epoch 47400  training loss: 1.190419316291809
epoch 47400  clean testing loss: 6.162938594818115
epoch 47500  training loss: 1.1157565116882324
epoch 47500  clean testing loss: 6.1466264724731445
epoch 47600  training loss: 1.1108235120773315
epoch 47600  clean testing loss: 6.165335178375244
epoch 47700  training loss: 1.063798189163208

 16%|█████████████                                                                     | 47787/300000 [12:55<1:04:00, 65.67it/s]
epoch 47800  training loss: 1.1447557210922241
epoch 47800  clean testing loss: 6.135530948638916
epoch 47900  training loss: 1.1494070291519165

 16%|█████████████                                                                     | 47920/300000 [12:57<1:03:17, 66.39it/s]
epoch 48000  training loss: 1.141092300415039
epoch 48000  clean testing loss: 6.245966911315918


 16%|█████████████▏                                                                    | 48179/300000 [13:01<1:03:04, 66.54it/s]
epoch 48100  training loss: 1.1682740449905396

 16%|█████████████▏                                                                    | 48312/300000 [13:03<1:03:36, 65.95it/s]
epoch 48200  training loss: 1.1720471382141113
epoch 48200  clean testing loss: 6.231248378753662
epoch 48300  training loss: 1.1841371059417725

 16%|█████████████▏                                                                    | 48445/300000 [13:05<1:02:44, 66.82it/s]
epoch 48400  training loss: 1.2863034009933472

 16%|█████████████▍                                                                    | 49126/300000 [13:17<1:03:54, 65.42it/s]
epoch 48500  training loss: 1.2652565240859985
epoch 48500  clean testing loss: 6.378697872161865
epoch 48600  training loss: 1.22775399684906
epoch 48600  clean testing loss: 6.337353706359863
epoch 48700  training loss: 1.0874351263046265
epoch 48700  clean testing loss: 6.403903484344482
epoch 48800  training loss: 1.2375500202178955
epoch 48800  clean testing loss: 6.370235443115234
epoch 48900  training loss: 1.2372088432312012
epoch 48900  clean testing loss: 6.373458385467529
epoch 49000  training loss: 1.1978586912155151
epoch 49000  clean testing loss: 6.346025466918945

 16%|█████████████▍                                                                    | 49259/300000 [13:19<1:03:18, 66.00it/s]
epoch 49100  training loss: 1.1909116506576538
epoch 49100  clean testing loss: 6.425621509552002
epoch 49200  training loss: 1.1423147916793823

 16%|█████████████▌                                                                    | 49392/300000 [13:21<1:02:54, 66.39it/s]
epoch 49300  training loss: 1.1054033041000366

 17%|█████████████▌                                                                    | 49525/300000 [13:23<1:02:33, 66.74it/s]
epoch 49400  training loss: 1.2455562353134155

 17%|█████████████▌                                                                    | 49658/300000 [13:25<1:02:27, 66.79it/s]
epoch 49500  training loss: 1.121006727218628
epoch 49500  clean testing loss: 6.435675144195557
epoch 49600  training loss: 1.2532117366790771

 17%|█████████████▌                                                                    | 49791/300000 [13:27<1:03:35, 65.58it/s]
epoch 49700  training loss: 1.094416856765747

 17%|█████████████▋                                                                    | 50085/300000 [13:31<1:03:24, 65.70it/s]
epoch 49800  training loss: 1.2434468269348145
epoch 49800  clean testing loss: 6.448200225830078
epoch 49900  training loss: 1.0706807374954224
epoch 49900  clean testing loss: 6.468424320220947
epoch 50000  training loss: 1.1481282711029053
epoch 50000  clean testing loss: 6.433712005615234

 17%|█████████████▋                                                                    | 50218/300000 [13:33<1:02:45, 66.33it/s]
epoch 50100  training loss: 1.0872161388397217
epoch 50100  clean testing loss: 6.503544330596924
epoch 50200  training loss: 1.1657168865203857

 17%|█████████████▊                                                                    | 50351/300000 [13:35<1:02:24, 66.67it/s]
epoch 50300  training loss: 1.0903130769729614

 17%|█████████████▊                                                                    | 50484/300000 [13:37<1:03:14, 65.76it/s]
epoch 50400  training loss: 1.1943843364715576

 17%|█████████████▊                                                                    | 50617/300000 [13:39<1:02:38, 66.35it/s]
epoch 50500  training loss: 1.1628671884536743
epoch 50500  clean testing loss: 6.384926795959473
epoch 50600  training loss: 1.0995051860809326

 17%|█████████████▊                                                                    | 50750/300000 [13:41<1:02:39, 66.30it/s]
epoch 50700  training loss: 1.1412147283554077

 17%|█████████████▉                                                                    | 50883/300000 [13:43<1:02:57, 65.95it/s]
epoch 50800  training loss: 1.1390255689620972

 17%|█████████████▉                                                                    | 51016/300000 [13:45<1:02:35, 66.31it/s]
epoch 50900  training loss: 1.1540042161941528
epoch 50900  clean testing loss: 6.489287376403809
epoch 51000  training loss: 1.0302402973175049
epoch 51000  clean testing loss: 6.482965469360352

 17%|█████████████▉                                                                    | 51142/300000 [13:47<1:02:40, 66.17it/s]
epoch 51100  training loss: 1.142513394355774

 17%|██████████████                                                                    | 51275/300000 [13:49<1:02:25, 66.41it/s]
epoch 51200  training loss: 1.101529598236084
epoch 51200  clean testing loss: 6.507985591888428
epoch 51300  training loss: 1.1176331043243408
epoch 51300  clean testing loss: 6.53789758682251
epoch 51400  training loss: 1.1001299619674683
epoch 51400  clean testing loss: 6.488901615142822
epoch 51500  training loss: 1.11616849899292
epoch 51500  clean testing loss: 6.525348663330078
epoch 51600  training loss: 1.1069908142089844

 17%|██████████████                                                                    | 51646/300000 [13:55<1:01:53, 66.88it/s]
epoch 51700  training loss: 0.9938939809799194

 17%|██████████████▏                                                                   | 51772/300000 [13:57<1:03:17, 65.36it/s]
epoch 51800  training loss: 1.104133129119873
epoch 51800  clean testing loss: 6.475831508636475
epoch 51900  training loss: 1.1317085027694702

 17%|██████████████▏                                                                   | 51905/300000 [13:59<1:02:29, 66.17it/s]
epoch 52000  training loss: 1.0737578868865967
epoch 52000  clean testing loss: 6.500237464904785

 17%|██████████████▏                                                                   | 52038/300000 [14:01<1:01:44, 66.93it/s]
epoch 52100  training loss: 1.0537644624710083


 17%|██████████████▎                                                                   | 52304/300000 [14:05<1:01:52, 66.72it/s]
epoch 52200  training loss: 1.0669934749603271
epoch 52200  clean testing loss: 6.547266006469727
epoch 52300  training loss: 0.9928317070007324

 17%|██████████████▎                                                                   | 52437/300000 [14:07<1:02:09, 66.39it/s]
epoch 52400  training loss: 1.0959923267364502
epoch 52400  clean testing loss: 6.420068264007568
epoch 52500  training loss: 1.1015691757202148

 18%|██████████████▎                                                                   | 52570/300000 [14:09<1:02:23, 66.10it/s]
epoch 52600  training loss: 1.1691956520080566
epoch 52600  clean testing loss: 6.467441558837891
epoch 52700  training loss: 1.0726449489593506

 18%|██████████████▍                                                                   | 52703/300000 [14:11<1:02:37, 65.81it/s]
epoch 52800  training loss: 1.161584496498108

 18%|██████████████▍                                                                   | 52836/300000 [14:13<1:02:36, 65.80it/s]
epoch 52900  training loss: 1.1651990413665771

 18%|██████████████▍                                                                   | 52969/300000 [14:15<1:01:43, 66.70it/s]
epoch 53000  training loss: 1.1107419729232788
epoch 53000  clean testing loss: 6.483412742614746


 18%|██████████████▌                                                                   | 53116/300000 [14:27<1:08:16, 60.26it/s]
epoch 53100  training loss: 1.031185507774353

 18%|██████████████▌                                                                   | 53249/300000 [14:29<1:02:53, 65.38it/s]
epoch 53200  training loss: 1.0550199747085571

 18%|██████████████▌                                                                   | 53382/300000 [14:31<1:02:52, 65.37it/s]
epoch 53300  training loss: 1.0728777647018433

 18%|██████████████▋                                                                   | 53508/300000 [14:33<1:02:27, 65.78it/s]
epoch 53400  training loss: 1.1099112033843994
epoch 53400  clean testing loss: 6.550320625305176
epoch 53500  training loss: 1.2252165079116821

 18%|██████████████▋                                                                   | 53641/300000 [14:35<1:01:55, 66.30it/s]
epoch 53600  training loss: 1.1843154430389404

 18%|██████████████▋                                                                   | 53774/300000 [14:37<1:01:45, 66.46it/s]
epoch 53700  training loss: 1.1702427864074707

 18%|██████████████▋                                                                   | 53907/300000 [14:39<1:01:27, 66.74it/s]
epoch 53800  training loss: 1.198779821395874
epoch 53800  clean testing loss: 6.562283515930176
epoch 53900  training loss: 1.0796263217926025
epoch 53900  clean testing loss: 6.541990756988525
epoch 54000  training loss: 1.159445881843567
epoch 54000  clean testing loss: 6.542580604553223
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise5.00e+00_invop1 ...
epoch 54100  training loss: 1.1318286657333374

 18%|██████████████▊                                                                   | 54125/300000 [14:52<1:06:19, 61.78it/s]
epoch 54200  training loss: 1.165396809577942

 18%|██████████████▊                                                                   | 54258/300000 [14:54<1:02:00, 66.06it/s]
epoch 54300  training loss: 1.2390177249908447

 18%|██████████████▊                                                                   | 54391/300000 [14:56<1:01:21, 66.72it/s]
epoch 54400  training loss: 1.1411863565444946
epoch 54400  clean testing loss: 6.630462169647217
epoch 54500  training loss: 1.1083587408065796

 18%|██████████████▉                                                                   | 54524/300000 [14:58<1:02:47, 65.15it/s]
epoch 54600  training loss: 1.182389497756958

 18%|██████████████▉                                                                   | 54622/300000 [14:59<1:01:24, 66.59it/s]
epoch 54700  training loss: 1.2328054904937744

 18%|██████████████▉                                                                   | 54755/300000 [15:01<1:01:32, 66.41it/s]
epoch 54800  training loss: 1.1430954933166504
epoch 54800  clean testing loss: 6.654293537139893
epoch 54900  training loss: 1.180045247077942

 18%|███████████████                                                                   | 54888/300000 [15:03<1:03:08, 64.70it/s]
epoch 55000  training loss: 1.230170488357544
epoch 55000  clean testing loss: 6.758001327514648

 18%|███████████████                                                                   | 55021/300000 [15:05<1:02:08, 65.70it/s]
epoch 55100  training loss: 1.1731362342834473

 18%|███████████████                                                                   | 55154/300000 [15:07<1:02:20, 65.45it/s]
epoch 55200  training loss: 1.2676880359649658
epoch 55200  clean testing loss: 6.7133612632751465
epoch 55300  training loss: 1.1409051418304443

 18%|███████████████                                                                   | 55287/300000 [15:09<1:01:17, 66.55it/s]
epoch 55400  training loss: 1.2015047073364258

 18%|███████████████▏                                                                  | 55420/300000 [15:11<1:01:33, 66.22it/s]
epoch 55500  training loss: 1.180902361869812

 19%|███████████████▏                                                                  | 55546/300000 [15:13<1:02:21, 65.34it/s]
epoch 55600  training loss: 1.3041648864746094
epoch 55600  clean testing loss: 6.825352668762207
epoch 55700  training loss: 1.317095160484314

 19%|███████████████▏                                                                  | 55679/300000 [15:15<1:01:51, 65.83it/s]
epoch 55800  training loss: 1.1580572128295898

 19%|███████████████▎                                                                  | 55812/300000 [15:17<1:01:28, 66.21it/s]
epoch 55900  training loss: 1.1980972290039062

 19%|███████████████▎                                                                  | 55945/300000 [15:19<1:01:11, 66.48it/s]
epoch 56000  training loss: 1.224005103111267
epoch 56000  clean testing loss: 6.8402204513549805
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise5.00e+00_invop1 ...
epoch 56100  training loss: 1.2592803239822388

 19%|███████████████▎                                                                  | 56078/300000 [15:21<1:01:21, 66.25it/s]
epoch 56200  training loss: 1.2258522510528564

 19%|███████████████▎                                                                  | 56211/300000 [15:23<1:00:52, 66.74it/s]
epoch 56300  training loss: 1.330548644065857

 19%|███████████████▍                                                                  | 56344/300000 [15:25<1:01:23, 66.15it/s]
epoch 56400  training loss: 1.187451720237732

 19%|███████████████▍                                                                  | 56477/300000 [15:27<1:00:57, 66.58it/s]
epoch 56500  training loss: 1.3496371507644653
epoch 56500  clean testing loss: 6.850146770477295
epoch 56600  training loss: 1.155991792678833

 19%|███████████████▍                                                                  | 56610/300000 [15:29<1:00:45, 66.76it/s]
epoch 56700  training loss: 1.0965040922164917

 19%|███████████████▌                                                                  | 56743/300000 [15:31<1:01:02, 66.42it/s]
epoch 56800  training loss: 1.1158840656280518


 19%|███████████████▌                                                                  | 57101/300000 [15:45<1:11:44, 56.44it/s]
epoch 56900  training loss: 1.1515388488769531
epoch 56900  clean testing loss: 6.951302528381348
epoch 57000  training loss: 1.1639567613601685
epoch 57000  clean testing loss: 6.955942630767822
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise5.00e+00_invop1 ...
epoch 57100  training loss: 1.268284797668457

 19%|███████████████▋                                                                  | 57234/300000 [15:47<1:01:34, 65.71it/s]
epoch 57200  training loss: 1.2065950632095337

 19%|███████████████▋                                                                  | 57367/300000 [15:49<1:01:24, 65.85it/s]
epoch 57300  training loss: 1.2191309928894043

 19%|███████████████▋                                                                  | 57500/300000 [15:51<1:00:23, 66.92it/s]
epoch 57400  training loss: 1.3057371377944946
epoch 57400  clean testing loss: 6.980428695678711
epoch 57500  training loss: 1.266822338104248

 19%|███████████████▊                                                                  | 57668/300000 [15:54<1:00:51, 66.36it/s]
epoch 57600  training loss: 1.1621075868606567

 19%|███████████████▊                                                                  | 57801/300000 [15:56<1:01:09, 66.00it/s]
epoch 57700  training loss: 1.1263751983642578

 19%|███████████████▊                                                                  | 57934/300000 [15:58<1:01:14, 65.88it/s]
epoch 57800  training loss: 1.2061183452606201
epoch 57800  clean testing loss: 7.052682876586914
epoch 57900  training loss: 1.3594653606414795

 19%|███████████████▊                                                                  | 57997/300000 [15:59<1:00:51, 66.28it/s]
epoch 58000  training loss: 1.1778450012207031
epoch 58000  clean testing loss: 6.992635726928711
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise5.00e+00_invop1 ...
epoch 58100  training loss: 1.1462750434875488

 19%|███████████████▉                                                                  | 58123/300000 [16:02<1:01:24, 65.64it/s]
epoch 58200  training loss: 1.1006579399108887

 19%|███████████████▉                                                                  | 58256/300000 [16:04<1:01:06, 65.94it/s]
epoch 58300  training loss: 1.173745036125183

 19%|███████████████▉                                                                  | 58389/300000 [16:06<1:00:15, 66.82it/s]
epoch 58400  training loss: 1.2878321409225464
epoch 58400  clean testing loss: 6.931076526641846
epoch 58500  training loss: 1.1157029867172241

 20%|███████████████▉                                                                  | 58515/300000 [16:08<1:01:09, 65.80it/s]
epoch 58600  training loss: 1.208726406097412

 20%|████████████████                                                                  | 58648/300000 [16:10<1:00:14, 66.77it/s]
epoch 58700  training loss: 1.1252752542495728

 20%|████████████████                                                                  | 58781/300000 [16:12<1:01:38, 65.22it/s]
epoch 58800  training loss: 1.191336989402771

 20%|████████████████                                                                  | 58914/300000 [16:14<1:00:57, 65.92it/s]
epoch 58900  training loss: 1.1885014772415161
epoch 58900  clean testing loss: 7.028916835784912
epoch 59000  training loss: 1.1718356609344482
epoch 59000  clean testing loss: 7.007477760314941

 20%|████████████████▏                                                                 | 59047/300000 [16:16<1:01:30, 65.28it/s]
epoch 59100  training loss: 1.227528691291809

 20%|████████████████▏                                                                 | 59180/300000 [16:18<1:00:51, 65.95it/s]
epoch 59200  training loss: 1.271830677986145

 20%|████████████████▏                                                                 | 59313/300000 [16:20<1:01:17, 65.45it/s]
epoch 59300  training loss: 1.1891483068466187
epoch 59300  clean testing loss: 6.791499137878418
epoch 59400  training loss: 1.2134472131729126

 20%|████████████████▏                                                                 | 59446/300000 [16:22<1:00:50, 65.89it/s]
epoch 59500  training loss: 1.2551299333572388

 20%|████████████████▎                                                                 | 59579/300000 [16:24<1:01:03, 65.63it/s]
epoch 59600  training loss: 1.2185137271881104
epoch 59600  clean testing loss: 6.801662445068359
epoch 59700  training loss: 1.297849416732788
epoch 59700  clean testing loss: 6.8373003005981445
epoch 59800  training loss: 1.119202971458435
epoch 59800  clean testing loss: 6.807506084442139
epoch 59900  training loss: 1.0411744117736816
epoch 59900  clean testing loss: 6.910187721252441
epoch 60000  training loss: 1.1718504428863525
epoch 60000  clean testing loss: 6.905658721923828
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise5.00e+00_invop1 ...
epoch 60100  training loss: 1.024327039718628

 20%|████████████████▍                                                                 | 60105/300000 [16:34<1:03:49, 62.64it/s]
epoch 60200  training loss: 1.1968942880630493

 20%|████████████████▍                                                                 | 60238/300000 [16:36<1:00:41, 65.85it/s]
epoch 60300  training loss: 1.077768325805664

 20%|████████████████▍                                                                 | 60364/300000 [16:38<1:01:04, 65.39it/s]
epoch 60400  training loss: 1.1948472261428833
epoch 60400  clean testing loss: 6.904802322387695
epoch 60500  training loss: 1.0953565835952759

 20%|████████████████▉                                                                   | 60497/300000 [16:40<59:57, 66.58it/s]
epoch 60600  training loss: 1.1031655073165894

 20%|████████████████▌                                                                 | 60630/300000 [16:42<1:01:06, 65.29it/s]
epoch 60700  training loss: 1.0875844955444336

 20%|████████████████▌                                                                 | 60763/300000 [16:44<1:00:31, 65.87it/s]
epoch 60800  training loss: 1.1726164817810059
epoch 60800  clean testing loss: 6.921494483947754
epoch 60900  training loss: 1.2063430547714233

 20%|████████████████▋                                                                 | 60896/300000 [16:46<1:00:25, 65.95it/s]
epoch 61000  training loss: 1.15095853805542
epoch 61000  clean testing loss: 6.917891502380371

 20%|████████████████▋                                                                 | 61029/300000 [16:48<1:00:37, 65.70it/s]
epoch 61100  training loss: 1.1135326623916626

 20%|█████████████████▏                                                                  | 61162/300000 [16:50<59:30, 66.89it/s]
epoch 61200  training loss: 1.0635464191436768
epoch 61200  clean testing loss: 6.939360618591309
epoch 61300  training loss: 1.173599362373352

 20%|████████████████▊                                                                 | 61295/300000 [16:53<1:01:20, 64.86it/s]
epoch 61400  training loss: 1.0885246992111206

 20%|████████████████▊                                                                 | 61421/300000 [16:54<1:00:11, 66.06it/s]
epoch 61500  training loss: 1.1231106519699097

 21%|█████████████████▏                                                                  | 61554/300000 [16:56<59:41, 66.58it/s]
epoch 61600  training loss: 1.1295801401138306

 21%|█████████████████▎                                                                  | 61687/300000 [16:58<59:55, 66.28it/s]
epoch 61700  training loss: 1.162700891494751
epoch 61700  clean testing loss: 6.956538677215576
epoch 61800  training loss: 1.1036055088043213
epoch 61800  clean testing loss: 6.882926940917969
epoch 61900  training loss: 1.049106240272522
epoch 61900  clean testing loss: 6.94680118560791
epoch 62000  training loss: 1.1324156522750854
epoch 62000  clean testing loss: 7.007482051849365


 21%|████████████████▉                                                                 | 62184/300000 [17:06<1:00:06, 65.94it/s]
epoch 62100  training loss: 1.157227635383606

 21%|█████████████████▍                                                                  | 62317/300000 [17:08<59:53, 66.15it/s]
epoch 62200  training loss: 1.1500263214111328
epoch 62200  clean testing loss: 7.021860599517822
epoch 62300  training loss: 1.203700065612793

 21%|█████████████████                                                                 | 62450/300000 [17:10<1:00:02, 65.94it/s]
epoch 62400  training loss: 1.1246713399887085

 21%|█████████████████▌                                                                  | 62583/300000 [17:12<59:39, 66.33it/s]
epoch 62500  training loss: 1.0982069969177246

 21%|█████████████████▏                                                                | 62716/300000 [17:14<1:00:02, 65.86it/s]
epoch 62600  training loss: 1.141802430152893
epoch 62600  clean testing loss: 7.036362648010254
epoch 62700  training loss: 1.1289668083190918

 21%|█████████████████▌                                                                  | 62849/300000 [17:16<59:43, 66.19it/s]
epoch 62800  training loss: 1.0060057640075684

 21%|█████████████████▋                                                                  | 62982/300000 [17:18<59:47, 66.07it/s]
epoch 62900  training loss: 1.0802444219589233

 21%|█████████████████▋                                                                  | 63115/300000 [17:20<59:52, 65.94it/s]
epoch 63000  training loss: 1.143998384475708
epoch 63000  clean testing loss: 6.901697635650635
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise5.00e+00_invop1 ...
epoch 63100  training loss: 1.0954943895339966

 21%|█████████████████▋                                                                  | 63248/300000 [17:22<59:49, 65.96it/s]
epoch 63200  training loss: 1.0977966785430908

 21%|█████████████████▋                                                                  | 63381/300000 [17:24<59:24, 66.38it/s]
epoch 63300  training loss: 1.1305015087127686

 21%|█████████████████▊                                                                  | 63514/300000 [17:26<59:55, 65.78it/s]
epoch 63400  training loss: 1.2247308492660522
epoch 63400  clean testing loss: 6.925807952880859
epoch 63500  training loss: 1.2535346746444702

 21%|█████████████████▊                                                                  | 63647/300000 [17:28<59:02, 66.72it/s]
epoch 63600  training loss: 1.1526000499725342

 21%|█████████████████▍                                                                | 63773/300000 [17:30<1:00:03, 65.55it/s]
epoch 63700  training loss: 1.2172887325286865

 21%|█████████████████▉                                                                  | 63906/300000 [17:32<59:37, 65.99it/s]
epoch 63800  training loss: 1.197147011756897
epoch 63800  clean testing loss: 6.851795673370361
epoch 63900  training loss: 1.2624928951263428

 21%|█████████████████▉                                                                  | 64039/300000 [17:34<59:07, 66.51it/s]
epoch 64000  training loss: 1.1408103704452515
epoch 64000  clean testing loss: 6.797593116760254

 21%|█████████████████▌                                                                | 64172/300000 [17:36<1:00:11, 65.30it/s]
epoch 64100  training loss: 1.131193995475769

 21%|██████████████████                                                                  | 64305/300000 [17:38<59:36, 65.90it/s]
epoch 64200  training loss: 1.0383139848709106
epoch 64200  clean testing loss: 6.642210006713867
epoch 64300  training loss: 1.1357449293136597

 21%|██████████████████                                                                  | 64438/300000 [17:40<59:27, 66.02it/s]
epoch 64400  training loss: 1.2451175451278687

 22%|█████████████████▊                                                                | 65113/300000 [17:53<1:00:56, 64.23it/s]
epoch 64500  training loss: 1.186131477355957
epoch 64500  clean testing loss: 6.610788822174072
epoch 64600  training loss: 1.0715245008468628
epoch 64600  clean testing loss: 6.589591026306152
epoch 64700  training loss: 1.2502622604370117
epoch 64700  clean testing loss: 6.629831790924072
epoch 64800  training loss: 1.1732929944992065
epoch 64800  clean testing loss: 6.568710803985596
epoch 64900  training loss: 1.2118884325027466
epoch 64900  clean testing loss: 6.5987982749938965
epoch 65000  training loss: 1.0536391735076904
epoch 65000  clean testing loss: 6.564037322998047
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise5.00e+00_invop1 ...
epoch 65100  training loss: 1.2469385862350464

 22%|█████████████████▊                                                                | 65246/300000 [17:55<1:00:15, 64.93it/s]
epoch 65200  training loss: 1.1995187997817993

 22%|██████████████████▎                                                                 | 65379/300000 [17:57<59:08, 66.12it/s]
epoch 65300  training loss: 1.1855149269104004

 22%|██████████████████▎                                                                 | 65512/300000 [17:59<59:52, 65.27it/s]
epoch 65400  training loss: 1.2453423738479614
epoch 65400  clean testing loss: 6.55026388168335
epoch 65500  training loss: 1.245591640472412

 22%|██████████████████▍                                                                 | 65645/300000 [18:01<58:47, 66.43it/s]
epoch 65600  training loss: 1.068623661994934

 22%|██████████████████▍                                                                 | 65771/300000 [18:03<59:20, 65.78it/s]
epoch 65700  training loss: 1.0980256795883179

 22%|██████████████████▍                                                                 | 65904/300000 [18:05<59:13, 65.89it/s]
epoch 65800  training loss: 1.2737782001495361
epoch 65800  clean testing loss: 6.5344672203063965
epoch 65900  training loss: 1.1245849132537842

 22%|██████████████████                                                                | 66037/300000 [18:07<1:00:09, 64.82it/s]
epoch 66000  training loss: 1.1681770086288452
epoch 66000  clean testing loss: 6.5494465827941895

 22%|██████████████████▌                                                                 | 66170/300000 [18:09<59:09, 65.87it/s]
epoch 66100  training loss: 1.2334057092666626

 22%|██████████████████▌                                                                 | 66303/300000 [18:11<59:13, 65.76it/s]
epoch 66200  training loss: 1.218851923942566
epoch 66200  clean testing loss: 6.604315280914307
epoch 66300  training loss: 1.208789348602295

 22%|██████████████████▌                                                                 | 66436/300000 [18:13<58:53, 66.10it/s]
epoch 66400  training loss: 1.1314440965652466

 22%|██████████████████▋                                                                 | 66569/300000 [18:15<58:59, 65.96it/s]
epoch 66500  training loss: 1.1230090856552124

 22%|██████████████████▋                                                                 | 66702/300000 [18:17<58:45, 66.17it/s]
epoch 66600  training loss: 1.2000271081924438

 22%|██████████████████▋                                                                 | 66835/300000 [18:19<58:38, 66.28it/s]
epoch 66700  training loss: 1.22343909740448
epoch 66700  clean testing loss: 6.549047946929932
epoch 66800  training loss: 1.1368008852005005

 22%|██████████████████▊                                                                 | 66968/300000 [18:21<59:25, 65.36it/s]
epoch 66900  training loss: 1.1927697658538818
epoch 66900  clean testing loss: 6.5045013427734375
epoch 67000  training loss: 1.1234256029129028
epoch 67000  clean testing loss: 6.4654221534729
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise5.00e+00_invop1 ...
epoch 67100  training loss: 1.1356134414672852

 22%|██████████████████▎                                                               | 67121/300000 [18:33<1:04:48, 59.89it/s]
epoch 67200  training loss: 1.1743279695510864


 22%|██████████████████▊                                                                 | 67387/300000 [18:37<58:28, 66.30it/s]
epoch 67300  training loss: 1.1710896492004395

 23%|██████████████████▉                                                                 | 67520/300000 [18:39<58:39, 66.05it/s]
epoch 67400  training loss: 1.21125328540802
epoch 67400  clean testing loss: 6.44132137298584
epoch 67500  training loss: 1.1618998050689697
epoch 67500  clean testing loss: 6.454532623291016
epoch 67600  training loss: 1.1949858665466309
epoch 67600  clean testing loss: 6.450392723083496
epoch 67700  training loss: 1.1729519367218018
epoch 67700  clean testing loss: 6.4716901779174805
epoch 67800  training loss: 1.0898109674453735

 23%|██████████████████▉                                                                 | 67849/300000 [18:44<58:12, 66.48it/s]
epoch 67900  training loss: 1.1339205503463745

 23%|███████████████████                                                                 | 67982/300000 [18:46<57:39, 67.06it/s]
epoch 68000  training loss: 1.1789882183074951
epoch 68000  clean testing loss: 6.409252166748047
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise5.00e+00_invop1 ...
epoch 68100  training loss: 1.0927072763442993

 23%|███████████████████                                                                 | 68115/300000 [18:48<57:48, 66.86it/s]
epoch 68200  training loss: 1.015535831451416

 23%|███████████████████                                                                 | 68248/300000 [18:50<58:13, 66.35it/s]
epoch 68300  training loss: 1.1402002573013306

 23%|███████████████████▏                                                                | 68381/300000 [18:52<58:12, 66.32it/s]
epoch 68400  training loss: 1.2221095561981201
epoch 68400  clean testing loss: 6.2727370262146
epoch 68500  training loss: 1.2359915971755981

 23%|███████████████████▏                                                                | 68514/300000 [18:54<57:55, 66.61it/s]
epoch 68600  training loss: 1.192441701889038

 23%|███████████████████▏                                                                | 68640/300000 [18:56<58:52, 65.50it/s]
epoch 68700  training loss: 1.052241563796997

 23%|███████████████████▎                                                                | 68773/300000 [18:58<58:12, 66.22it/s]
epoch 68800  training loss: 1.2323638200759888

 23%|███████████████████▎                                                                | 68906/300000 [19:00<58:29, 65.85it/s]
epoch 68900  training loss: 1.2047269344329834
epoch 68900  clean testing loss: 6.263176918029785
epoch 69000  training loss: 1.2728455066680908
epoch 69000  clean testing loss: 6.266260147094727

 23%|███████████████████▎                                                                | 69039/300000 [19:02<58:14, 66.09it/s]
epoch 69100  training loss: 1.097726583480835

 23%|███████████████████▎                                                                | 69172/300000 [19:04<58:04, 66.24it/s]
epoch 69200  training loss: 1.2341035604476929

 23%|███████████████████▍                                                                | 69270/300000 [19:05<57:32, 66.83it/s]
epoch 69300  training loss: 1.1925173997879028
epoch 69300  clean testing loss: 6.2553181648254395
epoch 69400  training loss: 1.1088495254516602

 23%|███████████████████▍                                                                | 69403/300000 [19:07<58:29, 65.71it/s]
epoch 69500  training loss: 1.1593577861785889

 23%|███████████████████▍                                                                | 69536/300000 [19:09<58:38, 65.51it/s]
epoch 69600  training loss: 1.2019789218902588
epoch 69600  clean testing loss: 6.246789455413818
epoch 69700  training loss: 1.0863738059997559
epoch 69700  clean testing loss: 6.300179481506348
epoch 69800  training loss: 1.1063843965530396
epoch 69800  clean testing loss: 6.336864471435547
epoch 69900  training loss: 1.040710210800171
epoch 69900  clean testing loss: 6.277465343475342
epoch 70000  training loss: 1.067398190498352
epoch 70000  clean testing loss: 6.180387496948242
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise5.00e+00_invop1 ...
epoch 70100  training loss: 1.0631294250488281

 23%|███████████████████▋                                                                | 70127/300000 [19:19<58:00, 66.04it/s]
epoch 70200  training loss: 1.095251441001892

 23%|███████████████████▋                                                                | 70260/300000 [19:22<57:37, 66.44it/s]
epoch 70300  training loss: 1.152535319328308
epoch 70300  clean testing loss: 6.16618013381958
epoch 70400  training loss: 1.1123731136322021
epoch 70400  clean testing loss: 6.176895618438721
epoch 70500  training loss: 1.2831860780715942

 23%|███████████████████▋                                                                | 70484/300000 [19:25<58:27, 65.44it/s]
epoch 70600  training loss: 1.1139551401138306

 24%|███████████████████▊                                                                | 70617/300000 [19:27<58:12, 65.69it/s]
epoch 70700  training loss: 1.173898696899414

 24%|███████████████████▊                                                                | 70750/300000 [19:29<58:43, 65.06it/s]
epoch 70800  training loss: 1.2494553327560425
epoch 70800  clean testing loss: 6.227255821228027
epoch 70900  training loss: 1.058274269104004


 24%|███████████████████▉                                                                | 70995/300000 [19:33<56:30, 67.54it/s]
epoch 71000  training loss: 1.1206358671188354
epoch 71000  clean testing loss: 6.210087299346924

 24%|███████████████████▍                                                              | 71079/300000 [19:43<1:26:52, 43.92it/s]
epoch 71100  training loss: 1.291335940361023
epoch 71100  clean testing loss: 6.243606090545654
epoch 71200  training loss: 1.0824365615844727


 24%|███████████████████▉                                                                | 71338/300000 [19:47<58:15, 65.41it/s]
epoch 71300  training loss: 1.1391193866729736

 24%|████████████████████                                                                | 71471/300000 [19:49<58:20, 65.28it/s]
epoch 71400  training loss: 1.1640361547470093

 24%|████████████████████                                                                | 71604/300000 [19:51<57:21, 66.36it/s]
epoch 71500  training loss: 1.136759638786316
epoch 71500  clean testing loss: 6.207675933837891
epoch 71600  training loss: 1.0863384008407593

 24%|████████████████████                                                                | 71737/300000 [19:53<57:52, 65.73it/s]
epoch 71700  training loss: 1.2461810111999512

 24%|████████████████████                                                                | 71870/300000 [19:55<57:32, 66.08it/s]
epoch 71800  training loss: 1.2213457822799683

 24%|████████████████████▏                                                               | 71996/300000 [19:57<57:13, 66.41it/s]
epoch 71900  training loss: 1.1045557260513306
epoch 71900  clean testing loss: 6.245367527008057
epoch 72000  training loss: 1.2181873321533203
epoch 72000  clean testing loss: 6.311110496520996

 24%|████████████████████▏                                                               | 72136/300000 [20:01<57:45, 65.75it/s]
epoch 72100  training loss: 1.158084750175476

 24%|████████████████████▏                                                               | 72262/300000 [20:03<58:07, 65.31it/s]
epoch 72200  training loss: 1.1160240173339844

 24%|████████████████████▎                                                               | 72395/300000 [20:05<57:58, 65.43it/s]
epoch 72300  training loss: 1.209718108177185
epoch 72300  clean testing loss: 6.335303783416748
epoch 72400  training loss: 1.2464028596878052
epoch 72400  clean testing loss: 6.342864513397217
epoch 72500  training loss: 1.1670598983764648
epoch 72500  clean testing loss: 6.313643932342529
epoch 72600  training loss: 1.1583542823791504

 24%|████████████████████▎                                                               | 72598/300000 [20:08<57:46, 65.60it/s]
epoch 72700  training loss: 1.1409929990768433

 24%|████████████████████▎                                                               | 72731/300000 [20:10<58:17, 64.99it/s]
epoch 72800  training loss: 1.2155332565307617

 24%|████████████████████▍                                                               | 72864/300000 [20:12<56:54, 66.53it/s]
epoch 72900  training loss: 1.042861819267273
epoch 72900  clean testing loss: 6.416322708129883
epoch 73000  training loss: 1.215014934539795
epoch 73000  clean testing loss: 6.401769161224365

 24%|████████████████████▍                                                               | 72990/300000 [20:14<58:02, 65.19it/s]
epoch 73100  training loss: 1.2129753828048706

 24%|████████████████████▍                                                               | 73123/300000 [20:16<57:35, 65.65it/s]
epoch 73200  training loss: 1.2236413955688477

 24%|████████████████████▌                                                               | 73256/300000 [20:18<57:42, 65.49it/s]
epoch 73300  training loss: 1.2379217147827148
epoch 73300  clean testing loss: 6.452746391296387
epoch 73400  training loss: 1.2707890272140503

 24%|████████████████████▌                                                               | 73389/300000 [20:20<57:02, 66.22it/s]
epoch 73500  training loss: 1.198930025100708

 25%|████████████████████▌                                                               | 73522/300000 [20:22<56:50, 66.40it/s]
epoch 73600  training loss: 1.1151350736618042

 25%|████████████████████▌                                                               | 73655/300000 [20:24<56:43, 66.51it/s]
epoch 73700  training loss: 1.1847424507141113
epoch 73700  clean testing loss: 6.507392406463623
epoch 73800  training loss: 1.1503504514694214

 25%|████████████████████▋                                                               | 73788/300000 [20:26<56:35, 66.63it/s]
epoch 73900  training loss: 1.1156108379364014

 25%|████████████████████▋                                                               | 73921/300000 [20:28<57:16, 65.79it/s]
epoch 74000  training loss: 1.1027991771697998
epoch 74000  clean testing loss: 6.415598392486572



 25%|████████████████████▊                                                               | 74177/300000 [20:42<57:02, 65.99it/s]
epoch 74100  training loss: 1.1241123676300049

 25%|████████████████████▊                                                               | 74310/300000 [20:44<56:50, 66.17it/s]
epoch 74200  training loss: 1.1387072801589966
epoch 74200  clean testing loss: 6.420319557189941
epoch 74300  training loss: 1.1593183279037476

 25%|████████████████████▊                                                               | 74443/300000 [20:46<56:31, 66.50it/s]
epoch 74400  training loss: 1.1166679859161377

 25%|████████████████████▉                                                               | 74576/300000 [20:48<57:06, 65.80it/s]
epoch 74500  training loss: 1.0735344886779785

 25%|████████████████████▉                                                               | 74709/300000 [20:50<56:58, 65.90it/s]
epoch 74600  training loss: 1.0685285329818726
epoch 74600  clean testing loss: 6.369443893432617
epoch 74700  training loss: 1.1628366708755493

 25%|████████████████████▉                                                               | 74842/300000 [20:52<56:43, 66.15it/s]
epoch 74800  training loss: 1.0860676765441895
epoch 74800  clean testing loss: 6.389515399932861
epoch 74900  training loss: 1.2082353830337524
epoch 74900  clean testing loss: 6.35039758682251
epoch 75000  training loss: 1.2697674036026
epoch 75000  clean testing loss: 6.328280448913574
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise5.00e+00_invop1 ...
epoch 75100  training loss: 1.2171155214309692


 25%|█████████████████████                                                               | 75234/300000 [20:59<56:29, 66.32it/s]
epoch 75200  training loss: 1.2075679302215576

 25%|█████████████████████                                                               | 75367/300000 [21:01<57:00, 65.67it/s]
epoch 75300  training loss: 1.196810007095337
epoch 75300  clean testing loss: 6.376003265380859
epoch 75400  training loss: 1.2489161491394043
epoch 75400  clean testing loss: 6.3977274894714355
epoch 75500  training loss: 0.9797110557556152


 25%|█████████████████████▏                                                              | 75633/300000 [21:05<56:43, 65.93it/s]
epoch 75600  training loss: 1.2489490509033203

 25%|█████████████████████▏                                                              | 75766/300000 [21:07<57:16, 65.26it/s]
epoch 75700  training loss: 1.2089890241622925

 25%|█████████████████████▎                                                              | 75899/300000 [21:09<57:10, 65.34it/s]
epoch 75800  training loss: 1.1734665632247925
epoch 75800  clean testing loss: 6.332056045532227
epoch 75900  training loss: 1.204318642616272

 25%|█████████████████████▎                                                              | 76025/300000 [21:11<56:39, 65.88it/s]
epoch 76000  training loss: 1.163158893585205
epoch 76000  clean testing loss: 6.325683116912842

 25%|█████████████████████▎                                                              | 76158/300000 [21:13<55:54, 66.74it/s]
epoch 76100  training loss: 1.1828120946884155

 25%|█████████████████████▎                                                              | 76291/300000 [21:15<56:12, 66.34it/s]
epoch 76200  training loss: 1.186776041984558
epoch 76200  clean testing loss: 6.312079429626465
epoch 76300  training loss: 1.2144492864608765

 25%|█████████████████████▍                                                              | 76424/300000 [21:17<56:10, 66.34it/s]
epoch 76400  training loss: 1.068518877029419

 26%|█████████████████████▍                                                              | 76557/300000 [21:19<56:10, 66.29it/s]
epoch 76500  training loss: 1.1660621166229248

 26%|█████████████████████▍                                                              | 76690/300000 [21:21<56:09, 66.27it/s]
epoch 76600  training loss: 1.2109134197235107

 26%|█████████████████████▌                                                              | 77097/300000 [21:28<59:14, 62.71it/s]
epoch 76700  training loss: 1.1278420686721802
epoch 76700  clean testing loss: 6.378396987915039
epoch 76800  training loss: 1.1791516542434692
epoch 76800  clean testing loss: 6.425293445587158
epoch 76900  training loss: 1.0733948945999146
epoch 76900  clean testing loss: 6.40687370300293
epoch 77000  training loss: 1.093766212463379
epoch 77000  clean testing loss: 6.408265113830566
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise5.00e+00_invop1 ...
epoch 77100  training loss: 1.1458213329315186

 26%|█████████████████████▌                                                              | 77230/300000 [21:30<55:50, 66.49it/s]
epoch 77200  training loss: 1.124492883682251

 26%|█████████████████████▋                                                              | 77363/300000 [21:32<56:28, 65.71it/s]
epoch 77300  training loss: 1.189282774925232

 26%|█████████████████████▋                                                              | 77496/300000 [21:34<55:24, 66.93it/s]
epoch 77400  training loss: 1.1384092569351196
epoch 77400  clean testing loss: 6.427611827850342
epoch 77500  training loss: 1.1270618438720703

 26%|█████████████████████▋                                                              | 77629/300000 [21:36<56:07, 66.04it/s]
epoch 77600  training loss: 1.104743480682373

 26%|█████████████████████▊                                                              | 77762/300000 [21:38<56:04, 66.06it/s]
epoch 77700  training loss: 1.091296911239624
epoch 77700  clean testing loss: 6.401346206665039
epoch 77800  training loss: 1.1230494976043701
epoch 77800  clean testing loss: 6.393799781799316
epoch 77900  training loss: 1.199759602546692
epoch 77900  clean testing loss: 6.408823013305664
epoch 78000  training loss: 1.06203031539917
epoch 78000  clean testing loss: 6.399132251739502

 26%|█████████████████████▉                                                              | 78126/300000 [21:46<57:48, 63.96it/s]
epoch 78100  training loss: 1.1021963357925415
epoch 78100  clean testing loss: 6.383823871612549
epoch 78200  training loss: 1.098464012145996

 26%|█████████████████████▉                                                              | 78259/300000 [21:48<56:21, 65.57it/s]
epoch 78300  training loss: 1.18240487575531


 26%|█████████████████████▉                                                              | 78525/300000 [21:52<55:23, 66.64it/s]
epoch 78400  training loss: 1.1238973140716553

 26%|██████████████████████                                                              | 78658/300000 [21:54<56:40, 65.10it/s]
epoch 78500  training loss: 1.1625410318374634
epoch 78500  clean testing loss: 6.4856390953063965
epoch 78600  training loss: 1.1679116487503052

 26%|██████████████████████                                                              | 78756/300000 [21:56<56:15, 65.54it/s]
epoch 78700  training loss: 1.2865840196609497

 26%|██████████████████████                                                              | 78889/300000 [21:58<55:34, 66.31it/s]
epoch 78800  training loss: 1.1967345476150513

 26%|██████████████████████▏                                                             | 79022/300000 [22:00<56:09, 65.57it/s]
epoch 78900  training loss: 1.1816438436508179
epoch 78900  clean testing loss: 6.4804229736328125
epoch 79000  training loss: 1.2632195949554443
epoch 79000  clean testing loss: 6.507076263427734

 26%|██████████████████████▏                                                             | 79155/300000 [22:02<55:43, 66.04it/s]
epoch 79100  training loss: 1.076183557510376

 26%|██████████████████████▏                                                             | 79288/300000 [22:04<55:26, 66.35it/s]
epoch 79200  training loss: 1.0605363845825195

 26%|██████████████████████▏                                                             | 79421/300000 [22:06<55:34, 66.15it/s]
epoch 79300  training loss: 1.1288377046585083
epoch 79300  clean testing loss: 6.458571910858154
epoch 79400  training loss: 1.1149017810821533
epoch 79400  clean testing loss: 6.481109619140625
epoch 79500  training loss: 1.2176684141159058
epoch 79500  clean testing loss: 6.461345195770264
epoch 79600  training loss: 1.128454566001892
epoch 79600  clean testing loss: 6.476240634918213
epoch 79700  training loss: 1.1993896961212158
epoch 79700  clean testing loss: 6.4647626876831055
epoch 79800  training loss: 1.1876492500305176
epoch 79800  clean testing loss: 6.459040641784668
epoch 79900  training loss: 1.1755774021148682

 27%|██████████████████████▍                                                             | 79946/300000 [22:14<55:28, 66.11it/s]
epoch 80000  training loss: 1.149680733680725
epoch 80000  clean testing loss: 6.466557025909424

 27%|██████████████████████▍                                                             | 80079/300000 [22:16<55:22, 66.19it/s]
epoch 80100  training loss: 1.1683647632598877
epoch 80100  clean testing loss: 6.451061248779297
epoch 80200  training loss: 1.2009221315383911

 27%|██████████████████████▍                                                             | 80212/300000 [22:18<55:35, 65.89it/s]
epoch 80300  training loss: 1.1494630575180054

 27%|██████████████████████▍                                                             | 80345/300000 [22:20<55:25, 66.06it/s]
epoch 80400  training loss: 1.080186367034912

 27%|██████████████████████▌                                                             | 80478/300000 [22:22<55:37, 65.77it/s]
epoch 80500  training loss: 1.1987683773040771
epoch 80500  clean testing loss: 6.496252059936523
epoch 80600  training loss: 1.1234527826309204
epoch 80600  clean testing loss: 6.468792915344238
epoch 80700  training loss: 1.1968464851379395
epoch 80700  clean testing loss: 6.484716892242432
epoch 80800  training loss: 1.0958532094955444
epoch 80800  clean testing loss: 6.534184455871582
epoch 80900  training loss: 1.0954917669296265
epoch 80900  clean testing loss: 6.532361030578613
epoch 81000  training loss: 1.156959891319275
epoch 81000  clean testing loss: 6.532817363739014
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise5.00e+00_invop1 ...
epoch 81100  training loss: 1.1661901473999023

 27%|██████████████████████▋                                                             | 81116/300000 [22:35<56:28, 64.60it/s]
epoch 81200  training loss: 1.210484266281128


 27%|██████████████████████▊                                                             | 81452/300000 [22:40<54:41, 66.60it/s]
epoch 81300  training loss: 1.151051640510559
epoch 81300  clean testing loss: 6.478974342346191
epoch 81400  training loss: 0.9994770288467407
epoch 81400  clean testing loss: 6.5160980224609375
epoch 81500  training loss: 1.2195494174957275


 27%|██████████████████████▉                                                             | 81711/300000 [22:44<56:02, 64.92it/s]
epoch 81600  training loss: 1.058932900428772
epoch 81600  clean testing loss: 6.55998420715332
epoch 81700  training loss: 1.1966737508773804

 27%|██████████████████████▉                                                             | 81844/300000 [22:46<54:56, 66.17it/s]
epoch 81800  training loss: 1.1575782299041748

 27%|██████████████████████▉                                                             | 81977/300000 [22:48<55:09, 65.88it/s]
epoch 81900  training loss: 1.0103685855865479

 27%|██████████████████████▉                                                             | 82110/300000 [22:50<54:59, 66.04it/s]
epoch 82000  training loss: 1.1376954317092896
epoch 82000  clean testing loss: 6.547155857086182
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise5.00e+00_invop1 ...
epoch 82100  training loss: 1.0675270557403564

 27%|███████████████████████                                                             | 82243/300000 [22:52<54:52, 66.14it/s]
epoch 82200  training loss: 1.2025994062423706

 28%|███████████████████████▏                                                            | 82803/300000 [23:00<55:36, 65.09it/s]
epoch 82300  training loss: 0.9990739822387695
epoch 82300  clean testing loss: 6.558493614196777
epoch 82400  training loss: 1.1903550624847412
epoch 82400  clean testing loss: 6.557995319366455
epoch 82500  training loss: 1.1169493198394775
epoch 82500  clean testing loss: 6.538342475891113
epoch 82600  training loss: 1.1809288263320923
epoch 82600  clean testing loss: 6.530219078063965
epoch 82700  training loss: 1.146216869354248

 28%|███████████████████████▏                                                            | 82936/300000 [23:02<54:57, 65.82it/s]
epoch 82800  training loss: 1.1449354887008667
epoch 82800  clean testing loss: 6.583063125610352
epoch 82900  training loss: 1.191869854927063

 28%|███████████████████████▎                                                            | 83062/300000 [23:04<56:12, 64.33it/s]
epoch 83000  training loss: 1.157641887664795
epoch 83000  clean testing loss: 6.550582408905029

 28%|███████████████████████▎                                                            | 83195/300000 [23:06<54:43, 66.03it/s]
epoch 83100  training loss: 1.1711844205856323

 28%|███████████████████████▎                                                            | 83328/300000 [23:08<54:47, 65.91it/s]
epoch 83200  training loss: 1.247132658958435
epoch 83200  clean testing loss: 6.531688213348389
epoch 83300  training loss: 1.0809239149093628

 28%|███████████████████████▎                                                            | 83461/300000 [23:10<54:13, 66.55it/s]
epoch 83400  training loss: 1.1536973714828491

 28%|███████████████████████▍                                                            | 83594/300000 [23:12<54:38, 66.00it/s]
epoch 83500  training loss: 1.0748902559280396

 28%|███████████████████████▍                                                            | 83720/300000 [23:14<55:00, 65.53it/s]
epoch 83600  training loss: 1.063202142715454
epoch 83600  clean testing loss: 6.471253395080566
epoch 83700  training loss: 1.1107012033462524

 28%|███████████████████████▍                                                            | 83853/300000 [23:16<55:04, 65.41it/s]
epoch 83800  training loss: 1.1326251029968262

 28%|███████████████████████▌                                                            | 83986/300000 [23:18<54:26, 66.13it/s]
epoch 83900  training loss: 1.1105183362960815

 28%|███████████████████████▌                                                            | 84119/300000 [23:20<54:25, 66.11it/s]
epoch 84000  training loss: 1.12307870388031
epoch 84000  clean testing loss: 6.536509037017822
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise5.00e+00_invop1 ...
epoch 84100  training loss: 1.0635565519332886

 28%|███████████████████████▌                                                            | 84252/300000 [23:22<54:03, 66.52it/s]
epoch 84200  training loss: 1.139665961265564

 28%|███████████████████████▋                                                            | 84385/300000 [23:24<54:19, 66.15it/s]
epoch 84300  training loss: 1.1399822235107422

 28%|███████████████████████▋                                                            | 84518/300000 [23:26<54:31, 65.87it/s]
epoch 84400  training loss: 1.2049140930175781
epoch 84400  clean testing loss: 6.544915199279785
epoch 84500  training loss: 1.1019119024276733

 28%|███████████████████████▋                                                            | 84651/300000 [23:28<54:19, 66.07it/s]
epoch 84600  training loss: 1.0869649648666382
epoch 84600  clean testing loss: 6.5242695808410645
epoch 84700  training loss: 1.0640745162963867
epoch 84700  clean testing loss: 6.540857791900635
epoch 84800  training loss: 1.1705964803695679


 28%|███████████████████████▊                                                            | 84945/300000 [23:33<53:54, 66.49it/s]
epoch 84900  training loss: 1.05825936794281

 28%|███████████████████████▊                                                            | 85078/300000 [23:35<54:00, 66.33it/s]
epoch 85000  training loss: 1.1322517395019531
epoch 85000  clean testing loss: 6.597803115844727

 28%|███████████████████████▊                                                            | 85211/300000 [23:37<53:54, 66.40it/s]
epoch 85100  training loss: 1.1499379873275757
epoch 85100  clean testing loss: 6.582752227783203
epoch 85200  training loss: 1.1603350639343262

 28%|███████████████████████▉                                                            | 85344/300000 [23:39<54:02, 66.21it/s]
epoch 85300  training loss: 1.1838011741638184
epoch 85300  clean testing loss: 6.638749122619629
epoch 85400  training loss: 1.0645793676376343
epoch 85400  clean testing loss: 6.535268783569336
epoch 85500  training loss: 1.165921926498413
epoch 85500  clean testing loss: 6.580215930938721
epoch 85600  training loss: 1.0384159088134766
epoch 85600  clean testing loss: 6.604465961456299
epoch 85700  training loss: 1.181715965270996
epoch 85700  clean testing loss: 6.633111953735352
epoch 85800  training loss: 1.1592477560043335
epoch 85800  clean testing loss: 6.545158386230469
epoch 85900  training loss: 1.1295197010040283
epoch 85900  clean testing loss: 6.570202350616455
epoch 86000  training loss: 1.1174143552780151
epoch 86000  clean testing loss: 6.535107135772705

 29%|████████████████████████                                                            | 86044/300000 [23:49<53:46, 66.32it/s]
epoch 86100  training loss: 1.1200000047683716

 29%|████████████████████████▏                                                           | 86170/300000 [23:51<54:12, 65.74it/s]
epoch 86200  training loss: 1.2345561981201172

 29%|████████████████████████▏                                                           | 86303/300000 [23:53<54:10, 65.75it/s]
epoch 86300  training loss: 1.1625568866729736
epoch 86300  clean testing loss: 6.548061847686768
epoch 86400  training loss: 1.0830005407333374

 29%|████████████████████████▏                                                           | 86436/300000 [23:55<54:20, 65.50it/s]
epoch 86500  training loss: 1.188720464706421

 29%|████████████████████████▏                                                           | 86569/300000 [23:57<53:46, 66.15it/s]
epoch 86600  training loss: 1.2142010927200317

 29%|████████████████████████▎                                                           | 86702/300000 [23:59<54:13, 65.56it/s]
epoch 86700  training loss: 1.1748102903366089
epoch 86700  clean testing loss: 6.588733673095703
epoch 86800  training loss: 1.0994024276733398

 29%|████████████████████████▎                                                           | 86835/300000 [24:01<53:16, 66.69it/s]
epoch 86900  training loss: 1.1530641317367554

 29%|████████████████████████▎                                                           | 86968/300000 [24:03<53:10, 66.76it/s]
epoch 87000  training loss: 1.1214829683303833
epoch 87000  clean testing loss: 6.572113990783691

 29%|████████████████████████▎                                                           | 86999/300000 [24:04<48:53, 72.61it/s]
epoch 87100  training loss: 1.2087821960449219

 29%|████████████████████████▍                                                           | 87119/300000 [24:08<54:43, 64.83it/s]
epoch 87200  training loss: 1.1455752849578857


 29%|████████████████████████▍                                                           | 87378/300000 [24:12<55:00, 64.42it/s]
epoch 87300  training loss: 1.1653140783309937

 29%|████████████████████████▌                                                           | 87511/300000 [24:14<54:33, 64.90it/s]
epoch 87400  training loss: 1.0663130283355713
epoch 87400  clean testing loss: 6.666153430938721
epoch 87500  training loss: 1.1438857316970825

 29%|████████████████████████▌                                                           | 87644/300000 [24:16<53:26, 66.22it/s]
epoch 87600  training loss: 1.2103179693222046

 29%|████████████████████████▌                                                           | 87840/300000 [24:19<53:27, 66.14it/s]
epoch 87700  training loss: 1.1665217876434326
epoch 87700  clean testing loss: 6.6207404136657715
epoch 87800  training loss: 1.064630389213562

 29%|████████████████████████▋                                                           | 87973/300000 [24:21<53:29, 66.07it/s]
epoch 87900  training loss: 1.2403318881988525

 29%|████████████████████████                                                          | 88050/300000 [24:23<1:03:27, 55.66it/s]
epoch 88000  training loss: 1.1388217210769653
epoch 88000  clean testing loss: 6.679998397827148
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise5.00e+00_invop1 ...
epoch 88100  training loss: 1.0857635736465454

 29%|████████████████████████▋                                                           | 88183/300000 [24:25<53:42, 65.73it/s]
epoch 88200  training loss: 1.0679348707199097

 29%|████████████████████████▋                                                           | 88316/300000 [24:28<53:06, 66.43it/s]
epoch 88300  training loss: 1.1630711555480957

 29%|████████████████████████▊                                                           | 88449/300000 [24:30<53:04, 66.43it/s]
epoch 88400  training loss: 1.1073436737060547
epoch 88400  clean testing loss: 6.662395000457764
epoch 88500  training loss: 1.1380640268325806

 30%|████████████████████████▊                                                           | 88582/300000 [24:32<53:07, 66.33it/s]
epoch 88600  training loss: 1.1769999265670776

 30%|████████████████████████▊                                                           | 88680/300000 [24:33<52:58, 66.49it/s]
epoch 88700  training loss: 1.185791015625
epoch 88700  clean testing loss: 6.665754318237305
epoch 88800  training loss: 1.062845230102539
epoch 88800  clean testing loss: 6.661362648010254
epoch 88900  training loss: 1.0390174388885498
epoch 88900  clean testing loss: 6.647982597351074
epoch 89000  training loss: 1.0298230648040771
epoch 89000  clean testing loss: 6.685013771057129
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise5.00e+00_invop1 ...
epoch 89100  training loss: 1.1027756929397583

 30%|████████████████████████▉                                                           | 89114/300000 [24:40<53:27, 65.75it/s]
epoch 89200  training loss: 1.190293788909912

 30%|████████████████████████▉                                                           | 89247/300000 [24:42<53:05, 66.17it/s]
epoch 89300  training loss: 1.1811777353286743


 30%|█████████████████████████                                                           | 89513/300000 [24:46<53:14, 65.89it/s]
epoch 89400  training loss: 1.1167478561401367
epoch 89400  clean testing loss: 6.6370110511779785
epoch 89500  training loss: 1.0839202404022217

 30%|█████████████████████████▏                                                          | 90073/300000 [24:55<52:01, 67.26it/s]
epoch 89600  training loss: 0.9973288774490356
epoch 89600  clean testing loss: 6.696656703948975
epoch 89700  training loss: 1.170763611793518
epoch 89700  clean testing loss: 6.692430019378662
epoch 89800  training loss: 1.258160948753357
epoch 89800  clean testing loss: 6.657209396362305
epoch 89900  training loss: 1.110809087753296
epoch 89900  clean testing loss: 6.641768932342529
epoch 90000  training loss: 1.1290415525436401
epoch 90000  clean testing loss: 6.672995567321777

 30%|█████████████████████████▎                                                          | 90206/300000 [24:57<53:15, 65.64it/s]
epoch 90100  training loss: 1.0861265659332275

 30%|█████████████████████████▎                                                          | 90339/300000 [24:59<52:50, 66.14it/s]
epoch 90200  training loss: 1.1393142938613892
epoch 90200  clean testing loss: 6.668269634246826
epoch 90300  training loss: 1.0773004293441772

 30%|█████████████████████████▎                                                          | 90472/300000 [25:01<52:41, 66.27it/s]
epoch 90400  training loss: 1.1408835649490356

 30%|█████████████████████████▎                                                          | 90605/300000 [25:03<52:37, 66.32it/s]
epoch 90500  training loss: 1.1333435773849487

 30%|█████████████████████████▍                                                          | 90738/300000 [25:05<53:07, 65.66it/s]
epoch 90600  training loss: 1.1064987182617188
epoch 90600  clean testing loss: 6.644112586975098
epoch 90700  training loss: 1.1298364400863647

 30%|█████████████████████████▍                                                          | 90934/300000 [25:08<52:55, 65.83it/s]
epoch 90800  training loss: 1.1245263814926147
epoch 90800  clean testing loss: 6.601744174957275
epoch 90900  training loss: 1.1473509073257446

 30%|█████████████████████████▍                                                          | 91067/300000 [25:10<52:41, 66.09it/s]
epoch 91000  training loss: 1.1366134881973267
epoch 91000  clean testing loss: 6.58479642868042

 30%|█████████████████████████▌                                                          | 91200/300000 [25:12<52:15, 66.58it/s]
epoch 91100  training loss: 1.176519751548767
epoch 91100  clean testing loss: 6.675943374633789
epoch 91200  training loss: 1.1269067525863647

 30%|█████████████████████████▌                                                          | 91333/300000 [25:14<52:27, 66.30it/s]
epoch 91300  training loss: 1.1189122200012207

 30%|█████████████████████████▌                                                          | 91466/300000 [25:16<52:48, 65.81it/s]
epoch 91400  training loss: 1.0869163274765015

 31%|█████████████████████████▋                                                          | 91599/300000 [25:18<52:18, 66.41it/s]
epoch 91500  training loss: 1.0715402364730835
epoch 91500  clean testing loss: 6.606220245361328
epoch 91600  training loss: 1.2450964450836182

 31%|█████████████████████████▋                                                          | 91732/300000 [25:20<52:22, 66.27it/s]
epoch 91700  training loss: 1.170108437538147

 31%|█████████████████████████▋                                                          | 91865/300000 [25:22<52:03, 66.64it/s]
epoch 91800  training loss: 1.104413390159607
epoch 91800  clean testing loss: 6.502444744110107
epoch 91900  training loss: 1.0620901584625244

 31%|█████████████████████████▊                                                          | 91999/300000 [25:24<51:14, 67.65it/s]
epoch 92000  training loss: 1.0780285596847534
epoch 92000  clean testing loss: 6.512440204620361

 31%|█████████████████████████▏                                                        | 92027/300000 [25:26<2:13:50, 25.90it/s]
epoch 92100  training loss: 1.1944568157196045

 31%|█████████████████████████▊                                                          | 92160/300000 [25:28<52:05, 66.50it/s]
epoch 92200  training loss: 1.1409832239151

 31%|█████████████████████████▊                                                          | 92286/300000 [25:30<52:31, 65.91it/s]
epoch 92300  training loss: 1.1586757898330688

 31%|█████████████████████████▉                                                          | 92419/300000 [25:32<52:35, 65.79it/s]
epoch 92400  training loss: 1.2323651313781738
epoch 92400  clean testing loss: 6.553664207458496
epoch 92500  training loss: 1.173348069190979

 31%|█████████████████████████▉                                                          | 92552/300000 [25:34<52:04, 66.39it/s]
epoch 92600  training loss: 1.131751537322998

 31%|█████████████████████████▉                                                          | 92685/300000 [25:36<52:17, 66.07it/s]
epoch 92700  training loss: 1.2326645851135254

 31%|█████████████████████████▉                                                          | 92818/300000 [25:38<52:21, 65.94it/s]
epoch 92800  training loss: 1.2179639339447021
epoch 92800  clean testing loss: 6.456521987915039
epoch 92900  training loss: 1.2065720558166504

 31%|██████████████████████████                                                          | 92951/300000 [25:40<52:18, 65.97it/s]
epoch 93000  training loss: 1.1254541873931885
epoch 93000  clean testing loss: 6.448563575744629

 31%|██████████████████████████                                                          | 93000/300000 [25:40<52:12, 66.08it/s]
epoch 93100  training loss: 1.1459128856658936

 31%|██████████████████████████                                                          | 93133/300000 [25:44<52:55, 65.14it/s]
epoch 93200  training loss: 1.05411958694458

 31%|██████████████████████████                                                          | 93266/300000 [25:46<52:03, 66.18it/s]
epoch 93300  training loss: 1.04252028465271


 31%|██████████████████████████▏                                                         | 93658/300000 [25:52<52:07, 65.97it/s]
epoch 93400  training loss: 1.2005674839019775
epoch 93400  clean testing loss: 6.48093318939209
epoch 93500  training loss: 1.1126738786697388
epoch 93500  clean testing loss: 6.512896537780762
epoch 93600  training loss: 1.15315580368042

 31%|██████████████████████████▎                                                         | 93791/300000 [25:54<52:04, 66.00it/s]
epoch 93700  training loss: 1.108341097831726

 31%|██████████████████████████▎                                                         | 93924/300000 [25:56<51:58, 66.08it/s]
epoch 93800  training loss: 1.104205846786499
epoch 93800  clean testing loss: 6.4915032386779785
epoch 93900  training loss: 1.0823125839233398

 31%|██████████████████████████▎                                                         | 93994/300000 [25:57<51:53, 66.16it/s]
epoch 94000  training loss: 1.1046758890151978
epoch 94000  clean testing loss: 6.361791133880615
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise5.00e+00_invop1 ...
epoch 94100  training loss: 1.0274560451507568

 31%|██████████████████████████▎                                                         | 94113/300000 [26:02<53:49, 63.76it/s]
epoch 94200  training loss: 1.0571216344833374

 31%|██████████████████████████▍                                                         | 94246/300000 [26:04<51:39, 66.39it/s]
epoch 94300  training loss: 1.0636096000671387

 32%|██████████████████████████▌                                                         | 94946/300000 [26:15<51:33, 66.29it/s]
epoch 94400  training loss: 1.1303645372390747
epoch 94400  clean testing loss: 6.411776542663574
epoch 94500  training loss: 1.1169135570526123
epoch 94500  clean testing loss: 6.360105514526367
epoch 94600  training loss: 1.189450979232788
epoch 94600  clean testing loss: 6.488765716552734
epoch 94700  training loss: 1.1377190351486206
epoch 94700  clean testing loss: 6.388277053833008
epoch 94800  training loss: 1.124862551689148
epoch 94800  clean testing loss: 6.49170446395874
epoch 94900  training loss: 1.1109998226165771

 32%|██████████████████████████▌                                                         | 95079/300000 [26:17<51:42, 66.04it/s]
epoch 95000  training loss: 1.0339083671569824
epoch 95000  clean testing loss: 6.387213230133057

 32%|██████████████████████████▋                                                         | 95205/300000 [26:19<51:24, 66.39it/s]
epoch 95100  training loss: 1.101767659187317
epoch 95100  clean testing loss: 6.431575298309326
epoch 95200  training loss: 1.25013267993927

 32%|██████████████████████████▋                                                         | 95338/300000 [26:21<52:01, 65.56it/s]
epoch 95300  training loss: 1.120483160018921

 32%|██████████████████████████▋                                                         | 95471/300000 [26:23<51:29, 66.21it/s]
epoch 95400  training loss: 1.0844324827194214

 32%|██████████████████████████▊                                                         | 95604/300000 [26:25<51:23, 66.29it/s]
epoch 95500  training loss: 1.1643762588500977

 32%|██████████████████████████▊                                                         | 95737/300000 [26:27<51:08, 66.56it/s]
epoch 95600  training loss: 1.0900293588638306
epoch 95600  clean testing loss: 6.437697887420654
epoch 95700  training loss: 1.1460990905761719

 32%|██████████████████████████▊                                                         | 95870/300000 [26:29<51:36, 65.93it/s]
epoch 95800  training loss: 1.0869580507278442

 32%|██████████████████████████▉                                                         | 96003/300000 [26:31<52:45, 64.44it/s]
epoch 95900  training loss: 1.1523244380950928

 32%|██████████████████████████▉                                                         | 96136/300000 [26:33<51:55, 65.43it/s]
epoch 96000  training loss: 1.137439250946045
epoch 96000  clean testing loss: 6.461572647094727
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise5.00e+00_invop1 ...
epoch 96100  training loss: 1.117678165435791

 32%|██████████████████████████▉                                                         | 96269/300000 [26:35<51:17, 66.21it/s]
epoch 96200  training loss: 1.0674747228622437

 32%|██████████████████████████▉                                                         | 96395/300000 [26:37<51:47, 65.53it/s]
epoch 96300  training loss: 1.2176024913787842

 32%|███████████████████████████                                                         | 96528/300000 [26:39<51:57, 65.26it/s]
epoch 96400  training loss: 1.0903767347335815
epoch 96400  clean testing loss: 6.454631805419922
epoch 96500  training loss: 1.1726332902908325

 32%|███████████████████████████                                                         | 96633/300000 [26:41<50:52, 66.62it/s]
epoch 96600  training loss: 1.1914533376693726

 32%|███████████████████████████                                                         | 96766/300000 [26:43<51:08, 66.23it/s]
epoch 96700  training loss: 1.033691644668579

 32%|███████████████████████████▏                                                        | 96892/300000 [26:44<51:18, 65.99it/s]
epoch 96800  training loss: 1.0944194793701172
epoch 96800  clean testing loss: 6.4250969886779785
epoch 96900  training loss: 1.1134575605392456

 32%|███████████████████████████▏                                                        | 97025/300000 [26:46<50:47, 66.60it/s]
epoch 97000  training loss: 1.1451711654663086
epoch 97000  clean testing loss: 6.440338134765625

 32%|███████████████████████████▏                                                        | 97158/300000 [26:48<50:35, 66.82it/s]
epoch 97100  training loss: 1.0762817859649658

 32%|███████████████████████████▏                                                        | 97291/300000 [26:50<50:58, 66.29it/s]
epoch 97200  training loss: 1.1712822914123535
epoch 97200  clean testing loss: 6.484100341796875
epoch 97300  training loss: 1.1680301427841187

 32%|███████████████████████████▎                                                        | 97424/300000 [26:52<51:03, 66.13it/s]
epoch 97400  training loss: 1.1395481824874878

 33%|███████████████████████████▎                                                        | 97557/300000 [26:55<51:24, 65.63it/s]
epoch 97500  training loss: 1.2034844160079956

 33%|███████████████████████████▎                                                        | 97690/300000 [26:57<51:42, 65.22it/s]
epoch 97600  training loss: 1.1989405155181885
epoch 97600  clean testing loss: 6.460465908050537
epoch 97700  training loss: 1.2111812829971313

 33%|███████████████████████████▍                                                        | 97823/300000 [26:59<50:55, 66.17it/s]
epoch 97800  training loss: 1.1617300510406494

 33%|███████████████████████████▍                                                        | 97956/300000 [27:01<50:42, 66.40it/s]
epoch 97900  training loss: 1.136468768119812

 33%|███████████████████████████▍                                                        | 98082/300000 [27:03<50:21, 66.82it/s]
epoch 98000  training loss: 1.1669515371322632
epoch 98000  clean testing loss: 6.498152732849121
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise5.00e+00_invop1 ...
epoch 98100  training loss: 1.0974563360214233

 33%|███████████████████████████▌                                                        | 98215/300000 [27:05<50:38, 66.42it/s]
epoch 98200  training loss: 1.168426275253296

 33%|███████████████████████████▌                                                        | 98348/300000 [27:07<50:43, 66.26it/s]
epoch 98300  training loss: 1.0887776613235474

 33%|███████████████████████████▌                                                        | 98481/300000 [27:09<51:01, 65.82it/s]
epoch 98400  training loss: 1.2504738569259644
epoch 98400  clean testing loss: 6.5982160568237305
epoch 98500  training loss: 1.125665545463562

 33%|███████████████████████████▌                                                        | 98614/300000 [27:11<51:07, 65.65it/s]
epoch 98600  training loss: 1.2882288694381714

 33%|███████████████████████████▋                                                        | 98747/300000 [27:13<51:03, 65.69it/s]
epoch 98700  training loss: 1.1920045614242554
epoch 98700  clean testing loss: 6.565914154052734
epoch 98800  training loss: 1.2109509706497192
epoch 98800  clean testing loss: 6.581711292266846
epoch 98900  training loss: 1.153194785118103
epoch 98900  clean testing loss: 6.621316909790039
epoch 99000  training loss: 1.2472600936889648
epoch 99000  clean testing loss: 6.6103410720825195


 33%|███████████████████████████▊                                                        | 99181/300000 [27:19<50:17, 66.55it/s]
epoch 99100  training loss: 1.1973633766174316

 33%|███████████████████████████▊                                                        | 99314/300000 [27:21<50:12, 66.63it/s]
epoch 99200  training loss: 1.2996662855148315
epoch 99200  clean testing loss: 6.626806735992432
epoch 99300  training loss: 1.1939705610275269

 33%|███████████████████████████▊                                                        | 99447/300000 [27:23<49:50, 67.05it/s]
epoch 99400  training loss: 1.1941919326782227

 33%|███████████████████████████▉                                                        | 99580/300000 [27:25<49:54, 66.93it/s]
epoch 99500  training loss: 1.1599786281585693

 33%|███████████████████████████▉                                                        | 99713/300000 [27:27<49:57, 66.82it/s]
epoch 99600  training loss: 1.2381740808486938
epoch 99600  clean testing loss: 6.637754440307617
epoch 99700  training loss: 1.215421438217163

 33%|███████████████████████████▉                                                        | 99839/300000 [27:29<50:21, 66.24it/s]
epoch 99800  training loss: 1.294541835784912

 33%|███████████████████████████▉                                                        | 99972/300000 [27:31<50:38, 65.84it/s]
epoch 99900  training loss: 1.2195429801940918

 33%|███████████████████████████▉                                                        | 99996/300000 [27:31<46:12, 72.14it/s]
epoch 100000  training loss: 1.1881998777389526
epoch 100000  clean testing loss: 6.572025775909424
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise5.00e+00_invop1 ...
epoch 100100  training loss: 1.2289875745773315

 33%|███████████████████████████                                                      | 100113/300000 [27:53<1:01:46, 53.92it/s]
epoch 100200  training loss: 1.1245845556259155

 33%|███████████████████████████▋                                                       | 100239/300000 [27:55<51:25, 64.73it/s]
epoch 100300  training loss: 1.1863538026809692

 33%|███████████████████████████▊                                                       | 100372/300000 [27:57<52:06, 63.86it/s]
epoch 100400  training loss: 1.1997157335281372
epoch 100400  clean testing loss: 6.5849833488464355
epoch 100500  training loss: 1.2099864482879639

 33%|███████████████████████████▊                                                       | 100498/300000 [27:59<50:15, 66.16it/s]
epoch 100600  training loss: 1.1784850358963013

 34%|███████████████████████████▊                                                       | 100631/300000 [28:01<51:14, 64.85it/s]
epoch 100700  training loss: 1.1550966501235962

 34%|███████████████████████████▉                                                       | 100764/300000 [28:03<49:29, 67.09it/s]
epoch 100800  training loss: 1.2237119674682617
epoch 100800  clean testing loss: 6.602613925933838
epoch 100900  training loss: 1.1314035654067993

 34%|███████████████████████████▉                                                       | 100897/300000 [28:05<50:59, 65.08it/s]
epoch 101000  training loss: 1.1558878421783447
epoch 101000  clean testing loss: 6.603701591491699

 34%|███████████████████████████▉                                                       | 100995/300000 [28:06<50:03, 66.26it/s]
epoch 101100  training loss: 1.2047539949417114

 34%|███████████████████████████▉                                                       | 101128/300000 [28:13<51:41, 64.11it/s]
epoch 101200  training loss: 1.2309894561767578

 34%|████████████████████████████                                                       | 101254/300000 [28:15<50:33, 65.52it/s]
epoch 101300  training loss: 1.1860369443893433

 34%|████████████████████████████                                                       | 101387/300000 [28:17<51:46, 63.93it/s]
epoch 101400  training loss: 1.1742597818374634

 34%|████████████████████████████                                                       | 101520/300000 [28:19<50:21, 65.69it/s]
epoch 101500  training loss: 1.1437783241271973
epoch 101500  clean testing loss: 6.608826637268066
epoch 101600  training loss: 1.2561107873916626

 34%|████████████████████████████                                                       | 101654/300000 [28:21<49:55, 66.21it/s]
epoch 101700  training loss: 1.235736608505249

 34%|████████████████████████████▏                                                      | 101780/300000 [28:23<49:48, 66.33it/s]
epoch 101800  training loss: 1.1807224750518799

 34%|████████████████████████████▏                                                      | 101920/300000 [28:25<50:02, 65.98it/s]
epoch 101900  training loss: 1.1961721181869507
epoch 101900  clean testing loss: 6.619865894317627
epoch 102000  training loss: 1.1379368305206299
epoch 102000  clean testing loss: 6.608460426330566

 34%|████████████████████████████▏                                                      | 102046/300000 [28:27<50:09, 65.79it/s]
epoch 102100  training loss: 1.2211204767227173

 34%|████████████████████████████▎                                                      | 102179/300000 [28:29<49:48, 66.20it/s]
epoch 102200  training loss: 1.2623374462127686

 34%|████████████████████████████▎                                                      | 102312/300000 [28:31<49:39, 66.36it/s]
epoch 102300  training loss: 1.2170521020889282
epoch 102300  clean testing loss: 6.615238189697266
epoch 102400  training loss: 1.1573432683944702

 34%|████████████████████████████▎                                                      | 102445/300000 [28:33<49:51, 66.05it/s]
epoch 102500  training loss: 1.220938801765442

 34%|████████████████████████████▍                                                      | 102578/300000 [28:35<49:45, 66.14it/s]
epoch 102600  training loss: 1.19876229763031
epoch 102600  clean testing loss: 6.621049880981445
epoch 102700  training loss: 1.1182096004486084


 34%|████████████████████████████▍                                                      | 102879/300000 [28:40<49:42, 66.10it/s]
epoch 102800  training loss: 1.2008800506591797

 34%|████████████████████████████▍                                                      | 102998/300000 [28:42<49:26, 66.40it/s]
epoch 102900  training loss: 1.155869960784912
epoch 102900  clean testing loss: 6.606045246124268
epoch 103000  training loss: 1.1869670152664185
epoch 103000  clean testing loss: 6.654842853546143

 34%|████████████████████████████▌                                                      | 103103/300000 [28:45<50:45, 64.66it/s]
epoch 103100  training loss: 1.170295000076294

 34%|████████████████████████████▌                                                      | 103236/300000 [28:47<50:19, 65.15it/s]
epoch 103200  training loss: 1.2389814853668213

 34%|████████████████████████████▌                                                      | 103362/300000 [28:49<49:43, 65.90it/s]
epoch 103300  training loss: 1.089688777923584

 34%|████████████████████████████▋                                                      | 103495/300000 [28:51<49:21, 66.36it/s]
epoch 103400  training loss: 1.0894477367401123
epoch 103400  clean testing loss: 6.641429901123047
epoch 103500  training loss: 1.171006679534912

 35%|████████████████████████████▋                                                      | 103628/300000 [28:53<49:20, 66.33it/s]
epoch 103600  training loss: 1.1175432205200195

 35%|████████████████████████████▋                                                      | 103761/300000 [28:55<49:47, 65.68it/s]
epoch 103700  training loss: 1.1617776155471802

 35%|████████████████████████████▋                                                      | 103894/300000 [28:57<49:40, 65.81it/s]
epoch 103800  training loss: 1.1554921865463257

 35%|████████████████████████████▊                                                      | 103999/300000 [28:59<49:26, 66.07it/s]
epoch 103900  training loss: 1.131219744682312
epoch 103900  clean testing loss: 6.672379970550537
epoch 104000  training loss: 1.1488138437271118
epoch 104000  clean testing loss: 6.682826519012451

 35%|████████████████████████████                                                     | 104062/300000 [29:07<1:50:26, 29.57it/s]
epoch 104100  training loss: 1.0750478506088257

 35%|████████████████████████████▊                                                      | 104195/300000 [29:09<49:34, 65.83it/s]
epoch 104200  training loss: 1.1508516073226929
epoch 104200  clean testing loss: 6.650136947631836
epoch 104300  training loss: 1.1113765239715576

 35%|████████████████████████████▊                                                      | 104328/300000 [29:11<49:26, 65.95it/s]
epoch 104400  training loss: 1.1672968864440918

 35%|████████████████████████████▉                                                      | 104461/300000 [29:13<48:59, 66.51it/s]
epoch 104500  training loss: 1.1434744596481323

 35%|████████████████████████████▉                                                      | 104587/300000 [29:15<48:56, 66.55it/s]
epoch 104600  training loss: 1.1794133186340332
epoch 104600  clean testing loss: 6.6265950202941895
epoch 104700  training loss: 1.1189311742782593


 35%|█████████████████████████████                                                      | 104853/300000 [29:19<49:09, 66.16it/s]
epoch 104800  training loss: 1.219260811805725

 35%|█████████████████████████████                                                      | 104986/300000 [29:21<48:59, 66.33it/s]
epoch 104900  training loss: 1.144256353378296

 35%|█████████████████████████████                                                      | 105135/300000 [29:23<42:53, 75.71it/s]
epoch 105000  training loss: 1.0999385118484497
epoch 105000  clean testing loss: 6.601277828216553
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise5.00e+00_invop1 ...
epoch 105100  training loss: 1.130610704421997

 35%|█████████████████████████████▏                                                     | 105287/300000 [29:25<42:40, 76.05it/s]
epoch 105200  training loss: 1.172931432723999

 35%|█████████████████████████████▏                                                     | 105427/300000 [29:27<48:33, 66.79it/s]
epoch 105300  training loss: 1.1318933963775635
epoch 105300  clean testing loss: 6.602113723754883
epoch 105400  training loss: 1.1154298782348633

 35%|█████████████████████████████▏                                                     | 105560/300000 [29:29<49:07, 65.98it/s]
epoch 105500  training loss: 1.1542012691497803

 35%|█████████████████████████████▏                                                     | 105693/300000 [29:31<49:01, 66.06it/s]
epoch 105600  training loss: 1.156244158744812

 35%|█████████████████████████████▎                                                     | 105826/300000 [29:33<48:25, 66.82it/s]
epoch 105700  training loss: 1.1201629638671875
epoch 105700  clean testing loss: 6.611690044403076
epoch 105800  training loss: 1.2005012035369873

 35%|█████████████████████████████▎                                                     | 105875/300000 [29:34<49:07, 65.86it/s]
epoch 105900  training loss: 1.1729393005371094
epoch 105900  clean testing loss: 6.592062950134277
epoch 106000  training loss: 1.2572500705718994
epoch 106000  clean testing loss: 6.603788375854492
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise5.00e+00_invop1 ...
epoch 106100  training loss: 1.1981695890426636
epoch 106100  clean testing loss: 6.613605499267578
epoch 106200  training loss: 1.1777598857879639
epoch 106200  clean testing loss: 6.649386405944824
epoch 106300  training loss: 1.1000466346740723
epoch 106300  clean testing loss: 6.635457515716553
epoch 106400  training loss: 1.0897127389907837

 35%|███████████████████████████▋                                                  | 106478/300000 [47:02<3221:07:32, 59.92s/it]
epoch 106500  training loss: 1.1147992610931396
epoch 106500  clean testing loss: 6.644075393676758
epoch 106600  training loss: 1.131471037864685

 36%|████████████████████████████▊                                                    | 106610/300000 [47:04<3:32:48, 15.15it/s]
epoch 106700  training loss: 1.205396056175232

 36%|█████████████████████████████▌                                                     | 106743/300000 [47:06<48:48, 65.99it/s]
epoch 106800  training loss: 1.1315302848815918

 36%|█████████████████████████████▌                                                     | 106869/300000 [47:08<48:34, 66.27it/s]
epoch 106900  training loss: 1.1576608419418335
epoch 106900  clean testing loss: 6.642459869384766
epoch 107000  training loss: 1.2338701486587524
epoch 107000  clean testing loss: 6.636383056640625

 36%|█████████████████████████████▌                                                     | 106995/300000 [47:10<48:19, 66.56it/s]
epoch 107100  training loss: 1.1559187173843384

 36%|█████████████████████████████▋                                                     | 107128/300000 [47:12<48:51, 65.80it/s]
epoch 107200  training loss: 1.0806221961975098

 36%|█████████████████████████████▋                                                     | 107261/300000 [47:14<50:00, 64.24it/s]
epoch 107300  training loss: 1.102408528327942

 36%|█████████████████████████████▋                                                     | 107387/300000 [47:16<48:49, 65.75it/s]
epoch 107400  training loss: 1.1992740631103516
epoch 107400  clean testing loss: 6.639540195465088
epoch 107500  training loss: 1.0550991296768188

 36%|█████████████████████████████▋                                                     | 107520/300000 [47:18<48:57, 65.52it/s]
epoch 107600  training loss: 1.1320329904556274

 36%|█████████████████████████████▊                                                     | 107957/300000 [47:24<48:18, 66.25it/s]
epoch 107700  training loss: 1.0978320837020874
epoch 107700  clean testing loss: 6.659367561340332
epoch 107800  training loss: 1.1366355419158936
epoch 107800  clean testing loss: 6.654207706451416
epoch 107900  training loss: 1.0274876356124878

 36%|█████████████████████████████▉                                                     | 108083/300000 [47:26<48:47, 65.56it/s]
epoch 108000  training loss: 1.1052166223526
epoch 108000  clean testing loss: 6.641017913818359

 36%|█████████████████████████████▉                                                     | 108216/300000 [47:28<49:32, 64.53it/s]
epoch 108100  training loss: 1.1234221458435059
epoch 108100  clean testing loss: 6.65042781829834
epoch 108200  training loss: 1.07559072971344

 36%|█████████████████████████████▉                                                     | 108349/300000 [47:30<49:25, 64.62it/s]
epoch 108300  training loss: 1.066339135169983

 36%|██████████████████████████████                                                     | 108517/300000 [47:33<48:46, 65.43it/s]
epoch 108400  training loss: 1.095215082168579
epoch 108400  clean testing loss: 6.656639575958252
epoch 108500  training loss: 1.091995358467102

 36%|██████████████████████████████                                                     | 108650/300000 [47:35<48:06, 66.28it/s]
epoch 108600  training loss: 1.1361006498336792
epoch 108600  clean testing loss: 6.644504070281982
epoch 108700  training loss: 1.1167439222335815


 36%|██████████████████████████████▏                                                    | 108909/300000 [47:39<48:48, 65.26it/s]
epoch 108800  training loss: 0.9991097450256348
epoch 108800  clean testing loss: 6.665019989013672
epoch 108900  training loss: 1.0400192737579346

 36%|██████████████████████████████▏                                                    | 109042/300000 [47:41<48:22, 65.78it/s]
epoch 109000  training loss: 1.0703459978103638
epoch 109000  clean testing loss: 6.647522926330566

 36%|██████████████████████████████▏                                                    | 109175/300000 [47:43<48:19, 65.82it/s]
epoch 109100  training loss: 1.1714915037155151

 36%|██████████████████████████████▏                                                    | 109308/300000 [47:45<48:23, 65.68it/s]
epoch 109200  training loss: 1.0674381256103516
epoch 109200  clean testing loss: 6.664707183837891
epoch 109300  training loss: 1.176734447479248

 36%|██████████████████████████████▎                                                    | 109434/300000 [47:47<48:35, 65.36it/s]
epoch 109400  training loss: 1.1132630109786987

 37%|██████████████████████████████▎                                                    | 109567/300000 [47:49<48:05, 66.01it/s]
epoch 109500  training loss: 1.0442264080047607

 37%|██████████████████████████████▎                                                    | 109700/300000 [47:51<47:58, 66.11it/s]
epoch 109600  training loss: 1.1479673385620117

 37%|██████████████████████████████▍                                                    | 109939/300000 [47:54<48:11, 65.74it/s]
epoch 109700  training loss: 1.1183971166610718
epoch 109700  clean testing loss: 6.669949054718018
epoch 109800  training loss: 1.1092593669891357
epoch 109800  clean testing loss: 6.693146705627441
epoch 109900  training loss: 1.1951148509979248

 37%|██████████████████████████████▍                                                    | 110065/300000 [47:56<48:03, 65.87it/s]
epoch 110000  training loss: 1.1575003862380981
epoch 110000  clean testing loss: 6.690039157867432

 37%|██████████████████████████████▍                                                    | 110198/300000 [47:58<48:25, 65.33it/s]
epoch 110100  training loss: 1.1176221370697021

 37%|██████████████████████████████▌                                                    | 110331/300000 [48:00<47:30, 66.54it/s]
epoch 110200  training loss: 1.1221914291381836
epoch 110200  clean testing loss: 6.69045352935791
epoch 110300  training loss: 1.0989515781402588

 37%|██████████████████████████████▌                                                    | 110597/300000 [48:04<47:47, 66.06it/s]
epoch 110400  training loss: 1.2015361785888672
epoch 110400  clean testing loss: 6.699559688568115
epoch 110500  training loss: 1.0900061130523682

 37%|██████████████████████████████▋                                                    | 110730/300000 [48:06<47:31, 66.38it/s]
epoch 110600  training loss: 1.1520748138427734
epoch 110600  clean testing loss: 6.700568199157715
epoch 110700  training loss: 1.115182638168335

 37%|██████████████████████████████▋                                                    | 110867/300000 [48:08<42:45, 73.71it/s]
epoch 110800  training loss: 1.155490756034851

 37%|██████████████████████████████▋                                                    | 110993/300000 [48:10<38:05, 82.70it/s]
epoch 110900  training loss: 1.1515705585479736
epoch 110900  clean testing loss: 6.693914413452148
epoch 111000  training loss: 1.1231497526168823
epoch 111000  clean testing loss: 6.7111358642578125
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise5.00e+00_invop1 ...
epoch 111100  training loss: 1.052118182182312


 37%|██████████████████████████████▊                                                    | 111247/300000 [48:19<47:48, 65.81it/s]
epoch 111200  training loss: 1.1119647026062012
epoch 111200  clean testing loss: 6.704087257385254
epoch 111300  training loss: 1.2289541959762573

 37%|██████████████████████████████▊                                                    | 111380/300000 [48:22<47:27, 66.25it/s]
epoch 111400  training loss: 1.1199533939361572
epoch 111400  clean testing loss: 6.670248508453369
epoch 111500  training loss: 1.1289328336715698


 37%|██████████████████████████████▉                                                    | 111639/300000 [48:25<47:17, 66.39it/s]
epoch 111600  training loss: 1.1211395263671875
epoch 111600  clean testing loss: 6.699223518371582
epoch 111700  training loss: 1.1227411031723022

 37%|██████████████████████████████▉                                                    | 111772/300000 [48:27<47:24, 66.17it/s]
epoch 111800  training loss: 1.1271439790725708

 37%|██████████████████████████████▉                                                    | 111905/300000 [48:30<47:21, 66.19it/s]
epoch 111900  training loss: 1.1430628299713135
epoch 111900  clean testing loss: 6.7129106521606445
epoch 112000  training loss: 1.0565418004989624
epoch 112000  clean testing loss: 6.714932441711426


 37%|███████████████████████████████                                                    | 112164/300000 [48:34<48:08, 65.02it/s]
epoch 112100  training loss: 1.0486204624176025

 37%|███████████████████████████████                                                    | 112297/300000 [48:36<47:43, 65.55it/s]
epoch 112200  training loss: 1.124464750289917

 37%|███████████████████████████████                                                    | 112423/300000 [48:37<47:05, 66.39it/s]
epoch 112300  training loss: 1.2147904634475708
epoch 112300  clean testing loss: 6.713022708892822
epoch 112400  training loss: 1.0744037628173828

 38%|███████████████████████████████▏                                                   | 112556/300000 [48:40<47:38, 65.57it/s]
epoch 112500  training loss: 1.1322388648986816

 38%|███████████████████████████████▏                                                   | 112689/300000 [48:42<47:15, 66.07it/s]
epoch 112600  training loss: 1.1002366542816162

 38%|███████████████████████████████▏                                                   | 112822/300000 [48:44<47:35, 65.56it/s]
epoch 112700  training loss: 1.1111477613449097
epoch 112700  clean testing loss: 6.719833850860596
epoch 112800  training loss: 1.1800254583358765

 38%|███████████████████████████████▎                                                   | 112955/300000 [48:46<47:34, 65.54it/s]
epoch 112900  training loss: 1.1995221376419067

 38%|███████████████████████████████▎                                                   | 113103/300000 [48:50<48:41, 63.97it/s]
epoch 113000  training loss: 1.1559392213821411
epoch 113000  clean testing loss: 6.7182159423828125
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise5.00e+00_invop1 ...
epoch 113100  training loss: 1.2490936517715454

 38%|███████████████████████████████▎                                                   | 113236/300000 [48:52<46:51, 66.43it/s]
epoch 113200  training loss: 1.158853530883789

 38%|███████████████████████████████▎                                                   | 113369/300000 [48:54<46:38, 66.68it/s]
epoch 113300  training loss: 1.1080251932144165

 38%|███████████████████████████████▌                                                   | 113901/300000 [49:02<46:48, 66.26it/s]
epoch 113400  training loss: 1.118661880493164
epoch 113400  clean testing loss: 6.7192487716674805
epoch 113500  training loss: 1.115244746208191
epoch 113500  clean testing loss: 6.720530986785889
epoch 113600  training loss: 1.1838897466659546
epoch 113600  clean testing loss: 6.727850914001465
epoch 113700  training loss: 1.1948601007461548
epoch 113700  clean testing loss: 6.721050262451172
epoch 113800  training loss: 1.113263726234436
epoch 113800  clean testing loss: 6.718747615814209
epoch 113900  training loss: 1.2110896110534668

 38%|███████████████████████████████▌                                                   | 114034/300000 [49:04<46:56, 66.03it/s]
epoch 114000  training loss: 1.1511121988296509
epoch 114000  clean testing loss: 6.736896514892578

 38%|███████████████████████████████▌                                                   | 114167/300000 [49:06<46:47, 66.19it/s]
epoch 114100  training loss: 1.1431182622909546
epoch 114100  clean testing loss: 6.739501476287842
epoch 114200  training loss: 1.1837679147720337
epoch 114200  clean testing loss: 6.746103286743164
epoch 114300  training loss: 1.1085413694381714


 38%|███████████████████████████████▋                                                   | 114433/300000 [49:10<46:24, 66.65it/s]
epoch 114400  training loss: 1.1259058713912964

 38%|███████████████████████████████▋                                                   | 114566/300000 [49:12<47:11, 65.49it/s]
epoch 114500  training loss: 1.1672672033309937

 38%|███████████████████████████████▋                                                   | 114692/300000 [49:14<46:53, 65.87it/s]
epoch 114600  training loss: 1.1449987888336182
epoch 114600  clean testing loss: 6.77420711517334
epoch 114700  training loss: 1.1333699226379395

 38%|███████████████████████████████▊                                                   | 114825/300000 [49:16<46:36, 66.22it/s]
epoch 114800  training loss: 1.2564254999160767

 38%|███████████████████████████████▊                                                   | 114958/300000 [49:18<46:43, 66.00it/s]
epoch 114900  training loss: 1.1400178670883179

 38%|███████████████████████████████▊                                                   | 115091/300000 [49:20<46:42, 65.98it/s]
epoch 115000  training loss: 1.0920379161834717
epoch 115000  clean testing loss: 6.79378604888916
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise5.00e+00_invop1 ...
epoch 115100  training loss: 1.1465777158737183

 38%|███████████████████████████████▉                                                   | 115224/300000 [49:22<46:24, 66.36it/s]
epoch 115200  training loss: 1.1186717748641968

 38%|███████████████████████████████▉                                                   | 115357/300000 [49:24<46:56, 65.55it/s]
epoch 115300  training loss: 1.0940080881118774

 38%|███████████████████████████████▉                                                   | 115490/300000 [49:26<46:20, 66.36it/s]
epoch 115400  training loss: 1.096332311630249
epoch 115400  clean testing loss: 6.784027099609375
epoch 115500  training loss: 1.0320988893508911

 39%|███████████████████████████████▉                                                   | 115623/300000 [49:28<46:47, 65.67it/s]
epoch 115600  training loss: 1.0842652320861816

 39%|████████████████████████████████                                                   | 115756/300000 [49:30<46:13, 66.42it/s]
epoch 115700  training loss: 1.1552342176437378

 39%|████████████████████████████████                                                   | 115889/300000 [49:32<46:37, 65.81it/s]
epoch 115800  training loss: 1.1465004682540894
epoch 115800  clean testing loss: 6.788173675537109
epoch 115900  training loss: 1.0202784538269043

 39%|████████████████████████████████                                                   | 116022/300000 [49:34<46:21, 66.14it/s]
epoch 116000  training loss: 1.149245023727417
epoch 116000  clean testing loss: 6.78402042388916

 39%|████████████████████████████████▏                                                  | 116155/300000 [49:36<46:24, 66.02it/s]
epoch 116100  training loss: 1.0434417724609375

 39%|████████████████████████████████▏                                                  | 116288/300000 [49:38<46:17, 66.13it/s]
epoch 116200  training loss: 1.2255313396453857

 39%|████████████████████████████████▏                                                  | 116421/300000 [49:40<46:49, 65.35it/s]
epoch 116300  training loss: 1.0591745376586914
epoch 116300  clean testing loss: 6.80444860458374
epoch 116400  training loss: 1.0256907939910889

 39%|████████████████████████████████▏                                                  | 116547/300000 [49:42<46:28, 65.80it/s]
epoch 116500  training loss: 1.0821934938430786

 39%|████████████████████████████████▎                                                  | 116680/300000 [49:44<46:30, 65.69it/s]
epoch 116600  training loss: 1.0919172763824463

 39%|████████████████████████████████▎                                                  | 116813/300000 [49:46<46:44, 65.31it/s]
epoch 116700  training loss: 1.1138583421707153
epoch 116700  clean testing loss: 6.805270671844482
epoch 116800  training loss: 1.1104328632354736

 39%|████████████████████████████████▎                                                  | 116946/300000 [49:48<46:08, 66.12it/s]
epoch 116900  training loss: 1.1365879774093628

 39%|████████████████████████████████▎                                                  | 116995/300000 [49:49<46:16, 65.91it/s]
epoch 117000  training loss: 1.0957107543945312
epoch 117000  clean testing loss: 6.802765846252441

 39%|████████████████████████████████▍                                                  | 117121/300000 [49:55<47:34, 64.08it/s]
epoch 117100  training loss: 1.128665566444397

 39%|████████████████████████████████▍                                                  | 117254/300000 [49:57<46:04, 66.09it/s]
epoch 117200  training loss: 1.0541216135025024

 39%|████████████████████████████████▍                                                  | 117380/300000 [49:59<46:19, 65.71it/s]
epoch 117300  training loss: 1.1377562284469604

 39%|████████████████████████████████▌                                                  | 117513/300000 [50:01<46:26, 65.48it/s]
epoch 117400  training loss: 1.0871502161026
epoch 117400  clean testing loss: 6.813399791717529
epoch 117500  training loss: 1.0803958177566528

 39%|████████████████████████████████▌                                                  | 117646/300000 [50:03<46:22, 65.55it/s]
epoch 117600  training loss: 1.086383581161499

 39%|████████████████████████████████▌                                                  | 117779/300000 [50:05<46:08, 65.83it/s]
epoch 117700  training loss: 1.126359224319458

 39%|████████████████████████████████▌                                                  | 117912/300000 [50:07<46:12, 65.69it/s]
epoch 117800  training loss: 1.188938021659851
epoch 117800  clean testing loss: 6.799019813537598
epoch 117900  training loss: 1.2050151824951172
epoch 117900  clean testing loss: 6.815411567687988
epoch 118000  training loss: 1.2347705364227295
epoch 118000  clean testing loss: 6.803882122039795
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise5.00e+00_invop1 ...
epoch 118100  training loss: 1.1406612396240234

 39%|████████████████████████████████▋                                                  | 118101/300000 [50:17<51:46, 58.56it/s]
epoch 118200  training loss: 1.2052421569824219

 39%|████████████████████████████████▋                                                  | 118234/300000 [50:19<45:45, 66.20it/s]
epoch 118300  training loss: 1.2168371677398682

 39%|████████████████████████████████▋                                                  | 118367/300000 [50:21<46:39, 64.89it/s]
epoch 118400  training loss: 1.1194860935211182
epoch 118400  clean testing loss: 6.7911810874938965
epoch 118500  training loss: 1.1018913984298706

 40%|████████████████████████████████▊                                                  | 118500/300000 [50:23<45:35, 66.36it/s]
epoch 118600  training loss: 1.1448031663894653

 40%|████████████████████████████████▊                                                  | 118633/300000 [50:25<45:55, 65.82it/s]
epoch 118700  training loss: 1.0390710830688477

 40%|████████████████████████████████▊                                                  | 118766/300000 [50:27<45:24, 66.53it/s]
epoch 118800  training loss: 1.1179893016815186
epoch 118800  clean testing loss: 6.794512748718262
epoch 118900  training loss: 1.1639949083328247

 40%|████████████████████████████████▉                                                  | 118899/300000 [50:29<45:54, 65.75it/s]
epoch 119000  training loss: 1.05430006980896
epoch 119000  clean testing loss: 6.775973796844482

 40%|████████████████████████████████▉                                                  | 118997/300000 [50:30<45:27, 66.36it/s]
epoch 119100  training loss: 1.134883999824524

 40%|████████████████████████████████▉                                                  | 119130/300000 [50:41<47:42, 63.18it/s]
epoch 119200  training loss: 1.1222198009490967

 40%|████████████████████████████████▉                                                  | 119263/300000 [50:43<45:07, 66.76it/s]
epoch 119300  training loss: 1.1502907276153564

 40%|█████████████████████████████████                                                  | 119368/300000 [50:45<45:12, 66.60it/s]
epoch 119400  training loss: 1.1502526998519897
epoch 119400  clean testing loss: 6.773196697235107
epoch 119500  training loss: 1.2580618858337402

 40%|█████████████████████████████████                                                  | 119599/300000 [50:48<45:32, 66.02it/s]
epoch 119600  training loss: 1.182968020439148
epoch 119600  clean testing loss: 6.7810845375061035
epoch 119700  training loss: 1.0988203287124634

 40%|█████████████████████████████████▏                                                 | 119732/300000 [50:50<45:24, 66.17it/s]
epoch 119800  training loss: 1.1983639001846313

 40%|█████████████████████████████████▏                                                 | 119865/300000 [50:53<45:20, 66.20it/s]
epoch 119900  training loss: 1.154390811920166

 40%|█████████████████████████████████▏                                                 | 119963/300000 [50:54<45:19, 66.19it/s]
epoch 120000  training loss: 1.1793906688690186
epoch 120000  clean testing loss: 6.789998531341553

 40%|█████████████████████████████████▏                                                 | 120131/300000 [50:57<45:00, 66.60it/s]
epoch 120100  training loss: 1.1469656229019165
epoch 120100  clean testing loss: 6.781355857849121
epoch 120200  training loss: 1.1299811601638794

 40%|█████████████████████████████████▎                                                 | 120257/300000 [50:58<45:23, 65.99it/s]
epoch 120300  training loss: 1.1167532205581665

 40%|█████████████████████████████████▎                                                 | 120390/300000 [51:00<44:54, 66.66it/s]
epoch 120400  training loss: 1.1635913848876953

 40%|█████████████████████████████████▎                                                 | 120523/300000 [51:02<45:22, 65.93it/s]
epoch 120500  training loss: 1.1306747198104858
epoch 120500  clean testing loss: 6.764825820922852
epoch 120600  training loss: 1.1584521532058716

 40%|█████████████████████████████████▍                                                 | 120656/300000 [51:05<45:29, 65.70it/s]
epoch 120700  training loss: 1.1521742343902588

 40%|█████████████████████████████████▍                                                 | 120789/300000 [51:07<45:55, 65.05it/s]
epoch 120800  training loss: 1.1686408519744873

 40%|█████████████████████████████████▍                                                 | 120887/300000 [51:08<45:01, 66.29it/s]
epoch 120900  training loss: 1.202681541442871
epoch 120900  clean testing loss: 6.779825687408447
epoch 121000  training loss: 1.1632417440414429
epoch 121000  clean testing loss: 6.796211242675781

 40%|█████████████████████████████████▍                                                 | 121020/300000 [51:10<45:47, 65.14it/s]
epoch 121100  training loss: 1.056367039680481

 40%|█████████████████████████████████▌                                                 | 121153/300000 [51:12<45:01, 66.19it/s]
epoch 121200  training loss: 1.2387455701828003

 41%|█████████████████████████████████▋                                                 | 121615/300000 [51:19<44:33, 66.74it/s]
epoch 121300  training loss: 1.11493980884552
epoch 121300  clean testing loss: 6.7775092124938965
epoch 121400  training loss: 1.2046599388122559
epoch 121400  clean testing loss: 6.7648468017578125
epoch 121500  training loss: 1.252526879310608
epoch 121500  clean testing loss: 6.795265197753906
epoch 121600  training loss: 1.2132467031478882
epoch 121600  clean testing loss: 6.798336029052734
epoch 121700  training loss: 1.0514578819274902

 41%|█████████████████████████████████▋                                                 | 121748/300000 [51:21<44:47, 66.32it/s]
epoch 121800  training loss: 1.2534586191177368

 41%|█████████████████████████████████▋                                                 | 121881/300000 [51:23<44:19, 66.96it/s]
epoch 121900  training loss: 1.0627044439315796
epoch 121900  clean testing loss: 6.779962062835693
epoch 122000  training loss: 1.2076863050460815
epoch 122000  clean testing loss: 6.81006383895874


 41%|█████████████████████████████████▊                                                 | 122211/300000 [51:28<44:57, 65.91it/s]
epoch 122100  training loss: 1.0547904968261719
epoch 122100  clean testing loss: 6.779744625091553
epoch 122200  training loss: 1.1419756412506104

 41%|█████████████████████████████████▊                                                 | 122344/300000 [51:30<44:45, 66.15it/s]
epoch 122300  training loss: 1.1888749599456787

 41%|█████████████████████████████████▉                                                 | 122477/300000 [51:32<44:26, 66.58it/s]
epoch 122400  training loss: 1.1327805519104004
epoch 122400  clean testing loss: 6.791558742523193
epoch 122500  training loss: 1.0700281858444214
epoch 122500  clean testing loss: 6.783515930175781
epoch 122600  training loss: 1.157968282699585

 41%|█████████████████████████████████▉                                                 | 122603/300000 [51:34<45:16, 65.30it/s]
epoch 122700  training loss: 1.1962738037109375

 41%|█████████████████████████████████▉                                                 | 122736/300000 [51:36<44:40, 66.14it/s]
epoch 122800  training loss: 1.0350840091705322

 41%|█████████████████████████████████▉                                                 | 122869/300000 [51:38<44:34, 66.23it/s]
epoch 122900  training loss: 1.1112393140792847
epoch 122900  clean testing loss: 6.786561012268066
epoch 123000  training loss: 1.1360769271850586
epoch 123000  clean testing loss: 6.764911651611328
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise5.00e+00_invop1 ...
epoch 123100  training loss: 1.1743167638778687


 41%|██████████████████████████████████                                                 | 123248/300000 [51:50<44:36, 66.03it/s]
epoch 123200  training loss: 1.0486267805099487

 41%|██████████████████████████████████▏                                                | 123381/300000 [51:52<44:01, 66.87it/s]
epoch 123300  training loss: 1.0776386260986328
epoch 123300  clean testing loss: 6.799356460571289
epoch 123400  training loss: 1.1849725246429443
epoch 123400  clean testing loss: 6.806126117706299
epoch 123500  training loss: 1.2072257995605469

 41%|██████████████████████████████████▏                                                | 123514/300000 [51:54<44:17, 66.42it/s]
epoch 123600  training loss: 1.1105984449386597

 41%|██████████████████████████████████▏                                                | 123647/300000 [51:56<44:28, 66.08it/s]
epoch 123700  training loss: 1.150862693786621

 41%|██████████████████████████████████▏                                                | 123780/300000 [51:58<44:26, 66.08it/s]
epoch 123800  training loss: 1.155464768409729
epoch 123800  clean testing loss: 6.8035430908203125
epoch 123900  training loss: 1.0407471656799316

 41%|██████████████████████████████████▎                                                | 123913/300000 [52:00<44:15, 66.30it/s]
epoch 124000  training loss: 1.220440149307251
epoch 124000  clean testing loss: 6.808830738067627

 41%|██████████████████████████████████▎                                                | 124046/300000 [52:02<43:54, 66.78it/s]
epoch 124100  training loss: 1.1365011930465698

 41%|██████████████████████████████████▎                                                | 124179/300000 [52:04<44:15, 66.22it/s]
epoch 124200  training loss: 1.0907005071640015
epoch 124200  clean testing loss: 6.785399913787842
epoch 124300  training loss: 1.2072192430496216


 42%|██████████████████████████████████▍                                                | 124676/300000 [52:11<44:03, 66.31it/s]
epoch 124400  training loss: 1.1617432832717896
epoch 124400  clean testing loss: 6.772694110870361
epoch 124500  training loss: 1.0924090147018433
epoch 124500  clean testing loss: 6.782529830932617
epoch 124600  training loss: 1.2332664728164673

 42%|██████████████████████████████████▌                                                | 124809/300000 [52:13<44:21, 65.83it/s]
epoch 124700  training loss: 1.1293879747390747

 42%|██████████████████████████████████▌                                                | 124942/300000 [52:15<44:02, 66.24it/s]
epoch 124800  training loss: 1.1361061334609985
epoch 124800  clean testing loss: 6.781832218170166
epoch 124900  training loss: 1.1592280864715576

 42%|██████████████████████████████████▌                                                | 125068/300000 [52:17<44:17, 65.83it/s]
epoch 125000  training loss: 1.2094402313232422
epoch 125000  clean testing loss: 6.772632122039795

 42%|██████████████████████████████████▋                                                | 125201/300000 [52:19<44:02, 66.15it/s]
epoch 125100  training loss: 1.1804399490356445

 42%|██████████████████████████████████▋                                                | 125334/300000 [52:21<44:35, 65.30it/s]
epoch 125200  training loss: 1.124658465385437
epoch 125200  clean testing loss: 6.784788608551025
epoch 125300  training loss: 1.14696204662323

 42%|██████████████████████████████████▋                                                | 125467/300000 [52:23<44:12, 65.80it/s]
epoch 125400  training loss: 1.1720906496047974

 42%|██████████████████████████████████▋                                                | 125600/300000 [52:25<44:42, 65.02it/s]
epoch 125500  training loss: 1.1475032567977905
epoch 125500  clean testing loss: 6.790432929992676
epoch 125600  training loss: 1.0930202007293701
epoch 125600  clean testing loss: 6.7923126220703125
epoch 125700  training loss: 1.264322280883789
epoch 125700  clean testing loss: 6.774302959442139
epoch 125800  training loss: 1.085399866104126
epoch 125800  clean testing loss: 6.7869367599487305
epoch 125900  training loss: 1.0921714305877686
epoch 125900  clean testing loss: 6.787862777709961
epoch 126000  training loss: 1.1870934963226318
epoch 126000  clean testing loss: 6.7792134284973145

 42%|██████████████████████████████████▊                                                | 125992/300000 [52:31<44:25, 65.29it/s]
epoch 126100  training loss: 1.0874574184417725

 42%|██████████████████████████████████▉                                                | 126140/300000 [52:33<39:22, 73.61it/s]
epoch 126200  training loss: 1.117807149887085


 42%|██████████████████████████████████▉                                                | 126402/300000 [52:37<44:08, 65.54it/s]
epoch 126300  training loss: 1.1358455419540405
epoch 126300  clean testing loss: 6.796260356903076
epoch 126400  training loss: 1.2460718154907227

 42%|███████████████████████████████████                                                | 126535/300000 [52:39<43:36, 66.29it/s]
epoch 126500  training loss: 1.228403091430664

 42%|███████████████████████████████████                                                | 126668/300000 [52:41<44:12, 65.34it/s]
epoch 126600  training loss: 1.1263916492462158
epoch 126600  clean testing loss: 6.786143779754639
epoch 126700  training loss: 1.1298335790634155
epoch 126700  clean testing loss: 6.794848918914795
epoch 126800  training loss: 1.2324296236038208

 42%|███████████████████████████████████                                                | 126801/300000 [52:43<43:33, 66.28it/s]
epoch 126900  training loss: 1.0680798292160034

 42%|███████████████████████████████████                                                | 126934/300000 [52:45<43:45, 65.91it/s]
epoch 127000  training loss: 1.158493161201477
epoch 127000  clean testing loss: 6.791472911834717
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise5.00e+00_invop1 ...
epoch 127100  training loss: 1.1029798984527588


 42%|███████████████████████████████████▏                                               | 127256/300000 [52:54<43:09, 66.70it/s]
epoch 127200  training loss: 1.17783522605896

 42%|███████████████████████████████████▏                                               | 127389/300000 [52:56<43:30, 66.13it/s]
epoch 127300  training loss: 1.0603035688400269

 43%|███████████████████████████████████▎                                               | 127522/300000 [52:58<43:02, 66.79it/s]
epoch 127400  training loss: 1.0833377838134766
epoch 127400  clean testing loss: 6.785089492797852
epoch 127500  training loss: 1.2077646255493164

 43%|███████████████████████████████████▎                                               | 127655/300000 [53:00<43:19, 66.29it/s]
epoch 127600  training loss: 1.132104754447937

 43%|███████████████████████████████████▎                                               | 127788/300000 [53:02<43:25, 66.10it/s]
epoch 127700  training loss: 1.0756949186325073

 43%|███████████████████████████████████▍                                               | 127921/300000 [53:04<43:35, 65.80it/s]
epoch 127800  training loss: 1.1712781190872192
epoch 127800  clean testing loss: 6.790670394897461
epoch 127900  training loss: 1.1702085733413696

 43%|███████████████████████████████████▍                                               | 128047/300000 [53:06<43:08, 66.42it/s]
epoch 128000  training loss: 1.1287469863891602
epoch 128000  clean testing loss: 6.792834281921387

 43%|███████████████████████████████████▍                                               | 128180/300000 [53:08<43:14, 66.22it/s]
epoch 128100  training loss: 1.1795531511306763
epoch 128100  clean testing loss: 6.81477165222168
epoch 128200  training loss: 1.139289140701294

 43%|███████████████████████████████████▍                                               | 128313/300000 [53:10<43:11, 66.25it/s]
epoch 128300  training loss: 1.072037696838379
epoch 128300  clean testing loss: 6.795764923095703
epoch 128400  training loss: 1.1022588014602661
epoch 128400  clean testing loss: 6.797243595123291
epoch 128500  training loss: 1.2015156745910645

 43%|███████████████████████████████████▌                                               | 128544/300000 [53:14<43:33, 65.61it/s]
epoch 128600  training loss: 1.0788401365280151

 43%|███████████████████████████████████▌                                               | 128677/300000 [53:16<43:19, 65.91it/s]
epoch 128700  training loss: 1.1162136793136597
epoch 128700  clean testing loss: 6.789668083190918
epoch 128800  training loss: 1.1639279127120972

 43%|███████████████████████████████████▋                                               | 128810/300000 [53:18<43:10, 66.07it/s]
epoch 128900  training loss: 1.184951663017273

 43%|███████████████████████████████████▋                                               | 128943/300000 [53:20<42:48, 66.60it/s]
epoch 129000  training loss: 1.1257290840148926
epoch 129000  clean testing loss: 6.80452299118042

 43%|███████████████████████████████████▋                                               | 129076/300000 [53:22<43:12, 65.92it/s]
epoch 129100  training loss: 1.2561558485031128
epoch 129100  clean testing loss: 6.800634384155273
epoch 129200  training loss: 1.0922306776046753

 43%|███████████████████████████████████▋                                               | 129209/300000 [53:24<42:07, 67.56it/s]
epoch 129300  training loss: 1.174808144569397

 43%|███████████████████████████████████▊                                               | 129342/300000 [53:26<43:07, 65.94it/s]
epoch 129400  training loss: 1.0233969688415527

 43%|███████████████████████████████████▊                                               | 129475/300000 [53:28<43:29, 65.35it/s]
epoch 129500  training loss: 1.1172568798065186
epoch 129500  clean testing loss: 6.795125961303711
epoch 129600  training loss: 1.1453635692596436

 43%|███████████████████████████████████▊                                               | 129608/300000 [53:30<42:49, 66.32it/s]
epoch 129700  training loss: 1.1364226341247559


 43%|████████████████████████████████████                                               | 130127/300000 [53:42<44:03, 64.25it/s]
epoch 129800  training loss: 1.1009902954101562
epoch 129800  clean testing loss: 6.796732425689697
epoch 129900  training loss: 1.141221523284912
epoch 129900  clean testing loss: 6.782766819000244
epoch 130000  training loss: 1.0996973514556885
epoch 130000  clean testing loss: 6.789756774902344
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise5.00e+00_invop1 ...
epoch 130100  training loss: 1.1000328063964844
epoch 130100  clean testing loss: 6.7835373878479
epoch 130200  training loss: 1.1626461744308472

 43%|████████████████████████████████████                                               | 130260/300000 [53:44<42:42, 66.24it/s]
epoch 130300  training loss: 1.1631834506988525

 43%|████████████████████████████████████                                               | 130393/300000 [53:46<42:41, 66.22it/s]
epoch 130400  training loss: 1.0725586414337158
epoch 130400  clean testing loss: 6.784282207489014
epoch 130500  training loss: 1.1074867248535156

 44%|████████████████████████████████████                                               | 130526/300000 [53:48<42:36, 66.28it/s]
epoch 130600  training loss: 1.2344880104064941

 44%|████████████████████████████████████▏                                              | 130624/300000 [53:50<42:38, 66.21it/s]
epoch 130700  training loss: 1.124627947807312


 44%|████████████████████████████████████▎                                              | 131058/300000 [53:56<42:23, 66.41it/s]
epoch 130800  training loss: 1.09148371219635
epoch 130800  clean testing loss: 6.7818450927734375
epoch 130900  training loss: 1.0749868154525757
epoch 130900  clean testing loss: 6.780301094055176
epoch 131000  training loss: 1.1782490015029907
epoch 131000  clean testing loss: 6.808837413787842

 44%|████████████████████████████████████▎                                              | 131191/300000 [53:58<42:14, 66.60it/s]
epoch 131100  training loss: 1.1987520456314087

 44%|████████████████████████████████████▎                                              | 131324/300000 [54:00<42:26, 66.25it/s]
epoch 131200  training loss: 1.0603970289230347

 44%|████████████████████████████████████▎                                              | 131450/300000 [54:02<42:06, 66.71it/s]
epoch 131300  training loss: 1.1398389339447021
epoch 131300  clean testing loss: 6.773858547210693
epoch 131400  training loss: 1.1108897924423218

 44%|████████████████████████████████████▍                                              | 131583/300000 [54:04<42:00, 66.81it/s]
epoch 131500  training loss: 1.1733402013778687

 44%|████████████████████████████████████▍                                              | 131716/300000 [54:06<42:18, 66.28it/s]
epoch 131600  training loss: 1.1040544509887695

 44%|████████████████████████████████████▍                                              | 131849/300000 [54:08<42:11, 66.42it/s]
epoch 131700  training loss: 1.0421894788742065
epoch 131700  clean testing loss: 6.764795303344727
epoch 131800  training loss: 1.0836892127990723

 44%|████████████████████████████████████▌                                              | 131982/300000 [54:10<42:07, 66.49it/s]
epoch 131900  training loss: 1.0806392431259155

 44%|████████████████████████████████████▌                                              | 132115/300000 [54:12<42:11, 66.33it/s]
epoch 132000  training loss: 1.1463487148284912
epoch 132000  clean testing loss: 6.782433032989502

 44%|████████████████████████████████████▌                                              | 132248/300000 [54:14<42:05, 66.41it/s]
epoch 132100  training loss: 1.1758636236190796
epoch 132100  clean testing loss: 6.767124652862549
epoch 132200  training loss: 1.2388721704483032

 44%|████████████████████████████████████▋                                              | 132381/300000 [54:16<42:33, 65.64it/s]
epoch 132300  training loss: 1.1732083559036255

 44%|████████████████████████████████████▋                                              | 132514/300000 [54:18<42:20, 65.92it/s]
epoch 132400  training loss: 1.0996752977371216

 44%|████████████████████████████████████▋                                              | 132647/300000 [54:20<42:14, 66.03it/s]
epoch 132500  training loss: 1.095821499824524
epoch 132500  clean testing loss: 6.768157482147217
epoch 132600  training loss: 1.143338680267334

 44%|████████████████████████████████████▊                                              | 133011/300000 [54:26<42:25, 65.60it/s]
epoch 132700  training loss: 1.1565024852752686
epoch 132700  clean testing loss: 6.768854141235352
epoch 132800  training loss: 1.179864525794983
epoch 132800  clean testing loss: 6.762370586395264
epoch 132900  training loss: 1.1829086542129517
epoch 132900  clean testing loss: 6.760610103607178
epoch 133000  training loss: 1.0949097871780396
epoch 133000  clean testing loss: 6.7612504959106445

 44%|████████████████████████████████████▊                                              | 133137/300000 [54:28<42:28, 65.47it/s]
epoch 133100  training loss: 1.1314096450805664
epoch 133100  clean testing loss: 6.768235206604004
epoch 133200  training loss: 1.1097383499145508

 44%|████████████████████████████████████▊                                              | 133270/300000 [54:30<42:07, 65.96it/s]
epoch 133300  training loss: 1.1845842599868774
epoch 133300  clean testing loss: 6.7699174880981445
epoch 133400  training loss: 1.1970469951629639

 44%|████████████████████████████████████▉                                              | 133403/300000 [54:32<42:13, 65.75it/s]
epoch 133500  training loss: 1.1606347560882568
epoch 133500  clean testing loss: 6.767408847808838
epoch 133600  training loss: 1.18505859375
epoch 133600  clean testing loss: 6.768354415893555
epoch 133700  training loss: 1.1321765184402466
epoch 133700  clean testing loss: 6.769464015960693
epoch 133800  training loss: 1.260404348373413

 45%|█████████████████████████████████████                                              | 133872/300000 [54:39<41:35, 66.58it/s]
epoch 133900  training loss: 1.0697808265686035
epoch 133900  clean testing loss: 6.763777732849121
epoch 134000  training loss: 1.1510891914367676
epoch 134000  clean testing loss: 6.764216899871826


 45%|█████████████████████████████████████                                              | 134131/300000 [54:43<41:52, 66.01it/s]
epoch 134100  training loss: 1.1329988241195679

 45%|█████████████████████████████████████▏                                             | 134264/300000 [54:45<41:38, 66.34it/s]
epoch 134200  training loss: 1.1453242301940918

 45%|█████████████████████████████████████▏                                             | 134397/300000 [54:47<42:01, 65.69it/s]
epoch 134300  training loss: 1.0839399099349976

 45%|█████████████████████████████████████▏                                             | 134530/300000 [54:49<42:06, 65.49it/s]
epoch 134400  training loss: 1.113394856452942
epoch 134400  clean testing loss: 6.766965389251709
epoch 134500  training loss: 1.2368218898773193

 45%|█████████████████████████████████████▎                                             | 134663/300000 [54:51<41:51, 65.83it/s]
epoch 134600  training loss: 1.1493070125579834

 45%|█████████████████████████████████████▎                                             | 134796/300000 [54:53<41:43, 65.99it/s]
epoch 134700  training loss: 1.1103239059448242

 45%|█████████████████████████████████████▎                                             | 134929/300000 [54:55<41:44, 65.90it/s]
epoch 134800  training loss: 1.1202138662338257
epoch 134800  clean testing loss: 6.770075798034668
epoch 134900  training loss: 1.0981976985931396

 45%|█████████████████████████████████████▎                                             | 135055/300000 [54:57<41:42, 65.91it/s]
epoch 135000  training loss: 1.133618950843811
epoch 135000  clean testing loss: 6.772543907165527

 45%|█████████████████████████████████████▍                                             | 135188/300000 [54:59<41:09, 66.73it/s]
epoch 135100  training loss: 1.207719087600708

 45%|█████████████████████████████████████▍                                             | 135321/300000 [55:01<41:44, 65.75it/s]
epoch 135200  training loss: 1.1382718086242676
epoch 135200  clean testing loss: 6.754025459289551
epoch 135300  training loss: 1.0918374061584473

 45%|█████████████████████████████████████▍                                             | 135454/300000 [55:03<41:05, 66.75it/s]
epoch 135400  training loss: 1.081464171409607

 45%|█████████████████████████████████████▌                                             | 135587/300000 [55:05<41:42, 65.71it/s]
epoch 135500  training loss: 1.098105549812317

 45%|█████████████████████████████████████▌                                             | 135720/300000 [55:07<41:31, 65.95it/s]
epoch 135600  training loss: 1.173047661781311
epoch 135600  clean testing loss: 6.756715297698975
epoch 135700  training loss: 1.1228653192520142

 45%|█████████████████████████████████████▌                                             | 135853/300000 [55:09<41:18, 66.23it/s]
epoch 135800  training loss: 1.0950655937194824

 45%|█████████████████████████████████████▌                                             | 135986/300000 [55:11<41:05, 66.54it/s]
epoch 135900  training loss: 1.1900264024734497

 45%|████████████████████████████████████▋                                            | 136100/300000 [55:12<1:06:29, 41.08it/s]
epoch 136000  training loss: 1.1968082189559937
epoch 136000  clean testing loss: 6.76400089263916
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise5.00e+00_invop1 ...
epoch 136100  training loss: 1.1764445304870605
epoch 136100  clean testing loss: 6.756916522979736
Validation loss variation < 1e-6, trained to interpolation, stop
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise5.00e+00_invop1 ...