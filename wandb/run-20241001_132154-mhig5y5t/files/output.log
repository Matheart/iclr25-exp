
  0%|                                                         | 118/300000 [00:01<57:18, 87.20it/s]
epoch 0  training loss: 55.31189727783203
epoch 0  clean testing loss: 53.47922134399414
epoch 100  training loss: 0.868627667427063

  0%|                                                         | 289/300000 [00:03<56:38, 88.20it/s]
epoch 200  training loss: 0.07967445999383926
epoch 200  clean testing loss: 0.10008642822504044
epoch 300  training loss: 0.046394895762205124

  0%|                                                         | 469/300000 [00:05<56:43, 88.00it/s]
epoch 400  training loss: 0.03139352798461914
epoch 400  clean testing loss: 0.05300983786582947
epoch 500  training loss: 0.02502959594130516

  0%|                                                         | 649/300000 [00:07<56:23, 88.46it/s]
epoch 600  training loss: 0.02008824050426483
epoch 600  clean testing loss: 0.04097593203186989
epoch 700  training loss: 0.016638394445180893

  0%|▏                                                        | 793/300000 [00:09<56:16, 88.62it/s]
epoch 800  training loss: 0.013012287206947803

  1%|▎                                                       | 1531/300000 [00:17<56:08, 88.62it/s]
epoch 900  training loss: 0.010027279146015644
epoch 900  clean testing loss: 0.028922876343131065
epoch 1000  training loss: 0.006799428258091211
epoch 1000  clean testing loss: 0.02939450368285179
epoch 1100  training loss: 0.007592359092086554
epoch 1100  clean testing loss: 0.027541372925043106
epoch 1200  training loss: 0.006472072098404169
epoch 1200  clean testing loss: 0.02522939443588257
epoch 1300  training loss: 0.006184493191540241
epoch 1300  clean testing loss: 0.023651551455259323
epoch 1400  training loss: 0.005627367179840803
epoch 1400  clean testing loss: 0.024682195857167244
epoch 1500  training loss: 0.004836743697524071

  1%|▎                                                       | 1711/300000 [00:19<56:47, 87.55it/s]
epoch 1600  training loss: 0.004828709177672863
epoch 1600  clean testing loss: 0.02242053672671318
epoch 1700  training loss: 0.0048651802353560925

  1%|▎                                                       | 1891/300000 [00:21<56:37, 87.74it/s]
epoch 1800  training loss: 0.004614257253706455
epoch 1800  clean testing loss: 0.022558238357305527
epoch 1900  training loss: 0.004419352859258652

  1%|▍                                                       | 2062/300000 [00:23<56:35, 87.74it/s]
epoch 2000  training loss: 0.004233699291944504
epoch 2000  clean testing loss: 0.01698831096291542
epoch 2100  training loss: 0.004555250518023968

  1%|▍                                                       | 2242/300000 [00:25<56:32, 87.78it/s]
epoch 2200  training loss: 0.0036616583820432425

  1%|▍                                                       | 2413/300000 [00:27<56:30, 87.76it/s]
epoch 2300  training loss: 0.0032671077642589808
epoch 2300  clean testing loss: 0.017241835594177246
epoch 2400  training loss: 0.0036083480808883905

  1%|▍                                                       | 2593/300000 [00:29<56:18, 88.03it/s]
epoch 2500  training loss: 0.0039725471287965775
epoch 2500  clean testing loss: 0.015682503581047058
epoch 2600  training loss: 0.0032027200795710087

  1%|▌                                                       | 2773/300000 [00:31<55:56, 88.56it/s]
epoch 2700  training loss: 0.003526261541992426
epoch 2700  clean testing loss: 0.015884794294834137
epoch 2800  training loss: 0.0038520328234881163

  1%|▌                                                       | 2944/300000 [00:33<56:02, 88.34it/s]
epoch 2900  training loss: 0.003021404379978776
epoch 2900  clean testing loss: 0.015538481995463371
epoch 3000  training loss: 0.0028842398896813393

  1%|▌                                                       | 3124/300000 [00:35<56:25, 87.69it/s]
epoch 3100  training loss: 0.0021855083759874105


  1%|▊                                                       | 4492/300000 [00:51<56:09, 87.69it/s]
epoch 3200  training loss: 0.0025714002549648285
epoch 3200  clean testing loss: 0.014439409598708153
epoch 3300  training loss: 0.0031825245823711157
epoch 3300  clean testing loss: 0.016201801598072052
epoch 3400  training loss: 0.0027799759991467
epoch 3400  clean testing loss: 0.013468042016029358
epoch 3500  training loss: 0.0023955709766596556
epoch 3500  clean testing loss: 0.014062048867344856
epoch 3600  training loss: 0.0024870657362043858
epoch 3600  clean testing loss: 0.016082076355814934
epoch 3700  training loss: 0.0029484890401363373
epoch 3700  clean testing loss: 0.016604594886302948
epoch 3800  training loss: 0.0032305496279150248
epoch 3800  clean testing loss: 0.01734255626797676
epoch 3900  training loss: 0.0033245685044676065
epoch 3900  clean testing loss: 0.01673933118581772
epoch 4000  training loss: 0.003249875735491514
epoch 4000  clean testing loss: 0.017213745042681694
epoch 4100  training loss: 0.0031725913286209106
epoch 4100  clean testing loss: 0.014343908987939358
epoch 4200  training loss: 0.003450209740549326
epoch 4200  clean testing loss: 0.0168202705681324
epoch 4300  training loss: 0.0029574986547231674
epoch 4300  clean testing loss: 0.013676959089934826
epoch 4400  training loss: 0.0027264778036624193
epoch 4400  clean testing loss: 0.014529041945934296
epoch 4500  training loss: 0.003182668937370181
epoch 4500  clean testing loss: 0.015722064301371574
epoch 4600  training loss: 0.00268630962818861

  2%|▊                                                       | 4672/300000 [00:53<56:52, 86.55it/s]
epoch 4700  training loss: 0.0027258978225290775
epoch 4700  clean testing loss: 0.01370970532298088
epoch 4800  training loss: 0.002864619717001915

  2%|▉                                                       | 4843/300000 [00:55<56:12, 87.52it/s]
epoch 4900  training loss: 0.002192926127463579
epoch 4900  clean testing loss: 0.013625639490783215
epoch 5000  training loss: 0.002504912205040455

  2%|▉                                                       | 5023/300000 [00:57<56:03, 87.69it/s]
epoch 5100  training loss: 0.0021852031350135803
epoch 5100  clean testing loss: 0.011719360947608948
epoch 5200  training loss: 0.0030141740571707487

  2%|▉                                                       | 5194/300000 [00:59<55:43, 88.17it/s]
epoch 5300  training loss: 0.002057180507108569

  2%|█                                                       | 5374/300000 [01:01<55:49, 87.97it/s]
epoch 5400  training loss: 0.0025768971536308527
epoch 5400  clean testing loss: 0.011892547830939293
epoch 5500  training loss: 0.0025367694906890392

  2%|█                                                       | 5554/300000 [01:03<55:57, 87.71it/s]
epoch 5600  training loss: 0.0025292232166975737
epoch 5600  clean testing loss: 0.012179219163954258
epoch 5700  training loss: 0.0018793088383972645

  2%|█                                                       | 5725/300000 [01:05<55:31, 88.34it/s]
epoch 5800  training loss: 0.0021843251306563616
epoch 5800  clean testing loss: 0.011209864169359207
epoch 5900  training loss: 0.0019581711385399103


  2%|█▏                                                      | 6076/300000 [01:09<56:45, 86.31it/s]
epoch 6000  training loss: 0.002596216043457389

  2%|█▏                                                      | 6256/300000 [01:11<55:29, 88.23it/s]
epoch 6100  training loss: 0.0016087079420685768
epoch 6100  clean testing loss: 0.00886553805321455
epoch 6200  training loss: 0.0013330192305147648

  2%|█▏                                                      | 6427/300000 [01:13<55:20, 88.41it/s]
epoch 6300  training loss: 0.0015395329101011157
epoch 6300  clean testing loss: 0.008596863597631454
epoch 6400  training loss: 0.001961326692253351

  2%|█▏                                                      | 6607/300000 [01:15<55:21, 88.32it/s]
epoch 6500  training loss: 0.0014788848347961903
epoch 6500  clean testing loss: 0.009040417149662971
epoch 6600  training loss: 0.0016426675720140338

  2%|█▎                                                      | 6787/300000 [01:17<55:08, 88.62it/s]
epoch 6700  training loss: 0.001403764239512384

  2%|█▎                                                      | 6967/300000 [01:19<55:11, 88.49it/s]
epoch 6800  training loss: 0.0015649028355255723
epoch 6800  clean testing loss: 0.007503584958612919
epoch 6900  training loss: 0.00178608950227499

  2%|█▎                                                      | 7138/300000 [01:21<55:05, 88.59it/s]
epoch 7000  training loss: 0.001286397920921445
epoch 7000  clean testing loss: 0.006661543156951666
epoch 7100  training loss: 0.0013524621026590466

  2%|█▎                                                      | 7318/300000 [01:23<55:05, 88.55it/s]
epoch 7200  training loss: 0.0012502050958573818
epoch 7200  clean testing loss: 0.007384078111499548
epoch 7300  training loss: 0.0014990480849519372

  2%|█▍                                                      | 7498/300000 [01:25<55:18, 88.15it/s]
epoch 7400  training loss: 0.0018402767600491643

  3%|█▍                                                      | 7669/300000 [01:27<54:55, 88.71it/s]
epoch 7500  training loss: 0.001558489864692092
epoch 7500  clean testing loss: 0.007078785449266434
epoch 7600  training loss: 0.0022440648172050714

  3%|█▍                                                      | 7849/300000 [01:29<55:02, 88.46it/s]
epoch 7700  training loss: 0.002019993495196104
epoch 7700  clean testing loss: 0.008699668571352959
epoch 7800  training loss: 0.0015369595494121313

  3%|█▍                                                      | 8029/300000 [01:31<55:11, 88.16it/s]
epoch 7900  training loss: 0.0013358178548514843
epoch 7900  clean testing loss: 0.009093917906284332
epoch 8000  training loss: 0.001461111125536263

  3%|█▌                                                      | 8209/300000 [01:33<55:00, 88.40it/s]
epoch 8100  training loss: 0.0017976131057366729
epoch 8100  clean testing loss: 0.009106393903493881
epoch 8200  training loss: 0.001600419171154499

  3%|█▌                                                      | 8380/300000 [01:35<54:50, 88.63it/s]
epoch 8300  training loss: 0.0014887257711961865

  3%|█▌                                                      | 8560/300000 [01:37<55:03, 88.21it/s]
epoch 8400  training loss: 0.0018829770851880312
epoch 8400  clean testing loss: 0.008821466937661171
epoch 8500  training loss: 0.0020729294046759605

  3%|█▌                                                      | 8695/300000 [01:38<54:49, 88.54it/s]
epoch 8600  training loss: 0.0014670283999294043
epoch 8600  clean testing loss: 0.008710989728569984
epoch 8700  training loss: 0.0012249661376699805

  3%|█▊                                                      | 9451/300000 [01:47<54:46, 88.41it/s]
epoch 8800  training loss: 0.0011009883601218462
epoch 8800  clean testing loss: 0.007338353432714939
epoch 8900  training loss: 0.001316601992584765
epoch 8900  clean testing loss: 0.006678172852844
epoch 9000  training loss: 0.002269563265144825
epoch 9000  clean testing loss: 0.007532330229878426
epoch 9100  training loss: 0.0015555896097794175
epoch 9100  clean testing loss: 0.0074853370897471905
epoch 9200  training loss: 0.0010958878556266427
epoch 9200  clean testing loss: 0.007269526831805706
epoch 9300  training loss: 0.0010662758722901344
epoch 9300  clean testing loss: 0.007277540396898985
epoch 9400  training loss: 0.0016979380743578076

  3%|█▊                                                      | 9622/300000 [01:49<54:37, 88.60it/s]
epoch 9500  training loss: 0.001509377732872963
epoch 9500  clean testing loss: 0.0071194469928741455
epoch 9600  training loss: 0.001535520888864994

  3%|█▊                                                      | 9802/300000 [01:51<54:33, 88.64it/s]
epoch 9700  training loss: 0.0011752024292945862
epoch 9700  clean testing loss: 0.006392281968146563
epoch 9800  training loss: 0.0013483583461493254

  3%|█▊                                                      | 9982/300000 [01:53<54:23, 88.86it/s]
epoch 9900  training loss: 0.0014117868850007653

  3%|█▊                                                     | 10162/300000 [01:55<54:33, 88.55it/s]
epoch 10000  training loss: 0.001201703678816557
epoch 10000  clean testing loss: 0.007580952253192663
epoch 10100  training loss: 0.0013829885283485055

  3%|█▉                                                     | 10333/300000 [01:57<54:26, 88.68it/s]
epoch 10200  training loss: 0.0012297993525862694
epoch 10200  clean testing loss: 0.006607049144804478
epoch 10300  training loss: 0.0012484182370826602

  3%|█▉                                                     | 10396/300000 [01:58<54:23, 88.73it/s]
epoch 10400  training loss: 0.001152418670244515

  4%|█▉                                                     | 10693/300000 [02:01<54:18, 88.79it/s]
epoch 10500  training loss: 0.0011900386307388544
epoch 10500  clean testing loss: 0.00650046207010746
epoch 10600  training loss: 0.0013751116348430514

  4%|█▉                                                     | 10873/300000 [02:03<54:17, 88.75it/s]
epoch 10700  training loss: 0.0010987365385517478
epoch 10700  clean testing loss: 0.005817230325192213
epoch 10800  training loss: 0.0017046092543751001

  4%|██                                                     | 11044/300000 [02:05<54:23, 88.55it/s]
epoch 10900  training loss: 0.0010337942512705922
epoch 10900  clean testing loss: 0.0051265014335513115
epoch 11000  training loss: 0.0009189816191792488

  4%|██                                                     | 11224/300000 [02:07<54:49, 87.79it/s]
epoch 11100  training loss: 0.0010994161712005734
epoch 11100  clean testing loss: 0.005892941728234291
epoch 11200  training loss: 0.0010701987193897367

  4%|██                                                     | 11404/300000 [02:09<54:59, 87.47it/s]
epoch 11300  training loss: 0.001384615316055715

  4%|██                                                     | 11575/300000 [02:11<54:19, 88.50it/s]
epoch 11400  training loss: 0.001577917137183249
epoch 11400  clean testing loss: 0.005756437312811613
epoch 11500  training loss: 0.0010746816406026483

  4%|██▏                                                    | 11755/300000 [02:13<54:09, 88.71it/s]
epoch 11600  training loss: 0.0013873694697394967
epoch 11600  clean testing loss: 0.005682350602000952
epoch 11700  training loss: 0.0009020224097184837

  4%|██▏                                                    | 11935/300000 [02:15<54:21, 88.33it/s]
epoch 11800  training loss: 0.0008250288083218038
epoch 11800  clean testing loss: 0.004278167150914669
epoch 11900  training loss: 0.0007852610433474183

  4%|██▏                                                    | 11998/300000 [02:16<54:38, 87.84it/s]
epoch 12000  training loss: 0.0010619725799188018
epoch 12000  clean testing loss: 0.0051019033417105675
epoch 12100  training loss: 0.0007740223663859069
epoch 12100  clean testing loss: 0.004937211517244577
epoch 12200  training loss: 0.0008973824442364275
epoch 12200  clean testing loss: 0.004709061700850725
epoch 12300  training loss: 0.0007823046762496233
  4%|██▎                                                    | 12357/300000 [02:20<54:24, 88.12it/s]
Traceback (most recent call last):
  File "/home/howon/aistats25-exp/nn_exp.py", line 239, in <module>
    losses.backward()
  File "/home/howon/.conda/envs/cs552/lib/python3.10/site-packages/torch/_tensor.py", line 525, in backward
    torch.autograd.backward(
  File "/home/howon/.conda/envs/cs552/lib/python3.10/site-packages/torch/autograd/__init__.py", line 267, in backward
    _engine_run_backward(
  File "/home/howon/.conda/envs/cs552/lib/python3.10/site-packages/torch/autograd/graph.py", line 744, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt