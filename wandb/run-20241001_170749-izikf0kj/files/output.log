
  0%|          | 39/100000 [00:01<52:25, 31.78it/s]
epoch 0  training loss: 47.538387298583984
epoch 0  clean testing loss: 45.753604888916016

  0%|          | 103/100000 [00:03<51:43, 32.19it/s]
epoch 100  training loss: 0.6898808479309082


  0%|          | 235/100000 [00:07<51:28, 32.30it/s]
epoch 200  training loss: 0.14898847043514252

  0%|          | 299/100000 [00:09<51:27, 32.29it/s]
epoch 300  training loss: 0.12627959251403809


  0%|          | 427/100000 [00:13<51:30, 32.22it/s]
epoch 400  training loss: 0.11768471449613571

  0%|          | 491/100000 [00:15<51:22, 32.28it/s]
epoch 500  training loss: 0.11447472870349884


  1%|          | 623/100000 [00:19<51:19, 32.27it/s]
epoch 600  training loss: 0.1094721257686615


  1%|          | 751/100000 [00:23<51:16, 32.26it/s]
epoch 700  training loss: 0.10690917819738388

  1%|          | 799/100000 [00:24<51:14, 32.26it/s]
epoch 800  training loss: 0.10511424392461777

  1%|          | 943/100000 [00:29<51:11, 32.25it/s]
epoch 900  training loss: 0.10342124104499817

  1%|          | 1007/100000 [00:31<51:42, 31.90it/s]
epoch 1000  training loss: 0.10335895419120789
epoch 1000  clean testing loss: 0.012035314925014973


  1%|          | 1135/100000 [00:35<51:11, 32.19it/s]
epoch 1100  training loss: 0.10168030858039856

  1%|          | 1199/100000 [00:37<51:08, 32.20it/s]
epoch 1200  training loss: 0.10016854852437973


  1%|▏         | 1327/100000 [00:41<51:07, 32.17it/s]
epoch 1300  training loss: 0.09921730309724808


  1%|▏         | 1459/100000 [00:45<50:59, 32.20it/s]
epoch 1400  training loss: 0.09827272593975067

  2%|▏         | 1523/100000 [00:47<51:05, 32.13it/s]
epoch 1500  training loss: 0.09713344275951385


  2%|▏         | 1651/100000 [00:51<51:06, 32.08it/s]
epoch 1600  training loss: 0.09618233889341354

  2%|▏         | 1715/100000 [00:53<51:02, 32.09it/s]
epoch 1700  training loss: 0.09547948092222214


  2%|▏         | 1843/100000 [00:57<50:51, 32.16it/s]
epoch 1800  training loss: 0.09606292843818665

  2%|▏         | 1907/100000 [00:59<50:59, 32.06it/s]
epoch 1900  training loss: 0.09891811013221741


  2%|▏         | 2035/100000 [01:03<50:52, 32.09it/s]
epoch 2000  training loss: 0.09742658585309982
epoch 2000  clean testing loss: 0.008918105624616146

  2%|▏         | 2099/100000 [01:05<50:42, 32.18it/s]
epoch 2100  training loss: 0.09603843092918396


  2%|▏         | 2227/100000 [01:09<50:43, 32.13it/s]
epoch 2200  training loss: 0.0959741547703743


  2%|▏         | 2359/100000 [01:13<50:34, 32.18it/s]
epoch 2300  training loss: 0.09580478072166443

  2%|▏         | 2423/100000 [01:15<50:37, 32.13it/s]
epoch 2400  training loss: 0.0951775312423706


  3%|▎         | 2551/100000 [01:19<50:37, 32.08it/s]
epoch 2500  training loss: 0.09441930800676346

  3%|▎         | 2615/100000 [01:21<50:34, 32.10it/s]
epoch 2600  training loss: 0.09388750046491623


  3%|▎         | 2739/100000 [01:25<58:28, 27.72it/s]
epoch 2700  training loss: 0.09361429512500763

  3%|▎         | 2807/100000 [01:27<50:35, 32.02it/s]
epoch 2800  training loss: 0.09283963590860367


  3%|▎         | 2935/100000 [01:31<50:21, 32.13it/s]
epoch 2900  training loss: 0.09267796576023102

  3%|▎         | 2999/100000 [01:33<50:20, 32.12it/s]
epoch 3000  training loss: 0.09225820004940033
epoch 3000  clean testing loss: 0.008699416182935238


  3%|▎         | 3127/100000 [01:37<50:21, 32.07it/s]
epoch 3100  training loss: 0.09169945865869522


  3%|▎         | 3255/100000 [01:41<50:13, 32.10it/s]
epoch 3200  training loss: 0.0916304886341095

  3%|▎         | 3319/100000 [01:43<50:17, 32.04it/s]
epoch 3300  training loss: 0.09150749444961548


  3%|▎         | 3447/100000 [01:47<50:09, 32.08it/s]
epoch 3400  training loss: 0.09130948781967163

  4%|▎         | 3511/100000 [01:49<50:10, 32.05it/s]
epoch 3500  training loss: 0.09085635840892792


  4%|▎         | 3643/100000 [01:53<50:02, 32.10it/s]
epoch 3600  training loss: 0.09087807685136795

  4%|▎         | 3703/100000 [01:55<58:43, 27.33it/s]
epoch 3700  training loss: 0.0909966453909874


  4%|▍         | 3831/100000 [01:59<50:02, 32.03it/s]
epoch 3800  training loss: 0.09169766306877136


  4%|▍         | 3959/100000 [02:03<49:58, 32.03it/s]
epoch 3900  training loss: 0.0932467058300972

  4%|▍         | 4023/100000 [02:05<50:00, 31.98it/s]
epoch 4000  training loss: 0.09590668976306915
epoch 4000  clean testing loss: 0.008801378309726715


  4%|▍         | 4151/100000 [02:09<49:40, 32.16it/s]
epoch 4100  training loss: 0.09569553285837173

  4%|▍         | 4219/100000 [02:11<49:47, 32.06it/s]
epoch 4200  training loss: 0.09460917860269547


  4%|▍         | 4347/100000 [02:15<49:35, 32.15it/s]
epoch 4300  training loss: 0.0937892496585846

  4%|▍         | 4411/100000 [02:17<49:35, 32.13it/s]
epoch 4400  training loss: 0.09320095181465149


  5%|▍         | 4539/100000 [02:21<49:35, 32.09it/s]
epoch 4500  training loss: 0.09257872402667999

  5%|▍         | 4603/100000 [02:23<49:48, 31.93it/s]
epoch 4600  training loss: 0.09209437668323517


  5%|▍         | 4710/100000 [02:27<49:43, 31.94it/s]
epoch 4700  training loss: 0.09288891404867172


  5%|▍         | 4838/100000 [02:31<49:27, 32.07it/s]
epoch 4800  training loss: 0.09203576296567917

  5%|▍         | 4902/100000 [02:33<49:36, 31.95it/s]
epoch 4900  training loss: 0.09204714000225067


  5%|▌         | 5030/100000 [02:37<49:26, 32.01it/s]
epoch 5000  training loss: 0.09176143258810043
epoch 5000  clean testing loss: 0.008638143539428711


  5%|▌         | 5162/100000 [02:41<49:23, 32.00it/s]
epoch 5100  training loss: 0.0917249545454979

  5%|▌         | 5226/100000 [02:43<49:19, 32.03it/s]
epoch 5200  training loss: 0.09180215746164322


  5%|▌         | 5354/100000 [02:47<49:03, 32.15it/s]
epoch 5300  training loss: 0.09170147776603699

  5%|▌         | 5418/100000 [02:49<49:08, 32.08it/s]
epoch 5400  training loss: 0.0909915417432785


  6%|▌         | 5546/100000 [02:53<48:55, 32.18it/s]
epoch 5500  training loss: 0.09214337915182114

  6%|▌         | 5610/100000 [02:55<49:04, 32.05it/s]
epoch 5600  training loss: 0.09064007550477982


  6%|▌         | 5738/100000 [02:59<48:56, 32.10it/s]
epoch 5700  training loss: 0.09122218936681747

  6%|▌         | 5802/100000 [03:01<49:04, 31.99it/s]
epoch 5800  training loss: 0.0935089960694313


  6%|▌         | 5930/100000 [03:05<48:45, 32.15it/s]
epoch 5900  training loss: 0.09255358576774597


  6%|▌         | 6058/100000 [03:09<48:40, 32.17it/s]
epoch 6000  training loss: 0.09234938770532608
epoch 6000  clean testing loss: 0.008209890685975552

  6%|▌         | 6122/100000 [03:11<48:43, 32.11it/s]
epoch 6100  training loss: 0.09194070100784302


  6%|▋         | 6314/100000 [03:17<48:42, 32.05it/s]
epoch 6200  training loss: 0.0914483517408371
epoch 6200  clean testing loss: 0.008235626854002476
epoch 6300  training loss: 0.0912499725818634


  6%|▋         | 6446/100000 [03:21<48:33, 32.11it/s]
epoch 6400  training loss: 0.09086231887340546

  7%|▋         | 6510/100000 [03:23<48:39, 32.02it/s]
epoch 6500  training loss: 0.09060011804103851


  7%|▋         | 6633/100000 [03:27<48:31, 32.07it/s]
epoch 6600  training loss: 0.0913165807723999

  7%|▋         | 6697/100000 [03:29<48:27, 32.08it/s]
epoch 6700  training loss: 0.09052187204360962


  7%|▋         | 6829/100000 [03:33<48:22, 32.10it/s]
epoch 6800  training loss: 0.09045474976301193


  7%|▋         | 6957/100000 [03:37<48:18, 32.10it/s]
epoch 6900  training loss: 0.09025722742080688

  7%|▋         | 7021/100000 [03:39<48:22, 32.04it/s]
epoch 7000  training loss: 0.09044715017080307
epoch 7000  clean testing loss: 0.008902635425329208


  7%|▋         | 7145/100000 [03:43<48:12, 32.10it/s]
epoch 7100  training loss: 0.09107723087072372

  7%|▋         | 7213/100000 [03:45<48:18, 32.01it/s]
epoch 7200  training loss: 0.09115346521139145


  7%|▋         | 7341/100000 [03:49<48:07, 32.09it/s]
epoch 7300  training loss: 0.0908467248082161

  7%|▋         | 7405/100000 [03:51<48:13, 32.01it/s]
epoch 7400  training loss: 0.09085720777511597


  8%|▊         | 7533/100000 [03:55<48:06, 32.04it/s]
epoch 7500  training loss: 0.09065958112478256


  8%|▊         | 7661/100000 [03:59<48:01, 32.05it/s]
epoch 7600  training loss: 0.0901687741279602

  8%|▊         | 7725/100000 [04:01<47:58, 32.06it/s]
epoch 7700  training loss: 0.09006655961275101


  8%|▊         | 7853/100000 [04:05<47:48, 32.13it/s]
epoch 7800  training loss: 0.09017650038003922

  8%|▊         | 7917/100000 [04:07<47:57, 32.01it/s]
epoch 7900  training loss: 0.08962658792734146


  8%|▊         | 8045/100000 [04:11<47:47, 32.06it/s]
epoch 8000  training loss: 0.09081381559371948
epoch 8000  clean testing loss: 0.009025819599628448

  8%|▊         | 8109/100000 [04:13<47:51, 32.00it/s]
epoch 8100  training loss: 0.09008439630270004


  8%|▊         | 8205/100000 [04:22<1:04:14, 23.82it/s]
epoch 8200  training loss: 0.08984943479299545


  8%|▊         | 8333/100000 [04:26<47:38, 32.07it/s]
epoch 8300  training loss: 0.08996802568435669


  8%|▊         | 8405/100000 [04:28<47:48, 31.93it/s]
epoch 8400  training loss: 0.09029264003038406



  9%|▊         | 8537/100000 [04:35<47:26, 32.14it/s]
epoch 8500  training loss: 0.08928380906581879

  9%|▊         | 8601/100000 [04:37<47:29, 32.08it/s]
epoch 8600  training loss: 0.08955943584442139


  9%|▊         | 8729/100000 [04:41<47:26, 32.06it/s]
epoch 8700  training loss: 0.0891333520412445


  9%|▉         | 8857/100000 [04:45<47:19, 32.09it/s]
epoch 8800  training loss: 0.09110742062330246

  9%|▉         | 8921/100000 [04:47<47:17, 32.10it/s]
epoch 8900  training loss: 0.092519611120224


  9%|▉         | 9053/100000 [04:52<47:14, 32.09it/s]
epoch 9000  training loss: 0.09242463111877441
epoch 9000  clean testing loss: 0.008051827549934387

  9%|▉         | 9117/100000 [04:54<47:16, 32.04it/s]
epoch 9100  training loss: 0.09157641232013702


  9%|▉         | 9240/100000 [04:57<47:19, 31.96it/s]
epoch 9200  training loss: 0.09101139008998871

  9%|▉         | 9308/100000 [05:00<47:18, 31.95it/s]
epoch 9300  training loss: 0.09094454348087311


  9%|▉         | 9436/100000 [05:04<47:04, 32.07it/s]
epoch 9400  training loss: 0.09046588838100433


 10%|▉         | 9564/100000 [05:08<47:00, 32.07it/s]
epoch 9500  training loss: 0.09019089490175247

 10%|▉         | 9628/100000 [05:10<46:58, 32.06it/s]
epoch 9600  training loss: 0.09002966433763504


 10%|▉         | 9756/100000 [05:14<46:54, 32.06it/s]
epoch 9700  training loss: 0.08968953043222427

 10%|▉         | 9820/100000 [05:16<46:54, 32.04it/s]
epoch 9800  training loss: 0.0896262601017952


 10%|▉         | 9948/100000 [05:20<46:48, 32.06it/s]
epoch 9900  training loss: 0.09023087471723557

 10%|█         | 10012/100000 [05:22<47:08, 31.82it/s]
epoch 10000  training loss: 0.0889536440372467
epoch 10000  clean testing loss: 0.009340119548141956


 10%|█         | 10140/100000 [05:26<46:43, 32.05it/s]
epoch 10100  training loss: 0.08945794403553009

 10%|█         | 10204/100000 [05:28<47:05, 31.78it/s]
epoch 10200  training loss: 0.08897791802883148


 10%|█         | 10332/100000 [05:32<46:39, 32.03it/s]
epoch 10300  training loss: 0.08992510288953781


 10%|█         | 10460/100000 [05:36<46:32, 32.07it/s]
epoch 10400  training loss: 0.08918412774801254

 11%|█         | 10524/100000 [05:38<46:32, 32.04it/s]
epoch 10500  training loss: 0.08962797373533249


 11%|█         | 10652/100000 [05:42<46:26, 32.06it/s]
epoch 10600  training loss: 0.08916263282299042

 11%|█         | 10716/100000 [05:44<46:34, 31.95it/s]
epoch 10700  training loss: 0.08890164643526077


 11%|█         | 10844/100000 [05:48<46:19, 32.08it/s]
epoch 10800  training loss: 0.08923404663801193

 11%|█         | 10908/100000 [05:50<46:28, 31.95it/s]
epoch 10900  training loss: 0.08908534049987793


 11%|█         | 11036/100000 [05:54<46:16, 32.04it/s]
epoch 11000  training loss: 0.08916838467121124
epoch 11000  clean testing loss: 0.009189273230731487


 11%|█         | 11163/100000 [05:58<46:26, 31.88it/s]
epoch 11100  training loss: 0.08903757482767105

 11%|█         | 11227/100000 [06:00<46:12, 32.01it/s]
epoch 11200  training loss: 0.08910886198282242


 11%|█▏        | 11355/100000 [06:04<46:07, 32.03it/s]
epoch 11300  training loss: 0.08911092579364777

 11%|█▏        | 11419/100000 [06:06<46:08, 32.00it/s]
epoch 11400  training loss: 0.08874307572841644


 12%|█▏        | 11547/100000 [06:10<45:59, 32.05it/s]
epoch 11500  training loss: 0.08886244893074036

 12%|█▏        | 11611/100000 [06:12<46:04, 31.97it/s]
epoch 11600  training loss: 0.0885072723031044


 12%|█▏        | 11739/100000 [06:16<45:53, 32.05it/s]
epoch 11700  training loss: 0.0883641466498375

 12%|█▏        | 11803/100000 [06:18<46:01, 31.94it/s]
epoch 11800  training loss: 0.08843724429607391


 12%|█▏        | 11931/100000 [06:22<45:48, 32.05it/s]
epoch 11900  training loss: 0.09483585506677628


 12%|█▏        | 12059/100000 [06:26<45:42, 32.06it/s]
epoch 12000  training loss: 0.0968569815158844
epoch 12000  clean testing loss: 0.008692353963851929

 12%|█▏        | 12123/100000 [06:28<46:01, 31.83it/s]
epoch 12100  training loss: 0.09685128182172775


 12%|█▏        | 12211/100000 [06:36<3:04:32,  7.93it/s]
epoch 12200  training loss: 0.0949471965432167


 12%|█▏        | 12339/100000 [06:40<45:24, 32.18it/s]
epoch 12300  training loss: 0.0936630517244339

 12%|█▏        | 12403/100000 [06:42<45:37, 32.00it/s]
epoch 12400  training loss: 0.09363497048616409


 13%|█▎        | 12531/100000 [06:46<45:23, 32.12it/s]
epoch 12500  training loss: 0.0932910218834877


 13%|█▎        | 12663/100000 [06:50<45:18, 32.12it/s]
epoch 12600  training loss: 0.09299984574317932

 13%|█▎        | 12727/100000 [06:52<45:20, 32.07it/s]
epoch 12700  training loss: 0.09292886406183243


 13%|█▎        | 12855/100000 [06:56<45:15, 32.10it/s]
epoch 12800  training loss: 0.0922626405954361

 13%|█▎        | 12915/100000 [06:58<45:49, 31.68it/s]
epoch 12900  training loss: 0.09197700023651123


 13%|█▎        | 13011/100000 [07:02<2:43:58,  8.84it/s]
epoch 13000  training loss: 0.09181445091962814
epoch 13000  clean testing loss: 0.007298365700989962


 13%|█▎        | 13115/100000 [07:05<45:19, 31.94it/s]
epoch 13100  training loss: 0.0916566550731659


 13%|█▎        | 13207/100000 [07:12<45:23, 31.87it/s]
epoch 13200  training loss: 0.09156221896409988



 13%|█▎        | 13324/100000 [07:38<45:24, 31.81it/s]
epoch 13300  training loss: 0.09219519793987274


 13%|█▎        | 13428/100000 [07:41<44:49, 32.18it/s]
epoch 13400  training loss: 0.0911579579114914


 14%|█▎        | 13500/100000 [07:44<45:00, 32.03it/s]
epoch 13500  training loss: 0.09144864976406097



 14%|█▎        | 13668/100000 [07:52<44:36, 32.25it/s]
epoch 13600  training loss: 0.09105902165174484

 14%|█▎        | 13732/100000 [07:54<44:38, 32.21it/s]
epoch 13700  training loss: 0.09082318097352982


 14%|█▍        | 13859/100000 [07:58<45:18, 31.68it/s]
epoch 13800  training loss: 0.09080947190523148

 14%|█▍        | 13923/100000 [08:00<44:34, 32.18it/s]
epoch 13900  training loss: 0.09066866338253021


 14%|█▍        | 14000/100000 [08:02<49:25, 29.00it/s]
epoch 14000  training loss: 0.09072086215019226
epoch 14000  clean testing loss: 0.008094712160527706
Validation loss variation < 1e-6, trained to interpolation, stop
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size10000_noise1.00e-01_invop1 ...