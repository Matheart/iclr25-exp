
  0%|                                                                              | 42/300000 [00:01<1:35:44, 52.21it/s]
epoch 0  training loss: 47.01204299926758
epoch 0  clean testing loss: 41.62567901611328
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+00_invop1 ...
epoch 100  training loss: 1.3442689180374146

  0%|                                                                             | 203/300000 [00:03<1:02:53, 79.45it/s]
epoch 200  training loss: 0.8346222043037415
epoch 200  clean testing loss: 0.14576971530914307
epoch 300  training loss: 0.7717352509498596

  0%|                                                                             | 363/300000 [00:05<1:03:12, 79.00it/s]
epoch 400  training loss: 0.7157784700393677

  0%|▏                                                                            | 524/300000 [00:07<1:03:29, 78.61it/s]
epoch 500  training loss: 0.6889614462852478

  0%|▏                                                                            | 676/300000 [00:09<1:03:14, 78.88it/s]
epoch 600  training loss: 0.666824460029602
epoch 600  clean testing loss: 0.13632725179195404
epoch 700  training loss: 0.6410337090492249

  0%|▏                                                                            | 700/300000 [00:09<1:02:58, 79.21it/s]
epoch 800  training loss: 0.6040222644805908
epoch 800  clean testing loss: 0.1512438952922821
epoch 900  training loss: 0.5891815423965454
epoch 900  clean testing loss: 0.16003091633319855
epoch 1000  training loss: 0.570864737033844
epoch 1000  clean testing loss: 0.1625383049249649
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+00_invop1 ...
epoch 1100  training loss: 0.556399941444397


  0%|▎                                                                           | 1309/300000 [00:24<1:02:50, 79.23it/s]
epoch 1200  training loss: 0.5349323749542236

  0%|▎                                                                           | 1469/300000 [00:26<1:02:56, 79.05it/s]
epoch 1300  training loss: 0.5227450728416443
epoch 1300  clean testing loss: 0.20322883129119873
epoch 1400  training loss: 0.49516457319259644

  1%|▍                                                                           | 1629/300000 [00:28<1:02:55, 79.02it/s]
epoch 1500  training loss: 0.49235308170318604
epoch 1500  clean testing loss: 0.23441456258296967
epoch 1600  training loss: 0.47241073846817017

  1%|▍                                                                           | 1789/300000 [00:30<1:02:59, 78.90it/s]
epoch 1700  training loss: 0.46954965591430664

  1%|▍                                                                           | 1893/300000 [00:31<1:02:53, 79.01it/s]
epoch 1800  training loss: 0.442653626203537
epoch 1800  clean testing loss: 0.2777271270751953
epoch 1900  training loss: 0.44366246461868286

  1%|▌                                                                           | 2144/300000 [00:40<1:04:34, 76.88it/s]
epoch 2000  training loss: 0.473804235458374
epoch 2000  clean testing loss: 0.28499701619148254
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+00_invop1 ...
epoch 2100  training loss: 0.4709898829460144

  1%|▌                                                                           | 2300/300000 [00:42<1:02:12, 79.76it/s]
epoch 2200  training loss: 0.4672324061393738

  1%|▌                                                                           | 2462/300000 [00:44<1:02:38, 79.16it/s]
epoch 2300  training loss: 0.44197696447372437
epoch 2300  clean testing loss: 0.31716206669807434
epoch 2400  training loss: 0.4523419737815857

  1%|▋                                                                           | 2622/300000 [00:46<1:02:29, 79.32it/s]
epoch 2500  training loss: 0.44260770082473755
epoch 2500  clean testing loss: 0.3205147385597229
epoch 2600  training loss: 0.44006291031837463

  1%|▋                                                                           | 2734/300000 [00:47<1:02:35, 79.16it/s]
epoch 2700  training loss: 0.42920681834220886

  1%|▋                                                                           | 2894/300000 [00:49<1:02:34, 79.13it/s]
epoch 2800  training loss: 0.4253140687942505
epoch 2800  clean testing loss: 0.3835424482822418
epoch 2900  training loss: 0.4137727916240692

  1%|▊                                                                           | 3054/300000 [00:51<1:03:27, 77.99it/s]
epoch 3000  training loss: 0.42249149084091187
epoch 3000  clean testing loss: 0.42296308279037476

  1%|▊                                                                           | 3206/300000 [00:53<1:03:09, 78.32it/s]
epoch 3100  training loss: 0.40721434354782104
epoch 3100  clean testing loss: 0.39447021484375
epoch 3200  training loss: 0.4881511330604553

  1%|▊                                                                           | 3369/300000 [00:55<1:02:03, 79.66it/s]
epoch 3300  training loss: 0.4567889869213104

  1%|▉                                                                           | 3530/300000 [00:57<1:02:25, 79.16it/s]
epoch 3400  training loss: 0.42361804842948914
epoch 3400  clean testing loss: 0.40460461378097534
epoch 3500  training loss: 0.4011344313621521

  1%|▉                                                                           | 3690/300000 [00:59<1:02:34, 78.92it/s]
epoch 3600  training loss: 0.40536463260650635
epoch 3600  clean testing loss: 0.4526425004005432
epoch 3700  training loss: 0.3850952386856079

  1%|▉                                                                           | 3794/300000 [01:00<1:02:25, 79.08it/s]

  1%|█                                                                           | 4101/300000 [01:13<1:24:35, 58.30it/s]
epoch 3800  clean testing loss: 0.4765044152736664
epoch 3900  training loss: 0.3516307771205902
epoch 3900  clean testing loss: 0.49825698137283325
epoch 4000  training loss: 0.3788152039051056
epoch 4000  clean testing loss: 0.49015581607818604
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+00_invop1 ...
epoch 4100  training loss: 0.3751455247402191

  1%|█                                                                           | 4200/300000 [01:14<1:02:05, 79.40it/s]
epoch 4200  training loss: 0.4087269604206085

  2%|█▎                                                                          | 5132/300000 [01:27<1:02:26, 78.70it/s]
epoch 4300  training loss: 0.5147886276245117
epoch 4300  clean testing loss: 0.41215795278549194
epoch 4400  training loss: 0.47144654393196106
epoch 4400  clean testing loss: 0.39035776257514954
epoch 4500  training loss: 0.4547400176525116
epoch 4500  clean testing loss: 0.44590041041374207
epoch 4600  training loss: 0.40895771980285645
epoch 4600  clean testing loss: 0.4674074947834015
epoch 4700  training loss: 0.39235249161720276
epoch 4700  clean testing loss: 0.5113250017166138
epoch 4800  training loss: 0.3987099826335907
epoch 4800  clean testing loss: 0.5105369091033936
epoch 4900  training loss: 0.44056227803230286
epoch 4900  clean testing loss: 0.5290049314498901
epoch 5000  training loss: 0.40604037046432495
epoch 5000  clean testing loss: 0.5339651107788086
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+00_invop1 ...
epoch 5100  training loss: 0.38661453127861023

  2%|█▎                                                                          | 5294/300000 [01:29<1:02:33, 78.53it/s]
epoch 5200  training loss: 0.3867869973182678
epoch 5200  clean testing loss: 0.5618360638618469
epoch 5300  training loss: 0.380819171667099

  2%|█▍                                                                          | 5447/300000 [01:31<1:02:02, 79.12it/s]
epoch 5400  training loss: 0.3678908050060272

  2%|█▍                                                                          | 5607/300000 [01:33<1:02:07, 78.97it/s]
epoch 5500  training loss: 0.38960275053977966
epoch 5500  clean testing loss: 0.5618402361869812
epoch 5600  training loss: 0.3562331795692444

  2%|█▍                                                                          | 5769/300000 [01:35<1:01:45, 79.41it/s]
epoch 5700  training loss: 0.35212427377700806

  2%|█▌                                                                          | 5923/300000 [01:37<1:01:52, 79.20it/s]
epoch 5800  training loss: 0.3701278567314148
epoch 5800  clean testing loss: 0.5617141127586365
epoch 5900  training loss: 0.36030858755111694

  2%|█▌                                                                          | 6085/300000 [01:39<1:01:25, 79.75it/s]
epoch 6000  training loss: 0.365646630525589
epoch 6000  clean testing loss: 0.630696713924408
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+00_invop1 ...
epoch 6100  training loss: 0.33552101254463196

  2%|█▌                                                                          | 6245/300000 [01:41<1:01:47, 79.24it/s]
epoch 6200  training loss: 0.3563081920146942

  2%|█▌                                                                          | 6333/300000 [01:42<1:01:40, 79.35it/s]
epoch 6300  training loss: 0.32174786925315857

  2%|█▋                                                                          | 6568/300000 [01:45<1:01:22, 79.68it/s]
epoch 6400  training loss: 0.34424781799316406
epoch 6400  clean testing loss: 0.5980103015899658
epoch 6500  training loss: 0.33089765906333923

  2%|█▋                                                                          | 6721/300000 [01:47<1:01:48, 79.07it/s]
epoch 6600  training loss: 0.35130447149276733
epoch 6600  clean testing loss: 0.5898028612136841
epoch 6700  training loss: 0.3439098596572876

  2%|█▋                                                                          | 6881/300000 [01:49<1:01:46, 79.08it/s]
epoch 6800  training loss: 0.3329954743385315

  2%|█▊                                                                          | 7031/300000 [01:51<1:06:19, 73.62it/s]
epoch 6900  training loss: 0.32885921001434326
epoch 6900  clean testing loss: 0.6587300300598145
epoch 7000  training loss: 0.340681791305542
epoch 7000  clean testing loss: 0.6669886708259583

  2%|█▊                                                                          | 7191/300000 [01:53<1:01:34, 79.26it/s]
epoch 7100  training loss: 0.33366572856903076
epoch 7100  clean testing loss: 0.6278883218765259
epoch 7200  training loss: 0.3349897861480713

  2%|█▊                                                                          | 7353/300000 [01:55<1:01:39, 79.10it/s]
epoch 7300  training loss: 0.32406294345855713

  2%|█▉                                                                          | 7499/300000 [01:57<1:01:23, 79.41it/s]
epoch 7400  training loss: 0.34904977679252625
epoch 7400  clean testing loss: 0.5963646769523621
epoch 7500  training loss: 0.32976144552230835

  3%|█▉                                                                          | 7668/300000 [01:59<1:01:44, 78.90it/s]
epoch 7600  training loss: 0.29674074053764343

  3%|█▉                                                                          | 7829/300000 [02:01<1:01:38, 78.99it/s]
epoch 7700  training loss: 0.3711881935596466
epoch 7700  clean testing loss: 0.6177528500556946
epoch 7800  training loss: 0.31757989525794983

  3%|██                                                                          | 7989/300000 [02:03<1:01:44, 78.82it/s]
epoch 7900  training loss: 0.3383355438709259

  3%|██                                                                          | 8142/300000 [02:05<1:01:18, 79.33it/s]
epoch 8000  training loss: 0.30995386838912964
epoch 8000  clean testing loss: 0.6875919699668884
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+00_invop1 ...
epoch 8100  training loss: 0.3109579086303711

  3%|██                                                                          | 8295/300000 [02:07<1:00:56, 79.78it/s]
epoch 8200  training loss: 0.2866135537624359
epoch 8200  clean testing loss: 0.6952341794967651
epoch 8300  training loss: 0.3056016266345978

  3%|██▏                                                                         | 8459/300000 [02:09<1:00:57, 79.72it/s]
epoch 8400  training loss: 0.3049808442592621
epoch 8400  clean testing loss: 0.6836909651756287
epoch 8500  training loss: 0.3045247197151184

  3%|██▏                                                                         | 8592/300000 [02:11<1:00:47, 79.89it/s]
epoch 8600  training loss: 0.36619478464126587

  3%|██▏                                                                         | 8777/300000 [02:13<1:01:13, 79.29it/s]
epoch 8700  training loss: 0.3608532249927521
epoch 8700  clean testing loss: 0.6834537386894226
epoch 8800  training loss: 0.37804627418518066

  3%|██▎                                                                         | 8937/300000 [02:15<1:01:16, 79.16it/s]
epoch 8900  training loss: 0.3646012544631958

  3%|██▎                                                                         | 9091/300000 [02:17<1:01:16, 79.12it/s]
epoch 9000  training loss: 0.33094197511672974
epoch 9000  clean testing loss: 0.63646000623703
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+00_invop1 ...
epoch 9100  training loss: 0.33950844407081604

  3%|██▎                                                                         | 9099/300000 [02:17<1:01:15, 79.15it/s]
epoch 9200  training loss: 0.31610432267189026

  3%|██▍                                                                         | 9407/300000 [02:21<1:01:01, 79.37it/s]
epoch 9300  training loss: 0.339056134223938
epoch 9300  clean testing loss: 0.6488243341445923
epoch 9400  training loss: 0.31489428877830505

  3%|██▍                                                                         | 9567/300000 [02:23<1:01:05, 79.24it/s]
epoch 9500  training loss: 0.3299294114112854
epoch 9500  clean testing loss: 0.711474597454071
epoch 9600  training loss: 0.33335021138191223

  3%|██▍                                                                         | 9730/300000 [02:25<1:00:32, 79.91it/s]
epoch 9700  training loss: 0.3273293972015381

  3%|██▌                                                                         | 9885/300000 [02:27<1:00:51, 79.45it/s]
epoch 9800  training loss: 0.3883056938648224
epoch 9800  clean testing loss: 0.6818952560424805
epoch 9900  training loss: 0.42288658022880554

  3%|██▌                                                                        | 10045/300000 [02:29<1:01:12, 78.96it/s]
epoch 10000  training loss: 0.37474989891052246
epoch 10000  clean testing loss: 0.6745153069496155

  3%|██▌                                                                        | 10199/300000 [02:31<1:00:43, 79.53it/s]
epoch 10100  training loss: 0.34423574805259705
epoch 10100  clean testing loss: 0.6883707642555237
epoch 10200  training loss: 0.3543795645236969

  4%|██▋                                                                        | 10997/300000 [02:41<1:00:56, 79.04it/s]
epoch 10300  training loss: 0.3598502576351166
epoch 10300  clean testing loss: 0.7110932469367981
epoch 10400  training loss: 0.33446890115737915
epoch 10400  clean testing loss: 0.7468715906143188
epoch 10500  training loss: 0.3177027106285095
epoch 10500  clean testing loss: 0.7099151015281677
epoch 10600  training loss: 0.3331259787082672
epoch 10600  clean testing loss: 0.7732877731323242
epoch 10700  training loss: 0.33754074573516846
epoch 10700  clean testing loss: 0.7279444932937622
epoch 10800  training loss: 0.3143560290336609
epoch 10800  clean testing loss: 0.7422835826873779
epoch 10900  training loss: 0.2898220717906952
epoch 10900  clean testing loss: 0.8009122610092163
epoch 11000  training loss: 0.3416999578475952
epoch 11000  clean testing loss: 0.8327435851097107

  4%|██▊                                                                        | 11158/300000 [02:47<1:01:25, 78.38it/s]
epoch 11100  training loss: 0.3033553957939148
epoch 11100  clean testing loss: 0.768373966217041
epoch 11200  training loss: 0.3184020221233368

  4%|██▊                                                                        | 11317/300000 [02:49<1:00:22, 79.70it/s]
epoch 11300  training loss: 0.32630249857902527

  4%|██▊                                                                        | 11480/300000 [02:51<1:00:16, 79.79it/s]
epoch 11400  training loss: 0.31090253591537476
epoch 11400  clean testing loss: 0.7649778723716736
epoch 11500  training loss: 0.33704543113708496

  4%|██▉                                                                        | 11641/300000 [02:54<1:00:44, 79.12it/s]
epoch 11600  training loss: 0.31583550572395325

  4%|██▉                                                                        | 11797/300000 [02:55<1:00:20, 79.60it/s]
epoch 11700  training loss: 0.3150158226490021
epoch 11700  clean testing loss: 0.806833803653717
epoch 11800  training loss: 0.31991133093833923

  4%|██▉                                                                        | 11953/300000 [02:57<1:00:26, 79.44it/s]
epoch 11900  training loss: 0.2940657138824463
epoch 11900  clean testing loss: 0.7441520094871521
epoch 12000  training loss: 0.2980302572250366
epoch 12000  clean testing loss: 0.758262574672699

  4%|███                                                                        | 12122/300000 [03:00<1:00:22, 79.47it/s]
epoch 12100  training loss: 0.31121116876602173

  4%|███                                                                        | 12278/300000 [03:02<1:00:11, 79.67it/s]
epoch 12200  training loss: 0.3308870792388916
epoch 12200  clean testing loss: 0.7911471724510193
epoch 12300  training loss: 0.30755794048309326

  4%|███                                                                        | 12439/300000 [03:04<1:00:13, 79.59it/s]
epoch 12400  training loss: 0.31439027190208435

  4%|███▏                                                                       | 12594/300000 [03:06<1:00:15, 79.50it/s]
epoch 12500  training loss: 0.27969154715538025
epoch 12500  clean testing loss: 0.8724685311317444
epoch 12600  training loss: 0.293588787317276

  4%|███▏                                                                       | 12757/300000 [03:08<1:00:12, 79.52it/s]
epoch 12700  training loss: 0.2836002707481384

  4%|███▏                                                                       | 12917/300000 [03:10<1:00:19, 79.32it/s]
epoch 12800  training loss: 0.30947402119636536
epoch 12800  clean testing loss: 0.8316738605499268
epoch 12900  training loss: 0.2841649651527405

  4%|███▎                                                                       | 13071/300000 [03:12<1:01:00, 78.39it/s]
epoch 13000  training loss: 0.28513383865356445
epoch 13000  clean testing loss: 0.8552565574645996
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+00_invop1 ...
epoch 13100  training loss: 0.2839861214160919

  4%|███▎                                                                       | 13224/300000 [03:14<1:00:31, 78.97it/s]
epoch 13200  training loss: 0.2710663080215454

  4%|███▎                                                                       | 13386/300000 [03:16<1:00:02, 79.56it/s]
epoch 13300  training loss: 0.27679434418678284
epoch 13300  clean testing loss: 0.8583366870880127
epoch 13400  training loss: 0.2871329188346863

  5%|███▍                                                                       | 13546/300000 [03:18<1:00:20, 79.12it/s]
epoch 13500  training loss: 0.26818740367889404

  5%|███▍                                                                       | 13707/300000 [03:20<1:00:31, 78.83it/s]
epoch 13600  training loss: 0.29496869444847107
epoch 13600  clean testing loss: 0.8842050433158875
epoch 13700  training loss: 0.2942122519016266

  5%|███▍                                                                       | 13859/300000 [03:22<1:00:14, 79.16it/s]
epoch 13800  training loss: 0.26911064982414246
epoch 13800  clean testing loss: 0.832512617111206
epoch 13900  training loss: 0.27973905205726624

  5%|███▌                                                                       | 14019/300000 [03:24<1:02:18, 76.49it/s]
epoch 14000  training loss: 0.2607378661632538
epoch 14000  clean testing loss: 0.8147577047348022

  5%|███▋                                                                         | 14181/300000 [03:26<59:54, 79.51it/s]
epoch 14100  training loss: 0.2560856342315674
epoch 14100  clean testing loss: 0.8132205009460449
epoch 14200  training loss: 0.2554072439670563

  5%|███▌                                                                       | 14333/300000 [03:28<1:00:12, 79.09it/s]
epoch 14300  training loss: 0.27344968914985657

  5%|███▋                                                                         | 14494/300000 [03:30<59:55, 79.41it/s]
epoch 14400  training loss: 0.25282689929008484
epoch 14400  clean testing loss: 0.826336681842804
epoch 14500  training loss: 0.26070693135261536

  5%|███▋                                                                       | 14598/300000 [03:31<1:00:15, 78.95it/s]
epoch 14600  training loss: 0.26058992743492126

  5%|███▊                                                                         | 14974/300000 [03:36<59:56, 79.24it/s]
epoch 14700  training loss: 0.2646484971046448
epoch 14700  clean testing loss: 0.8129047751426697
epoch 14800  training loss: 0.27042776346206665
epoch 14800  clean testing loss: 0.8104538917541504
epoch 14900  training loss: 0.2586805522441864
epoch 14900  clean testing loss: 0.8243110775947571
epoch 15000  training loss: 0.2654421031475067
epoch 15000  clean testing loss: 0.8036581873893738

  5%|███▉                                                                         | 15134/300000 [03:38<59:57, 79.18it/s]
epoch 15100  training loss: 0.2616453468799591

  5%|███▊                                                                       | 15286/300000 [03:40<1:01:03, 77.71it/s]
epoch 15200  training loss: 0.2558887004852295
epoch 15200  clean testing loss: 0.8287574648857117
epoch 15300  training loss: 0.2553006708621979

  5%|███▊                                                                       | 15294/300000 [03:40<1:00:44, 78.13it/s]
epoch 15400  training loss: 0.25872159004211426
epoch 15400  clean testing loss: 0.8072347044944763
epoch 15500  training loss: 0.25979799032211304
epoch 15500  clean testing loss: 0.7821757793426514
epoch 15600  training loss: 0.254329115152359
epoch 15600  clean testing loss: 0.7897095680236816
epoch 15700  training loss: 0.2570156157016754
epoch 15700  clean testing loss: 0.8037714958190918
epoch 15800  training loss: 0.23370492458343506
epoch 15800  clean testing loss: 0.7828289270401001
epoch 15900  training loss: 0.24218355119228363
epoch 15900  clean testing loss: 0.7829482555389404
epoch 16000  training loss: 0.2509952485561371
epoch 16000  clean testing loss: 0.8040564060211182

  5%|████▏                                                                        | 16080/300000 [03:50<59:47, 79.14it/s]
epoch 16100  training loss: 0.23624834418296814
epoch 16100  clean testing loss: 0.7939576506614685
epoch 16200  training loss: 0.24186061322689056

  5%|████▏                                                                        | 16241/300000 [03:52<59:39, 79.28it/s]
epoch 16300  training loss: 0.2620372474193573

  5%|████▏                                                                        | 16394/300000 [03:54<59:27, 79.51it/s]
epoch 16400  training loss: 0.2380182445049286
epoch 16400  clean testing loss: 0.8539237976074219
epoch 16500  training loss: 0.2395000010728836

  6%|████▏                                                                        | 16554/300000 [03:56<59:41, 79.15it/s]
epoch 16600  training loss: 0.21252475678920746
epoch 16600  clean testing loss: 0.8732112646102905
epoch 16700  training loss: 0.2300822138786316

  6%|████▏                                                                      | 16714/300000 [03:58<1:00:17, 78.30it/s]
epoch 16800  training loss: 0.24511344730854034

  6%|████▏                                                                      | 16866/300000 [04:00<1:00:31, 77.98it/s]
epoch 16900  training loss: 0.2558118999004364
epoch 16900  clean testing loss: 0.8178620934486389
epoch 17000  training loss: 0.2541385889053345
epoch 17000  clean testing loss: 0.8165677785873413

  6%|████▎                                                                      | 17026/300000 [04:02<1:00:19, 78.18it/s]
epoch 17100  training loss: 0.2517530620098114

  6%|████▎                                                                      | 17186/300000 [04:04<1:00:03, 78.49it/s]
epoch 17200  training loss: 0.22890430688858032
epoch 17200  clean testing loss: 0.8221961855888367
epoch 17300  training loss: 0.24708594381809235

  6%|████▍                                                                        | 17346/300000 [04:06<59:20, 79.39it/s]
epoch 17400  training loss: 0.25187167525291443

  6%|████▍                                                                        | 17499/300000 [04:08<59:28, 79.16it/s]
epoch 17500  training loss: 0.2412022203207016
epoch 17500  clean testing loss: 0.8678610324859619
epoch 17600  training loss: 0.24690543115139008

  6%|████▌                                                                        | 17659/300000 [04:10<59:53, 78.58it/s]
epoch 17700  training loss: 0.23551779985427856
epoch 17700  clean testing loss: 0.8336755037307739
epoch 17800  training loss: 0.23945572972297668

  6%|████▌                                                                        | 17819/300000 [04:12<59:43, 78.75it/s]
epoch 17900  training loss: 0.24334263801574707

  6%|████▌                                                                        | 17971/300000 [04:14<59:34, 78.89it/s]
epoch 18000  training loss: 0.24977724254131317
epoch 18000  clean testing loss: 0.8260967135429382
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+00_invop1 ...
epoch 18100  training loss: 0.2374138981103897

  6%|████▋                                                                        | 18131/300000 [04:16<59:23, 79.09it/s]
epoch 18200  training loss: 0.23844222724437714

  6%|████▋                                                                        | 18291/300000 [04:18<59:24, 79.03it/s]
epoch 18300  training loss: 0.24728284776210785


  6%|████▊                                                                        | 18651/300000 [04:22<59:29, 78.83it/s]
epoch 18400  training loss: 0.24606400728225708
epoch 18400  clean testing loss: 0.867485523223877
epoch 18500  training loss: 0.2466035783290863
epoch 18500  clean testing loss: 0.8320092558860779
epoch 18600  training loss: 0.2550857961177826

  6%|████▊                                                                        | 18803/300000 [04:24<59:11, 79.17it/s]
epoch 18700  training loss: 0.23821422457695007

  6%|████▊                                                                        | 18963/300000 [04:26<59:04, 79.28it/s]
epoch 18800  training loss: 0.23031674325466156
epoch 18800  clean testing loss: 0.827268123626709
epoch 18900  training loss: 0.2263525277376175
epoch 18900  clean testing loss: 0.8208180665969849
epoch 19000  training loss: 0.23765553534030914
epoch 19000  clean testing loss: 0.808673083782196
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+00_invop1 ...
epoch 19100  training loss: 0.22697713971138


  6%|████▉                                                                        | 19283/300000 [04:30<59:14, 78.98it/s]
epoch 19200  training loss: 0.25085097551345825

  6%|████▉                                                                        | 19443/300000 [04:32<59:19, 78.82it/s]
epoch 19300  training loss: 0.2555980980396271
epoch 19300  clean testing loss: 0.8139045238494873
epoch 19400  training loss: 0.25619298219680786

  7%|█████                                                                        | 19595/300000 [04:34<59:10, 78.99it/s]
epoch 19500  training loss: 0.2694038152694702
epoch 19500  clean testing loss: 0.8036991953849792
epoch 19600  training loss: 0.27155739068984985
epoch 19600  clean testing loss: 0.8679393529891968
epoch 19700  training loss: 0.2832503914833069


  7%|█████                                                                        | 19915/300000 [04:38<58:57, 79.17it/s]
epoch 19800  training loss: 0.26904791593551636
epoch 19800  clean testing loss: 0.8897964954376221
epoch 19900  training loss: 0.2625134587287903

  7%|█████▏                                                                       | 20075/300000 [04:40<59:04, 78.97it/s]
epoch 20000  training loss: 0.25367075204849243
epoch 20000  clean testing loss: 0.9131130576133728

  7%|█████▏                                                                       | 20235/300000 [04:42<59:05, 78.90it/s]
epoch 20100  training loss: 0.2245367169380188
epoch 20100  clean testing loss: 0.8970060348510742
epoch 20200  training loss: 0.2475205659866333

  7%|█████▏                                                                       | 20387/300000 [04:44<59:02, 78.92it/s]
epoch 20300  training loss: 0.2719469368457794

  7%|█████▎                                                                       | 20547/300000 [04:46<59:18, 78.53it/s]
epoch 20400  training loss: 0.24302265048027039
epoch 20400  clean testing loss: 0.9503923058509827
epoch 20500  training loss: 0.2620600163936615

  7%|█████▎                                                                       | 20707/300000 [04:48<59:17, 78.51it/s]
epoch 20600  training loss: 0.2645440697669983

  7%|█████▎                                                                       | 20868/300000 [04:50<58:55, 78.95it/s]
epoch 20700  training loss: 0.2531757652759552
epoch 20700  clean testing loss: 0.9326337575912476
epoch 20800  training loss: 0.2614286541938782

  7%|█████▍                                                                       | 21028/300000 [04:52<59:18, 78.40it/s]
epoch 20900  training loss: 0.26304543018341064
epoch 20900  clean testing loss: 0.9546352624893188
epoch 21000  training loss: 0.2522181570529938
epoch 21000  clean testing loss: 0.8513225317001343

  7%|█████▍                                                                       | 21180/300000 [04:54<58:50, 78.97it/s]
epoch 21100  training loss: 0.23603831231594086

  7%|█████▍                                                                       | 21196/300000 [04:55<58:57, 78.82it/s]
epoch 21200  training loss: 0.2428506463766098
epoch 21200  clean testing loss: 0.8895668387413025
epoch 21300  training loss: 0.28387269377708435
epoch 21300  clean testing loss: 0.8559451103210449
epoch 21400  training loss: 0.28933870792388916
epoch 21400  clean testing loss: 0.811191737651825
epoch 21500  training loss: 0.2765324115753174
epoch 21500  clean testing loss: 0.8130354285240173
epoch 21600  training loss: 0.303707093000412
epoch 21600  clean testing loss: 0.800025999546051
epoch 21700  training loss: 0.2913009524345398


  7%|█████▋                                                                       | 21974/300000 [05:04<58:16, 79.52it/s]
epoch 21800  training loss: 0.2839352488517761
epoch 21800  clean testing loss: 0.8071917295455933
epoch 21900  training loss: 0.2938448488712311

  7%|█████▋                                                                       | 22134/300000 [05:06<58:46, 78.80it/s]
epoch 22000  training loss: 0.2794874906539917
epoch 22000  clean testing loss: 0.8258823752403259
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+00_invop1 ...
epoch 22100  training loss: 0.2974745035171509

  7%|█████▋                                                                       | 22294/300000 [05:09<58:51, 78.63it/s]
epoch 22200  training loss: 0.2736057937145233

  7%|█████▊                                                                       | 22446/300000 [05:10<58:44, 78.75it/s]
epoch 22300  training loss: 0.2653408944606781
epoch 22300  clean testing loss: 0.8184516429901123
epoch 22400  training loss: 0.26820066571235657

  8%|█████▊                                                                       | 22606/300000 [05:12<58:54, 78.48it/s]
epoch 22500  training loss: 0.2751980721950531

  8%|█████▊                                                                       | 22766/300000 [05:15<58:45, 78.64it/s]
epoch 22600  training loss: 0.26858606934547424
epoch 22600  clean testing loss: 0.837468147277832
epoch 22700  training loss: 0.2827519178390503

  8%|█████▉                                                                       | 22926/300000 [05:17<58:30, 78.93it/s]
epoch 22800  training loss: 0.26825740933418274
epoch 22800  clean testing loss: 0.7833375334739685
epoch 22900  training loss: 0.2728859484195709

  8%|█████▉                                                                       | 23086/300000 [05:19<58:25, 78.99it/s]
epoch 23000  training loss: 0.26129910349845886
epoch 23000  clean testing loss: 0.8078526854515076

  8%|█████▉                                                                       | 23238/300000 [05:20<58:12, 79.24it/s]
epoch 23100  training loss: 0.26294440031051636
epoch 23100  clean testing loss: 0.8271105885505676
epoch 23200  training loss: 0.2633878290653229

  8%|██████                                                                       | 23398/300000 [05:23<58:19, 79.03it/s]
epoch 23300  training loss: 0.25266507267951965

  8%|██████                                                                       | 23558/300000 [05:25<58:15, 79.09it/s]
epoch 23400  training loss: 0.23176877200603485
epoch 23400  clean testing loss: 0.8124446272850037
epoch 23500  training loss: 0.23771269619464874

  8%|██████                                                                       | 23719/300000 [05:27<58:11, 79.12it/s]
epoch 23600  training loss: 0.25131213665008545

  8%|██████▏                                                                      | 23874/300000 [05:29<58:18, 78.94it/s]
epoch 23700  training loss: 0.25857266783714294
epoch 23700  clean testing loss: 0.8258993029594421
epoch 23800  training loss: 0.24134761095046997

  8%|██████▏                                                                      | 24036/300000 [05:31<57:50, 79.51it/s]
epoch 23900  training loss: 0.24068528413772583
epoch 23900  clean testing loss: 0.8796150088310242
epoch 24000  training loss: 0.2614327371120453
epoch 24000  clean testing loss: 0.844043493270874

  8%|██████▏                                                                      | 24196/300000 [05:33<57:58, 79.29it/s]
epoch 24100  training loss: 0.2704979181289673

  8%|██████▎                                                                      | 24356/300000 [05:35<58:07, 79.03it/s]
epoch 24200  training loss: 0.25365009903907776
epoch 24200  clean testing loss: 0.8420422673225403
epoch 24300  training loss: 0.25412696599960327

  8%|██████▎                                                                      | 24509/300000 [05:37<58:08, 78.96it/s]
epoch 24400  training loss: 0.2524435818195343
epoch 24400  clean testing loss: 0.828840970993042
epoch 24500  training loss: 0.2735182046890259
epoch 24500  clean testing loss: 0.8487107157707214
epoch 24600  training loss: 0.2565508186817169

  8%|██████▎                                                                      | 24669/300000 [05:39<58:04, 79.02it/s]
epoch 24700  training loss: 0.234756737947464
epoch 24700  clean testing loss: 0.8873011469841003
epoch 24800  training loss: 0.2406320869922638

  8%|██████▎                                                                      | 24829/300000 [05:41<58:02, 79.02it/s]
epoch 24900  training loss: 0.23449048399925232

  8%|██████▍                                                                      | 24990/300000 [05:43<57:53, 79.18it/s]
epoch 25000  training loss: 0.2694556713104248
epoch 25000  clean testing loss: 0.8684870004653931
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+00_invop1 ...
epoch 25100  training loss: 0.2482563853263855

  8%|██████▍                                                                      | 25142/300000 [05:45<57:52, 79.16it/s]
epoch 25200  training loss: 0.23796996474266052


  8%|██████▌                                                                      | 25462/300000 [05:49<57:57, 78.95it/s]
epoch 25300  training loss: 0.22893506288528442
epoch 25300  clean testing loss: 0.8550333380699158
epoch 25400  training loss: 0.23666293919086456

  9%|██████▌                                                                      | 25598/300000 [05:50<57:46, 79.15it/s]
epoch 25500  training loss: 0.2386203110218048

  9%|██████▌                                                                      | 25783/300000 [05:53<57:32, 79.42it/s]
epoch 25600  training loss: 0.2558079957962036
epoch 25600  clean testing loss: 0.8201570510864258
epoch 25700  training loss: 0.23011817038059235

  9%|██████▋                                                                      | 25896/300000 [05:54<57:34, 79.34it/s]
epoch 25800  training loss: 0.24210862815380096
epoch 25800  clean testing loss: 0.8279885053634644
epoch 25900  training loss: 0.23652863502502441

  9%|██████▌                                                                    | 26041/300000 [05:57<1:19:14, 57.62it/s]
epoch 26000  training loss: 0.2386236935853958
epoch 26000  clean testing loss: 0.7969521880149841
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+00_invop1 ...
epoch 26100  training loss: 0.2260584682226181


  9%|██████▊                                                                      | 26314/300000 [06:00<57:37, 79.17it/s]
epoch 26200  training loss: 0.24286463856697083
epoch 26200  clean testing loss: 0.8469109535217285
epoch 26300  training loss: 0.2444029301404953

  9%|██████▊                                                                      | 26396/300000 [06:01<57:22, 79.47it/s]
epoch 26400  training loss: 0.23953761160373688

  9%|██████▊                                                                      | 26717/300000 [06:05<57:44, 78.88it/s]
epoch 26500  training loss: 0.23536306619644165
epoch 26500  clean testing loss: 0.7889651656150818
epoch 26600  training loss: 0.23794637620449066
epoch 26600  clean testing loss: 0.8061102032661438
epoch 26700  training loss: 0.24349068105220795

  9%|██████▉                                                                      | 26869/300000 [06:07<57:38, 78.98it/s]
epoch 26800  training loss: 0.2525678873062134

  9%|██████▉                                                                      | 27029/300000 [06:09<57:54, 78.58it/s]
epoch 26900  training loss: 0.25353801250457764
epoch 26900  clean testing loss: 0.8134192228317261
epoch 27000  training loss: 0.22912900149822235
epoch 27000  clean testing loss: 0.8401294350624084

  9%|██████▉                                                                      | 27189/300000 [06:11<57:33, 79.00it/s]
epoch 27100  training loss: 0.25391700863838196

  9%|███████                                                                      | 27349/300000 [06:13<57:31, 78.98it/s]
epoch 27200  training loss: 0.2585825026035309
epoch 27200  clean testing loss: 0.8486318588256836
epoch 27300  training loss: 0.2316102236509323

  9%|███████                                                                      | 27509/300000 [06:15<57:28, 79.02it/s]
epoch 27400  training loss: 0.23868396878242493
epoch 27400  clean testing loss: 0.8532680869102478
epoch 27500  training loss: 0.22509321570396423

  9%|███████                                                                      | 27661/300000 [06:17<57:19, 79.18it/s]
epoch 27600  training loss: 0.23454438149929047

  9%|███████▏                                                                     | 27821/300000 [06:19<57:20, 79.12it/s]
epoch 27700  training loss: 0.23560255765914917
epoch 27700  clean testing loss: 0.8366138935089111
epoch 27800  training loss: 0.2364126741886139

  9%|███████▏                                                                     | 27981/300000 [06:21<57:28, 78.87it/s]
epoch 27900  training loss: 0.2351812869310379

  9%|███████▏                                                                     | 28141/300000 [06:23<57:30, 78.78it/s]
epoch 28000  training loss: 0.2032722532749176
epoch 28000  clean testing loss: 0.8257678747177124
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+00_invop1 ...
epoch 28100  training loss: 0.23768825829029083

  9%|███████▏                                                                     | 28197/300000 [06:24<57:27, 78.84it/s]
epoch 28200  training loss: 0.2212897688150406
epoch 28200  clean testing loss: 0.81494140625
epoch 28300  training loss: 0.24846060574054718
epoch 28300  clean testing loss: 0.8468314409255981
epoch 28400  training loss: 0.23347245156764984
epoch 28400  clean testing loss: 0.8375036120414734
epoch 28500  training loss: 0.22209692001342773
epoch 28500  clean testing loss: 0.8167912364006042
epoch 28600  training loss: 0.21973319351673126
epoch 28600  clean testing loss: 0.8353155851364136
epoch 28700  training loss: 0.21975062787532806
epoch 28700  clean testing loss: 0.8232934474945068
epoch 28800  training loss: 0.23847317695617676

 10%|███████▍                                                                     | 28933/300000 [06:33<57:22, 78.74it/s]
epoch 28900  training loss: 0.23674295842647552
epoch 28900  clean testing loss: 0.8421609401702881
epoch 29000  training loss: 0.24187186360359192
epoch 29000  clean testing loss: 0.8391591310501099

 10%|███████▍                                                                     | 29093/300000 [06:35<57:25, 78.63it/s]
epoch 29100  training loss: 0.24108915030956268
epoch 29100  clean testing loss: 0.8516333699226379
epoch 29200  training loss: 0.262168288230896

 10%|███████▌                                                                     | 29245/300000 [06:37<57:12, 78.89it/s]
epoch 29300  training loss: 0.27585774660110474

 10%|███████▌                                                                     | 29405/300000 [06:39<57:19, 78.66it/s]
epoch 29400  training loss: 0.27566060423851013
epoch 29400  clean testing loss: 0.8554935455322266
epoch 29500  training loss: 0.26537734270095825

 10%|███████▌                                                                     | 29565/300000 [06:41<56:49, 79.31it/s]
epoch 29600  training loss: 0.2827698588371277

 10%|███████▋                                                                     | 29725/300000 [06:43<57:09, 78.81it/s]
epoch 29700  training loss: 0.26064810156822205
epoch 29700  clean testing loss: 0.8658775687217712
epoch 29800  training loss: 0.2693553566932678

 10%|███████▋                                                                     | 29885/300000 [06:45<57:11, 78.72it/s]
epoch 29900  training loss: 0.25961023569107056
epoch 29900  clean testing loss: 0.7923036217689514
epoch 30000  training loss: 0.2611951529979706
epoch 30000  clean testing loss: 0.7860727906227112

 10%|███████▋                                                                     | 30038/300000 [06:47<57:19, 78.49it/s]
epoch 30100  training loss: 0.24518606066703796

 10%|███████▊                                                                     | 30198/300000 [06:49<56:59, 78.89it/s]
epoch 30200  training loss: 0.25031542778015137
epoch 30200  clean testing loss: 0.8249866962432861
epoch 30300  training loss: 0.26267269253730774

 10%|███████▊                                                                     | 30359/300000 [06:51<56:27, 79.60it/s]
epoch 30400  training loss: 0.2570941746234894

 10%|███████▊                                                                     | 30496/300000 [06:53<56:42, 79.20it/s]
epoch 30500  training loss: 0.24260777235031128

 10%|███████▉                                                                     | 30833/300000 [06:57<56:47, 78.99it/s]
epoch 30600  training loss: 0.2482435405254364
epoch 30600  clean testing loss: 0.8124929666519165
epoch 30700  training loss: 0.24572335183620453
epoch 30700  clean testing loss: 0.8165580034255981
epoch 30800  training loss: 0.2555515170097351
epoch 30800  clean testing loss: 0.8363982439041138
epoch 30900  training loss: 0.2670449912548065

 10%|███████▉                                                                     | 30994/300000 [06:59<56:34, 79.26it/s]
epoch 31000  training loss: 0.2684485614299774
epoch 31000  clean testing loss: 0.8103124499320984

 10%|███████▉                                                                     | 31154/300000 [07:01<56:42, 79.01it/s]
epoch 31100  training loss: 0.2773318588733673
epoch 31100  clean testing loss: 0.7922528386116028
epoch 31200  training loss: 0.26676955819129944

 10%|████████                                                                     | 31314/300000 [07:03<56:53, 78.71it/s]
epoch 31300  training loss: 0.269349604845047

 10%|████████                                                                     | 31466/300000 [07:05<56:43, 78.89it/s]
epoch 31400  training loss: 0.259176105260849
epoch 31400  clean testing loss: 0.7893281579017639
epoch 31500  training loss: 0.24017955362796783

 11%|████████                                                                     | 31626/300000 [07:07<56:44, 78.82it/s]
epoch 31600  training loss: 0.24264876544475555
epoch 31600  clean testing loss: 0.8144572973251343
epoch 31700  training loss: 0.25928694009780884

 11%|████████▏                                                                    | 31786/300000 [07:09<56:37, 78.95it/s]
epoch 31800  training loss: 0.2538471221923828

 11%|████████▏                                                                    | 31946/300000 [07:11<56:21, 79.26it/s]
epoch 31900  training loss: 0.25441980361938477
epoch 31900  clean testing loss: 0.8487688899040222
epoch 32000  training loss: 0.24992622435092926
epoch 32000  clean testing loss: 0.860632061958313

 11%|████████▏                                                                    | 32106/300000 [07:13<56:53, 78.47it/s]
epoch 32100  training loss: 0.23141172528266907

 11%|████████▎                                                                    | 32258/300000 [07:15<56:30, 78.96it/s]
epoch 32200  training loss: 0.24094341695308685
epoch 32200  clean testing loss: 0.8866041898727417
epoch 32300  training loss: 0.24554747343063354

 11%|████████▎                                                                    | 32418/300000 [07:17<56:20, 79.16it/s]
epoch 32400  training loss: 0.24399757385253906

 11%|████████▎                                                                    | 32578/300000 [07:19<56:08, 79.40it/s]
epoch 32500  training loss: 0.270513117313385
epoch 32500  clean testing loss: 0.859982430934906
epoch 32600  training loss: 0.2529565691947937

 11%|████████▍                                                                    | 32699/300000 [07:21<56:36, 78.69it/s]
epoch 32700  training loss: 0.25391775369644165
epoch 32700  clean testing loss: 0.8459957838058472
epoch 32800  training loss: 0.24901151657104492

 11%|████████▍                                                                    | 32859/300000 [07:23<56:46, 78.43it/s]
epoch 32900  training loss: 0.2333209216594696

 11%|████████▍                                                                    | 33011/300000 [07:25<56:54, 78.19it/s]
epoch 33000  training loss: 0.240568608045578
epoch 33000  clean testing loss: 0.8245837092399597
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+00_invop1 ...
epoch 33100  training loss: 0.24264530837535858

 11%|████████▌                                                                    | 33171/300000 [07:27<56:33, 78.62it/s]
epoch 33200  training loss: 0.25740644335746765


 11%|████████▌                                                                    | 33299/300000 [07:29<56:14, 79.03it/s]
epoch 33300  training loss: 0.26453089714050293

 11%|████████▌                                                                  | 34079/300000 [07:45<1:39:56, 44.35it/s]
epoch 33400  training loss: 0.25877881050109863
epoch 33400  clean testing loss: 0.8318074941635132
epoch 33500  training loss: 0.26175251603126526
epoch 33500  clean testing loss: 0.8286163806915283
epoch 33600  training loss: 0.23543567955493927
epoch 33600  clean testing loss: 0.8544408082962036
epoch 33700  training loss: 0.23494434356689453
epoch 33700  clean testing loss: 0.8448963761329651
epoch 33800  training loss: 0.23225224018096924
epoch 33800  clean testing loss: 0.8327638506889343
epoch 33900  training loss: 0.23647557199001312
epoch 33900  clean testing loss: 0.8041616678237915
epoch 34000  training loss: 0.24289098381996155
epoch 34000  clean testing loss: 0.8012500405311584
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+00_invop1 ...
epoch 34100  training loss: 0.24524365365505219

 11%|████████▊                                                                    | 34240/300000 [07:47<55:47, 79.39it/s]
epoch 34200  training loss: 0.23587259650230408
epoch 34200  clean testing loss: 0.8111869692802429
epoch 34300  training loss: 0.2504448890686035

 11%|████████▊                                                                    | 34401/300000 [07:49<56:01, 79.02it/s]
epoch 34400  training loss: 0.21815992891788483

 12%|████████▊                                                                    | 34553/300000 [07:51<56:19, 78.54it/s]
epoch 34500  training loss: 0.23149752616882324
epoch 34500  clean testing loss: 0.8113903999328613
epoch 34600  training loss: 0.2438788115978241

 12%|████████▉                                                                    | 34714/300000 [07:53<56:07, 78.78it/s]
epoch 34700  training loss: 0.25851354002952576

 12%|████████▉                                                                    | 34874/300000 [07:55<56:01, 78.88it/s]
epoch 34800  training loss: 0.24080170691013336
epoch 34800  clean testing loss: 0.820163369178772
epoch 34900  training loss: 0.2429293543100357

 12%|████████▉                                                                    | 35034/300000 [07:57<55:57, 78.92it/s]
epoch 35000  training loss: 0.2414536029100418
epoch 35000  clean testing loss: 0.8277587294578552
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+00_invop1 ...
epoch 35100  training loss: 0.23533152043819427

 12%|█████████                                                                    | 35098/300000 [07:58<55:40, 79.31it/s]
epoch 35200  training loss: 0.2417028546333313
epoch 35200  clean testing loss: 0.832457423210144
epoch 35300  training loss: 0.24110054969787598
epoch 35300  clean testing loss: 0.835765540599823
epoch 35400  training loss: 0.22358374297618866

 12%|█████████                                                                    | 35466/300000 [08:03<55:52, 78.91it/s]
epoch 35500  training loss: 0.24387986958026886

 12%|█████████▏                                                                   | 35626/300000 [08:05<55:45, 79.03it/s]
epoch 35600  training loss: 0.24573396146297455
epoch 35600  clean testing loss: 0.8294954895973206
epoch 35700  training loss: 0.24143244326114655

 12%|█████████▏                                                                   | 35786/300000 [08:07<55:41, 79.06it/s]
epoch 35800  training loss: 0.2448015660047531

 12%|█████████▏                                                                   | 35947/300000 [08:09<55:49, 78.84it/s]
epoch 35900  training loss: 0.2564227879047394
epoch 35900  clean testing loss: 0.8318691253662109
epoch 36000  training loss: 0.2609604001045227
epoch 36000  clean testing loss: 0.854369044303894

 12%|█████████▎                                                                   | 36099/300000 [08:11<55:37, 79.06it/s]
epoch 36100  training loss: 0.22819896042346954
epoch 36100  clean testing loss: 0.839406430721283
epoch 36200  training loss: 0.23290333151817322


 12%|█████████▍                                                                   | 36815/300000 [08:20<55:37, 78.85it/s]
epoch 36300  training loss: 0.22929462790489197
epoch 36300  clean testing loss: 0.8792570233345032
epoch 36400  training loss: 0.22378294169902802
epoch 36400  clean testing loss: 0.8814009428024292
epoch 36500  training loss: 0.2251933366060257
epoch 36500  clean testing loss: 0.8739531636238098
epoch 36600  training loss: 0.22326794266700745
epoch 36600  clean testing loss: 0.8979964852333069
epoch 36700  training loss: 0.22930210828781128
epoch 36700  clean testing loss: 0.8755354285240173
epoch 36800  training loss: 0.2286052405834198

 12%|█████████▍                                                                   | 36975/300000 [08:22<55:33, 78.91it/s]
epoch 36900  training loss: 0.23865503072738647
epoch 36900  clean testing loss: 0.8694400191307068
epoch 37000  training loss: 0.207819864153862
epoch 37000  clean testing loss: 0.8779692053794861

 12%|█████████▌                                                                   | 37135/300000 [08:24<55:33, 78.86it/s]
epoch 37100  training loss: 0.23674927651882172

 12%|█████████▌                                                                   | 37295/300000 [08:26<55:35, 78.76it/s]
epoch 37200  training loss: 0.22129172086715698
epoch 37200  clean testing loss: 0.8605213165283203
epoch 37300  training loss: 0.22107569873332977

 12%|█████████▌                                                                   | 37455/300000 [08:28<55:14, 79.22it/s]
epoch 37400  training loss: 0.21711575984954834

 12%|█████████▌                                                                   | 37495/300000 [08:28<55:23, 78.99it/s]
epoch 37500  training loss: 0.23176494240760803

 13%|█████████▋                                                                   | 37767/300000 [08:32<55:17, 79.04it/s]
epoch 37600  training loss: 0.20864982903003693
epoch 37600  clean testing loss: 0.8612627387046814
epoch 37700  training loss: 0.21681909263134003

 13%|█████████▋                                                                   | 37927/300000 [08:34<55:10, 79.16it/s]
epoch 37800  training loss: 0.2229394167661667
epoch 37800  clean testing loss: 0.8902729749679565
epoch 37900  training loss: 0.22395269572734833

 13%|█████████▊                                                                   | 38087/300000 [08:36<55:24, 78.78it/s]
epoch 38000  training loss: 0.2319995015859604
epoch 38000  clean testing loss: 0.9043811559677124
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+00_invop1 ...
epoch 38100  training loss: 0.22320477664470673

 13%|█████████▊                                                                   | 38247/300000 [08:38<55:11, 79.03it/s]
epoch 38200  training loss: 0.23333904147148132

 13%|█████████▊                                                                   | 38399/300000 [08:40<55:16, 78.88it/s]
epoch 38300  training loss: 0.21659019589424133
epoch 38300  clean testing loss: 0.9001559615135193
epoch 38400  training loss: 0.2194392830133438

 13%|█████████▉                                                                   | 38560/300000 [08:42<55:02, 79.17it/s]
epoch 38500  training loss: 0.22200742363929749

 13%|█████████▉                                                                   | 38600/300000 [08:42<55:01, 79.17it/s]
epoch 38600  training loss: 0.24134992063045502

 13%|█████████▉                                                                   | 38882/300000 [08:46<55:03, 79.05it/s]
epoch 38700  training loss: 0.23791490495204926
epoch 38700  clean testing loss: 0.8937024474143982
epoch 38800  training loss: 0.23333698511123657

 13%|██████████                                                                   | 39035/300000 [08:48<55:18, 78.64it/s]
epoch 38900  training loss: 0.23287475109100342
epoch 38900  clean testing loss: 0.8695799708366394
epoch 39000  training loss: 0.23796729743480682
epoch 39000  clean testing loss: 0.8795010447502136

 13%|██████████                                                                   | 39195/300000 [08:50<55:10, 78.78it/s]
epoch 39100  training loss: 0.21724845468997955
epoch 39100  clean testing loss: 0.8836448192596436
epoch 39200  training loss: 0.21966664493083954

 13%|██████████                                                                   | 39355/300000 [08:52<54:58, 79.01it/s]
epoch 39300  training loss: 0.23435431718826294

 13%|██████████▏                                                                  | 39515/300000 [08:54<55:03, 78.86it/s]
epoch 39400  training loss: 0.22326749563217163
epoch 39400  clean testing loss: 0.8961983323097229
epoch 39500  training loss: 0.21290946006774902

 13%|██████████▏                                                                  | 39667/300000 [08:56<54:38, 79.41it/s]
epoch 39600  training loss: 0.21135762333869934

 13%|██████████▏                                                                  | 39796/300000 [08:57<54:47, 79.15it/s]
epoch 39700  training loss: 0.20566055178642273
epoch 39700  clean testing loss: 0.8893958926200867
epoch 39800  training loss: 0.21781690418720245

epoch 39800  clean testing loss: 0.8914319276809692
epoch 39900  training loss: 0.20157739520072937
epoch 39900  clean testing loss: 0.8826853036880493
epoch 40000  training loss: 0.23233701288700104
epoch 40000  clean testing loss: 0.8862789869308472
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+00_invop1 ...
epoch 40100  training loss: 0.20810174942016602

 13%|██████████▎                                                                  | 40269/300000 [09:12<54:42, 79.13it/s]
epoch 40200  training loss: 0.21713992953300476

 13%|██████████▍                                                                  | 40429/300000 [09:14<54:34, 79.26it/s]
epoch 40300  training loss: 0.21166260540485382
epoch 40300  clean testing loss: 0.9023249745368958
epoch 40400  training loss: 0.20958031713962555

 14%|██████████▍                                                                  | 40590/300000 [09:16<54:28, 79.37it/s]
epoch 40500  training loss: 0.20598545670509338
epoch 40500  clean testing loss: 0.9045864343643188
epoch 40600  training loss: 0.21367987990379333

 14%|██████████▍                                                                  | 40751/300000 [09:18<54:34, 79.18it/s]
epoch 40700  training loss: 0.2085612565279007

 14%|██████████▌                                                                  | 40911/300000 [09:20<54:43, 78.91it/s]
epoch 40800  training loss: 0.19346913695335388
epoch 40800  clean testing loss: 0.8926092982292175
epoch 40900  training loss: 0.20291092991828918

 14%|██████████▌                                                                  | 41063/300000 [09:22<54:42, 78.90it/s]
epoch 41000  training loss: 0.21650877594947815
epoch 41000  clean testing loss: 0.9071760773658752

 14%|██████████▌                                                                  | 41095/300000 [09:22<54:41, 78.89it/s]
epoch 41100  training loss: 0.21082577109336853

 14%|██████████▋                                                                  | 41420/300000 [09:26<54:21, 79.28it/s]
epoch 41200  training loss: 0.20835360884666443
epoch 41200  clean testing loss: 0.9055047035217285
epoch 41300  training loss: 0.21026962995529175
epoch 41300  clean testing loss: 0.8980556726455688
epoch 41400  training loss: 0.20934660732746124

 14%|██████████▋                                                                  | 41494/300000 [09:27<54:03, 79.69it/s]
epoch 41500  training loss: 0.2086808830499649

 14%|██████████▋                                                                  | 41738/300000 [09:30<54:39, 78.74it/s]
epoch 41600  training loss: 0.23634043335914612
epoch 41600  clean testing loss: 0.8849411606788635
epoch 41700  training loss: 0.20811475813388824
epoch 41700  clean testing loss: 0.9005295038223267
epoch 41800  training loss: 0.22503775358200073

 14%|██████████▊                                                                  | 41899/300000 [09:32<54:19, 79.19it/s]
epoch 41900  training loss: 0.21516190469264984

 14%|██████████▊                                                                  | 42059/300000 [09:34<54:41, 78.62it/s]
epoch 42000  training loss: 0.2084764838218689
epoch 42000  clean testing loss: 0.909892737865448
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+00_invop1 ...
epoch 42100  training loss: 0.21188288927078247

 14%|██████████▊                                                                  | 42220/300000 [09:36<54:15, 79.18it/s]
epoch 42200  training loss: 0.20517122745513916
epoch 42200  clean testing loss: 0.8910899758338928
epoch 42300  training loss: 0.21557362377643585

 14%|██████████▉                                                                  | 42376/300000 [09:38<53:44, 79.89it/s]
epoch 42400  training loss: 0.2189251184463501

 14%|██████████▉                                                                  | 42536/300000 [09:40<54:26, 78.82it/s]
epoch 42500  training loss: 0.22629627585411072
epoch 42500  clean testing loss: 0.8839191794395447
epoch 42600  training loss: 0.23001419007778168

 14%|██████████▉                                                                  | 42696/300000 [09:42<54:19, 78.95it/s]
epoch 42700  training loss: 0.23494867980480194

 14%|██████████▉                                                                  | 42856/300000 [09:44<54:29, 78.64it/s]
epoch 42800  training loss: 0.23408205807209015
epoch 42800  clean testing loss: 0.8912305235862732
epoch 42900  training loss: 0.22041156888008118

 14%|███████████                                                                  | 43000/300000 [09:46<54:02, 79.25it/s]
epoch 43000  training loss: 0.22689808905124664

 14%|██████████▊                                                                | 43048/300000 [09:54<4:18:08, 16.59it/s]
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+00_invop1 ...
epoch 43100  training loss: 0.23275403678417206

 14%|███████████                                                                  | 43208/300000 [09:56<54:59, 77.83it/s]
epoch 43200  training loss: 0.22952304780483246

 14%|███████████                                                                  | 43296/300000 [09:58<54:04, 79.13it/s]
epoch 43300  training loss: 0.22314897179603577

 15%|███████████▎                                                                 | 44088/300000 [10:08<57:57, 73.59it/s]
epoch 43400  training loss: 0.2122013419866562
epoch 43400  clean testing loss: 0.8981651067733765
epoch 43500  training loss: 0.22115552425384521
epoch 43500  clean testing loss: 0.8739813566207886
epoch 43600  training loss: 0.21931369602680206
epoch 43600  clean testing loss: 0.8749104142189026
epoch 43700  training loss: 0.2244957983493805
epoch 43700  clean testing loss: 0.883119523525238
epoch 43800  training loss: 0.22675354778766632
epoch 43800  clean testing loss: 0.8605929017066956
epoch 43900  training loss: 0.24066567420959473
epoch 43900  clean testing loss: 0.8850996494293213
epoch 44000  training loss: 0.23559707403182983
epoch 44000  clean testing loss: 0.8918723464012146
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+00_invop1 ...
epoch 44100  training loss: 0.22531187534332275

 15%|███████████▎                                                                 | 44248/300000 [10:10<54:05, 78.80it/s]
epoch 44200  training loss: 0.24044360220432281

 15%|███████████▍                                                                 | 44408/300000 [10:13<54:05, 78.75it/s]
epoch 44300  training loss: 0.22844931483268738
epoch 44300  clean testing loss: 0.8892282843589783
epoch 44400  training loss: 0.24763548374176025

 15%|███████████▍                                                                 | 44562/300000 [10:14<53:47, 79.15it/s]
epoch 44500  training loss: 0.23763591051101685
epoch 44500  clean testing loss: 0.8947969675064087
epoch 44600  training loss: 0.21800652146339417

 15%|███████████▍                                                                 | 44724/300000 [10:17<53:57, 78.86it/s]
epoch 44700  training loss: 0.22458170354366302

 15%|███████████▌                                                                 | 44885/300000 [10:19<53:47, 79.04it/s]
epoch 44800  training loss: 0.21901261806488037
epoch 44800  clean testing loss: 0.9160021543502808
epoch 44900  training loss: 0.23621931672096252

 15%|███████████▌                                                                 | 45045/300000 [10:21<53:37, 79.24it/s]
epoch 45000  training loss: 0.22062651813030243
epoch 45000  clean testing loss: 0.9263001084327698

 15%|███████████▌                                                                 | 45197/300000 [10:22<53:35, 79.24it/s]
epoch 45100  training loss: 0.23714029788970947
epoch 45100  clean testing loss: 0.9375635385513306
epoch 45200  training loss: 0.23495851457118988

 15%|███████████▋                                                                 | 45357/300000 [10:25<53:53, 78.75it/s]
epoch 45300  training loss: 0.2449229210615158
epoch 45300  clean testing loss: 0.9492474794387817
epoch 45400  training loss: 0.21696433424949646

 15%|███████████▋                                                                 | 45517/300000 [10:27<53:42, 78.97it/s]
epoch 45500  training loss: 0.23163612186908722

 15%|███████████▋                                                                 | 45677/300000 [10:29<53:35, 79.09it/s]
epoch 45600  training loss: 0.2160320281982422
epoch 45600  clean testing loss: 0.9479183554649353
epoch 45700  training loss: 0.2387521117925644

 15%|███████████▊                                                                 | 45837/300000 [10:31<53:54, 78.58it/s]
epoch 45800  training loss: 0.2337949573993683

 15%|███████████▊                                                                 | 45989/300000 [10:33<53:27, 79.19it/s]
epoch 45900  training loss: 0.24269834160804749
epoch 45900  clean testing loss: 0.951993465423584
epoch 46000  training loss: 0.22677353024482727
epoch 46000  clean testing loss: 0.942761242389679

 15%|███████████▊                                                                 | 46149/300000 [10:35<53:30, 79.07it/s]
epoch 46100  training loss: 0.23781757056713104

 15%|███████████▉                                                                 | 46309/300000 [10:37<53:48, 78.58it/s]
epoch 46200  training loss: 0.22150394320487976
epoch 46200  clean testing loss: 0.9464036226272583
epoch 46300  training loss: 0.22566278278827667

 15%|███████████▉                                                                 | 46469/300000 [10:39<53:34, 78.87it/s]
epoch 46400  training loss: 0.22802680730819702
epoch 46400  clean testing loss: 0.96979159116745
epoch 46500  training loss: 0.24163785576820374

 16%|███████████▉                                                                 | 46621/300000 [10:41<53:30, 78.92it/s]
epoch 46600  training loss: 0.23396654427051544

 16%|████████████                                                                 | 46783/300000 [10:43<53:38, 78.67it/s]
epoch 46700  training loss: 0.21700306236743927
epoch 46700  clean testing loss: 0.9838451147079468
epoch 46800  training loss: 0.21765756607055664

 16%|████████████                                                                 | 46943/300000 [10:45<53:08, 79.35it/s]
epoch 46900  training loss: 0.21790656447410583

 16%|████████████                                                                 | 47096/300000 [10:47<53:15, 79.15it/s]
epoch 47000  training loss: 0.20696067810058594
epoch 47000  clean testing loss: 0.9903435707092285
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+00_invop1 ...
epoch 47100  training loss: 0.21742072701454163

 16%|████████████▏                                                                | 47259/300000 [10:49<52:50, 79.73it/s]
epoch 47200  training loss: 0.22166118025779724
epoch 47200  clean testing loss: 1.0010422468185425
epoch 47300  training loss: 0.21264362335205078

 16%|████████████▏                                                                | 47419/300000 [10:51<53:20, 78.91it/s]
epoch 47400  training loss: 0.2115217000246048

 16%|████████████▏                                                                | 47580/300000 [10:53<53:16, 78.96it/s]
epoch 47500  training loss: 0.225538432598114
epoch 47500  clean testing loss: 0.9905323386192322
epoch 47600  training loss: 0.21453119814395905

 16%|████████████▎                                                                | 47732/300000 [10:55<53:08, 79.12it/s]
epoch 47700  training loss: 0.20939069986343384

 16%|████████████▎                                                                | 47893/300000 [10:57<52:51, 79.49it/s]
epoch 47800  training loss: 0.22112439572811127
epoch 47800  clean testing loss: 0.9848604202270508
epoch 47900  training loss: 0.20960374176502228

 16%|████████████▎                                                                | 48053/300000 [10:59<52:51, 79.44it/s]
epoch 48000  training loss: 0.1996365785598755
epoch 48000  clean testing loss: 0.9703357219696045

 16%|████████████▎                                                                | 48213/300000 [11:01<52:50, 79.41it/s]
epoch 48100  training loss: 0.2048414647579193
epoch 48100  clean testing loss: 0.9729475378990173
epoch 48200  training loss: 0.1983378827571869

 16%|████████████▍                                                                | 48373/300000 [11:03<52:55, 79.23it/s]
epoch 48300  training loss: 0.19130881130695343
epoch 48300  clean testing loss: 0.9832716584205627
epoch 48400  training loss: 0.20278184115886688

 16%|████████████▍                                                                | 48533/300000 [11:05<52:39, 79.59it/s]
epoch 48500  training loss: 0.20199376344680786

 16%|████████████▍                                                                | 48687/300000 [11:07<52:33, 79.69it/s]
epoch 48600  training loss: 0.21655383706092834
epoch 48600  clean testing loss: 1.0103304386138916
epoch 48700  training loss: 0.21244050562381744

 16%|████████████▌                                                                | 48850/300000 [11:09<52:45, 79.35it/s]
epoch 48800  training loss: 0.23166526854038239

 16%|████████████▌                                                                | 49011/300000 [11:11<53:16, 78.52it/s]
epoch 48900  training loss: 0.2134387195110321
epoch 48900  clean testing loss: 0.9872682690620422
epoch 49000  training loss: 0.2269018590450287
epoch 49000  clean testing loss: 0.9864345192909241

 16%|████████████▌                                                                | 49163/300000 [11:13<52:47, 79.19it/s]
epoch 49100  training loss: 0.2181212604045868
epoch 49100  clean testing loss: 0.9996870756149292
epoch 49200  training loss: 0.21517658233642578

 16%|████████████▋                                                                | 49325/300000 [11:15<52:47, 79.13it/s]
epoch 49300  training loss: 0.20829267799854279

 16%|████████████▋                                                                | 49485/300000 [11:17<52:39, 79.28it/s]
epoch 49400  training loss: 0.196879580616951
epoch 49400  clean testing loss: 1.0132601261138916
epoch 49500  training loss: 0.20763398706912994

 17%|████████████▋                                                                | 49645/300000 [11:19<52:32, 79.42it/s]
epoch 49600  training loss: 0.19811463356018066

 17%|████████████▊                                                                | 49693/300000 [11:19<52:28, 79.49it/s]
epoch 49700  training loss: 0.20821848511695862

 17%|████████████▊                                                                | 50160/300000 [11:32<49:49, 83.56it/s]
epoch 49800  training loss: 0.20157060027122498
epoch 49800  clean testing loss: 0.9946747422218323
epoch 49900  training loss: 0.21214279532432556
epoch 49900  clean testing loss: 0.991861879825592
epoch 50000  training loss: 0.19023212790489197
epoch 50000  clean testing loss: 0.9897981286048889
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+00_invop1 ...
epoch 50100  training loss: 0.19942504167556763

 17%|████████████▉                                                                | 50331/300000 [11:34<47:43, 87.19it/s]
epoch 50200  training loss: 0.19930338859558105
epoch 50200  clean testing loss: 0.9914332032203674
epoch 50300  training loss: 0.20227119326591492

 17%|████████████▉                                                                | 50511/300000 [11:36<47:53, 86.82it/s]
epoch 50400  training loss: 0.21835672855377197
epoch 50400  clean testing loss: 0.9931403994560242
epoch 50500  training loss: 0.215270534157753

 17%|█████████████                                                                | 50682/300000 [11:38<47:48, 86.91it/s]
epoch 50600  training loss: 0.20085667073726654

 17%|█████████████                                                                | 50853/300000 [11:40<47:43, 87.00it/s]
epoch 50700  training loss: 0.20656031370162964
epoch 50700  clean testing loss: 0.9978095889091492
epoch 50800  training loss: 0.19797153770923615

 17%|█████████████                                                                | 51033/300000 [11:42<48:11, 86.10it/s]
epoch 50900  training loss: 0.20950284600257874
epoch 50900  clean testing loss: 0.9935861825942993
epoch 51000  training loss: 0.20793409645557404
epoch 51000  clean testing loss: 0.9877839684486389

 17%|█████████████▏                                                               | 51204/300000 [11:44<47:46, 86.80it/s]
epoch 51100  training loss: 0.20681129395961761

 17%|█████████████▏                                                               | 51375/300000 [11:46<47:24, 87.40it/s]
epoch 51200  training loss: 0.19063514471054077
epoch 51200  clean testing loss: 0.9976189732551575
epoch 51300  training loss: 0.22377564013004303

 17%|█████████████▏                                                               | 51555/300000 [11:48<47:45, 86.71it/s]
epoch 51400  training loss: 0.2178652584552765
epoch 51400  clean testing loss: 1.0048710107803345
epoch 51500  training loss: 0.2168211042881012

 17%|█████████████▎                                                               | 51726/300000 [11:50<47:46, 86.61it/s]
epoch 51600  training loss: 0.23313114047050476
epoch 51600  clean testing loss: 1.0003612041473389
epoch 51700  training loss: 0.20768669247627258

 17%|█████████████▎                                                               | 51906/300000 [11:52<47:37, 86.83it/s]
epoch 51800  training loss: 0.21217958629131317

 17%|█████████████▎                                                               | 52077/300000 [11:54<47:20, 87.27it/s]
epoch 51900  training loss: 0.21073277294635773
epoch 51900  clean testing loss: 1.0047670602798462
epoch 52000  training loss: 0.19573163986206055
epoch 52000  clean testing loss: 0.9975757598876953

 17%|█████████████▍                                                               | 52248/300000 [11:56<47:16, 87.34it/s]
epoch 52100  training loss: 0.20102465152740479
epoch 52100  clean testing loss: 1.0010064840316772
epoch 52200  training loss: 0.19105857610702515

 17%|█████████████▍                                                               | 52293/300000 [11:56<47:17, 87.31it/s]
epoch 52300  training loss: 0.20813746750354767

 18%|█████████████▎                                                             | 53076/300000 [12:10<1:23:28, 49.30it/s]
epoch 52400  training loss: 0.2182626575231552
epoch 52400  clean testing loss: 0.9888442754745483
epoch 52500  training loss: 0.19532376527786255
epoch 52500  clean testing loss: 1.0081822872161865
epoch 52600  training loss: 0.20882993936538696
epoch 52600  clean testing loss: 1.0224648714065552
epoch 52700  training loss: 0.228719100356102
epoch 52700  clean testing loss: 1.0138626098632812
epoch 52800  training loss: 0.21582625806331635
epoch 52800  clean testing loss: 1.0190024375915527
epoch 52900  training loss: 0.2059316635131836
epoch 52900  clean testing loss: 1.0251071453094482
epoch 53000  training loss: 0.20265844464302063
epoch 53000  clean testing loss: 1.0311293601989746

 18%|█████████████▋                                                               | 53256/300000 [12:12<47:16, 86.99it/s]
epoch 53100  training loss: 0.21805550158023834
epoch 53100  clean testing loss: 1.018150806427002
epoch 53200  training loss: 0.2008589208126068

 18%|█████████████▋                                                               | 53427/300000 [12:14<47:03, 87.34it/s]
epoch 53300  training loss: 0.20992964506149292
epoch 53300  clean testing loss: 1.0065473318099976
epoch 53400  training loss: 0.19991730153560638

 18%|█████████████▊                                                               | 53607/300000 [12:16<47:22, 86.67it/s]
epoch 53500  training loss: 0.228622168302536

 18%|█████████████▊                                                               | 53778/300000 [12:18<47:07, 87.07it/s]
epoch 53600  training loss: 0.2050362080335617
epoch 53600  clean testing loss: 0.9981914162635803
epoch 53700  training loss: 0.21978913247585297

 18%|█████████████▊                                                               | 53949/300000 [12:20<47:15, 86.77it/s]
epoch 53800  training loss: 0.2185637652873993
epoch 53800  clean testing loss: 1.0024523735046387
epoch 53900  training loss: 0.21662317216396332

 18%|█████████████▊                                                               | 53994/300000 [12:20<47:16, 86.72it/s]
epoch 54000  training loss: 0.21454893052577972

 18%|█████████████▉                                                               | 54147/300000 [12:29<50:11, 81.64it/s]
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+00_invop1 ...
epoch 54100  training loss: 0.22404474020004272

 18%|█████████████▉                                                               | 54318/300000 [12:31<47:10, 86.80it/s]
epoch 54200  training loss: 0.22165952622890472
epoch 54200  clean testing loss: 1.0150997638702393
epoch 54300  training loss: 0.21098321676254272
epoch 54300  clean testing loss: 1.008293867111206
epoch 54400  training loss: 0.24322205781936646

 18%|█████████████▉                                                               | 54489/300000 [12:33<47:08, 86.80it/s]
epoch 54500  training loss: 0.2261221706867218
epoch 54500  clean testing loss: 0.986158013343811
epoch 54600  training loss: 0.2341611683368683

 18%|██████████████                                                               | 54669/300000 [12:35<47:08, 86.73it/s]
epoch 54700  training loss: 0.23079758882522583
epoch 54700  clean testing loss: 1.0030096769332886
epoch 54800  training loss: 0.23239627480506897

 18%|██████████████                                                               | 54840/300000 [12:37<47:13, 86.53it/s]
epoch 54900  training loss: 0.234961599111557

 18%|██████████████                                                               | 55011/300000 [12:39<47:26, 86.07it/s]
epoch 55000  training loss: 0.23507870733737946
epoch 55000  clean testing loss: 1.0041022300720215
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+00_invop1 ...
epoch 55100  training loss: 0.2317323386669159

 18%|██████████████▏                                                              | 55191/300000 [12:41<46:49, 87.13it/s]
epoch 55200  training loss: 0.2417430579662323
epoch 55200  clean testing loss: 1.0092850923538208
epoch 55300  training loss: 0.2508518099784851

 18%|██████████████▏                                                              | 55362/300000 [12:43<46:43, 87.26it/s]
epoch 55400  training loss: 0.23920823633670807
epoch 55400  clean testing loss: 1.0021346807479858
epoch 55500  training loss: 0.24811506271362305

 19%|██████████████▎                                                              | 55542/300000 [12:45<46:56, 86.81it/s]
epoch 55600  training loss: 0.24674634635448456

 19%|██████████████▎                                                              | 55713/300000 [12:47<46:39, 87.26it/s]
epoch 55700  training loss: 0.25076842308044434
epoch 55700  clean testing loss: 1.0105698108673096
epoch 55800  training loss: 0.24470451474189758

 19%|██████████████▎                                                              | 55884/300000 [12:49<46:30, 87.47it/s]
epoch 55900  training loss: 0.2396876960992813
epoch 55900  clean testing loss: 1.0275071859359741
epoch 56000  training loss: 0.22264401614665985
epoch 56000  clean testing loss: 1.0342332124710083

 19%|██████████████▍                                                              | 56064/300000 [12:51<46:43, 87.01it/s]
epoch 56100  training loss: 0.23294927179813385

 19%|██████████████▍                                                              | 56100/300000 [12:51<46:48, 86.85it/s]
epoch 56200  training loss: 0.23200160264968872
epoch 56200  clean testing loss: 1.0415937900543213
epoch 56300  training loss: 0.241192027926445
epoch 56300  clean testing loss: 1.032475233078003
epoch 56400  training loss: 0.22971558570861816
epoch 56400  clean testing loss: 1.0465025901794434
epoch 56500  training loss: 0.2543862462043762
epoch 56500  clean testing loss: 1.0366252660751343
epoch 56600  training loss: 0.21288801729679108
epoch 56600  clean testing loss: 1.0308915376663208
epoch 56700  training loss: 0.21382923424243927
epoch 56700  clean testing loss: 1.0310943126678467
epoch 56800  training loss: 0.21897901594638824
epoch 56800  clean testing loss: 1.035476803779602
epoch 56900  training loss: 0.2144552320241928
epoch 56900  clean testing loss: 1.038373351097107
epoch 57000  training loss: 0.22436609864234924
epoch 57000  clean testing loss: 1.0350788831710815
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+00_invop1 ...
epoch 57100  training loss: 0.23143388330936432

 19%|██████████████▋                                                              | 57135/300000 [13:06<49:01, 82.57it/s]
epoch 57200  training loss: 0.22505418956279755
epoch 57200  clean testing loss: 1.025424599647522
epoch 57300  training loss: 0.23737046122550964

 19%|██████████████▋                                                              | 57306/300000 [13:08<46:25, 87.13it/s]
epoch 57400  training loss: 0.21842606365680695
epoch 57400  clean testing loss: 1.034530520439148
epoch 57500  training loss: 0.22048042714595795

 19%|██████████████▊                                                              | 57486/300000 [13:10<46:24, 87.08it/s]
epoch 57600  training loss: 0.21595291793346405

 19%|██████████████▊                                                              | 57657/300000 [13:12<46:15, 87.30it/s]
epoch 57700  training loss: 0.20724333822727203
epoch 57700  clean testing loss: 1.0028992891311646
epoch 57800  training loss: 0.2187255620956421

 19%|██████████████▊                                                              | 57828/300000 [13:14<46:31, 86.76it/s]
epoch 57900  training loss: 0.21803535521030426
epoch 57900  clean testing loss: 0.9978834986686707
epoch 58000  training loss: 0.22688527405261993


 19%|██████████████▉                                                              | 58143/300000 [13:20<48:03, 83.88it/s]
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+00_invop1 ...
epoch 58100  training loss: 0.2380674034357071

 19%|██████████████▉                                                              | 58314/300000 [13:22<46:16, 87.06it/s]
epoch 58200  training loss: 0.22429151833057404
epoch 58200  clean testing loss: 1.0107568502426147
epoch 58300  training loss: 0.2135964184999466

 19%|███████████████                                                              | 58494/300000 [13:24<46:07, 87.28it/s]
epoch 58400  training loss: 0.2177457958459854
epoch 58400  clean testing loss: 1.0116779804229736
epoch 58500  training loss: 0.21300837397575378

 20%|███████████████                                                              | 58665/300000 [13:26<46:07, 87.19it/s]
epoch 58600  training loss: 0.21513999998569489

 20%|███████████████                                                              | 58845/300000 [13:28<45:58, 87.41it/s]
epoch 58700  training loss: 0.22248825430870056
epoch 58700  clean testing loss: 1.0072778463363647
epoch 58800  training loss: 0.20816010236740112

 20%|███████████████▏                                                             | 59016/300000 [13:30<46:16, 86.81it/s]
epoch 58900  training loss: 0.21026630699634552
epoch 58900  clean testing loss: 1.0004286766052246
epoch 59000  training loss: 0.20573154091835022
epoch 59000  clean testing loss: 0.9891750812530518

 20%|███████████████▏                                                             | 59196/300000 [13:32<45:57, 87.32it/s]
epoch 59100  training loss: 0.2015884667634964
epoch 59100  clean testing loss: 1.002598524093628
epoch 59200  training loss: 0.20990486443042755

 20%|███████████████▎                                                             | 59718/300000 [13:38<45:44, 87.55it/s]
epoch 59300  training loss: 0.19812311232089996
epoch 59300  clean testing loss: 0.9997457265853882
epoch 59400  training loss: 0.21444053947925568
epoch 59400  clean testing loss: 1.0103431940078735
epoch 59500  training loss: 0.21156273782253265
epoch 59500  clean testing loss: 1.0075191259384155
epoch 59600  training loss: 0.21655578911304474
epoch 59600  clean testing loss: 0.9999135732650757
epoch 59700  training loss: 0.22207428514957428

 20%|███████████████▎                                                             | 59898/300000 [13:40<45:53, 87.19it/s]
epoch 59800  training loss: 0.22516249120235443
epoch 59800  clean testing loss: 1.0066477060317993
epoch 59900  training loss: 0.20446524024009705

 20%|███████████████▍                                                             | 60069/300000 [13:42<45:42, 87.47it/s]
epoch 60000  training loss: 0.21546360850334167
epoch 60000  clean testing loss: 1.006514549255371

 20%|███████████████▍                                                             | 60249/300000 [13:44<45:41, 87.46it/s]
epoch 60100  training loss: 0.23769819736480713
epoch 60100  clean testing loss: 1.0040860176086426
epoch 60200  training loss: 0.23725208640098572

 20%|███████████████▌                                                             | 60420/300000 [13:46<45:48, 87.17it/s]
epoch 60300  training loss: 0.2252204716205597
epoch 60300  clean testing loss: 1.003653883934021
epoch 60400  training loss: 0.22914032638072968

 20%|███████████████▌                                                             | 60600/300000 [13:48<45:35, 87.51it/s]
epoch 60500  training loss: 0.21980741620063782
epoch 60500  clean testing loss: 1.009861946105957
epoch 60600  training loss: 0.22784700989723206

 20%|███████████████▌                                                             | 60771/300000 [13:50<45:48, 87.05it/s]
epoch 60700  training loss: 0.21537825465202332

 20%|███████████████▋                                                             | 60951/300000 [13:52<45:26, 87.69it/s]
epoch 60800  training loss: 0.22599604725837708
epoch 60800  clean testing loss: 0.9960929155349731
epoch 60900  training loss: 0.22986173629760742

 20%|███████████████▋                                                             | 61122/300000 [13:54<45:46, 86.98it/s]
epoch 61000  training loss: 0.2347913682460785
epoch 61000  clean testing loss: 1.007321834564209
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+00_invop1 ...
epoch 61100  training loss: 0.22280609607696533

 20%|███████████████▋                                                             | 61293/300000 [13:56<45:35, 87.28it/s]
epoch 61200  training loss: 0.2336786389350891
epoch 61200  clean testing loss: 0.9996204376220703
epoch 61300  training loss: 0.23638111352920532

 20%|███████████████▊                                                             | 61473/300000 [13:58<45:32, 87.30it/s]
epoch 61400  training loss: 0.23037861287593842

 21%|███████████████▊                                                             | 61644/300000 [14:00<45:23, 87.52it/s]
epoch 61500  training loss: 0.22583842277526855
epoch 61500  clean testing loss: 1.0143413543701172
epoch 61600  training loss: 0.22308482229709625

 21%|███████████████▊                                                             | 61698/300000 [14:01<45:15, 87.76it/s]
epoch 61700  training loss: 0.2171582132577896
epoch 61700  clean testing loss: 1.0179566144943237
epoch 61800  training loss: 0.24108347296714783
epoch 61800  clean testing loss: 1.0208019018173218
epoch 61900  training loss: 0.2303873598575592
epoch 61900  clean testing loss: 1.023824691772461
epoch 62000  training loss: 0.23982681334018707
epoch 62000  clean testing loss: 1.0167652368545532
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+00_invop1 ...
epoch 62100  training loss: 0.2375074326992035

 21%|███████████████▉                                                             | 62157/300000 [14:09<46:36, 85.04it/s]
epoch 62200  training loss: 0.24180464446544647
epoch 62200  clean testing loss: 1.0048295259475708
epoch 62300  training loss: 0.2394658923149109

 21%|███████████████▉                                                             | 62337/300000 [14:11<45:25, 87.20it/s]
epoch 62400  training loss: 0.24298880994319916
epoch 62400  clean testing loss: 1.0070029497146606
epoch 62500  training loss: 0.24169225990772247

 21%|████████████████                                                             | 62508/300000 [14:13<45:33, 86.87it/s]
epoch 62600  training loss: 0.24464112520217896

 21%|████████████████▏                                                            | 63156/300000 [14:24<46:21, 85.15it/s]
epoch 62700  training loss: 0.2372688204050064
epoch 62700  clean testing loss: 1.009972095489502
epoch 62800  training loss: 0.23636576533317566
epoch 62800  clean testing loss: 1.0173923969268799
epoch 62900  training loss: 0.2265741378068924
epoch 62900  clean testing loss: 1.021228551864624
epoch 63000  training loss: 0.22863160073757172
epoch 63000  clean testing loss: 1.014012336730957
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+00_invop1 ...
epoch 63100  training loss: 0.2358822226524353

 21%|████████████████▎                                                            | 63336/300000 [14:26<45:15, 87.17it/s]
epoch 63200  training loss: 0.24306528270244598
epoch 63200  clean testing loss: 0.9936326146125793
epoch 63300  training loss: 0.23250357806682587

 21%|████████████████▎                                                            | 63507/300000 [14:28<45:13, 87.17it/s]
epoch 63400  training loss: 0.2297288030385971
epoch 63400  clean testing loss: 1.0054075717926025
epoch 63500  training loss: 0.23405781388282776

 21%|████████████████▎                                                            | 63687/300000 [14:30<44:39, 88.18it/s]
epoch 63600  training loss: 0.24736179411411285

 21%|████████████████▍                                                            | 63858/300000 [14:32<44:47, 87.88it/s]
epoch 63700  training loss: 0.2425178587436676
epoch 63700  clean testing loss: 0.9949895739555359
epoch 63800  training loss: 0.2182110995054245

 21%|████████████████▍                                                            | 63894/300000 [14:33<44:54, 87.61it/s]
epoch 63900  training loss: 0.22808235883712769

epoch 63900  clean testing loss: 1.0134061574935913
epoch 64000  training loss: 0.24626098573207855
epoch 64000  clean testing loss: 1.0014562606811523
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+00_invop1 ...
epoch 64100  training loss: 0.22942881286144257

 21%|████████████████▌                                                            | 64299/300000 [14:44<44:41, 87.91it/s]
epoch 64200  training loss: 0.22710373997688293
epoch 64200  clean testing loss: 0.9965384006500244
epoch 64300  training loss: 0.24504999816417694

 21%|████████████████▌                                                            | 64470/300000 [14:46<44:35, 88.03it/s]
epoch 64400  training loss: 0.24560528993606567

 22%|████████████████▌                                                            | 64650/300000 [14:48<44:38, 87.88it/s]
epoch 64500  training loss: 0.2402944713830948
epoch 64500  clean testing loss: 0.9825090169906616
epoch 64600  training loss: 0.2363465130329132

 22%|████████████████▋                                                            | 64821/300000 [14:50<45:03, 87.00it/s]
epoch 64700  training loss: 0.24371682107448578
epoch 64700  clean testing loss: 0.9797037839889526
epoch 64800  training loss: 0.23952287435531616

 22%|████████████████▋                                                            | 64992/300000 [14:52<44:55, 87.20it/s]
epoch 64900  training loss: 0.25313800573349
epoch 64900  clean testing loss: 0.9931647777557373
epoch 65000  training loss: 0.24676258862018585
epoch 65000  clean testing loss: 0.9816733598709106

 22%|████████████████▋                                                            | 65145/300000 [14:54<45:01, 86.94it/s]
epoch 65100  training loss: 0.22982850670814514

 22%|████████████████▊                                                            | 65325/300000 [14:56<44:53, 87.14it/s]
epoch 65200  training loss: 0.24008360505104065
epoch 65200  clean testing loss: 0.9735376238822937
epoch 65300  training loss: 0.23086659610271454

 22%|████████████████▊                                                            | 65406/300000 [14:57<44:54, 87.07it/s]
epoch 65400  training loss: 0.22913773357868195

 22%|█████████████████▎                                                           | 67422/300000 [15:21<44:22, 87.34it/s]
epoch 65500  training loss: 0.23459260165691376
epoch 65500  clean testing loss: 0.9731460213661194
epoch 65600  training loss: 0.243272602558136
epoch 65600  clean testing loss: 0.9627413153648376
epoch 65700  training loss: 0.24100293219089508
epoch 65700  clean testing loss: 0.9646722674369812
epoch 65800  training loss: 0.24733872711658478
epoch 65800  clean testing loss: 0.9570045471191406
epoch 65900  training loss: 0.23947297036647797
epoch 65900  clean testing loss: 0.9560351967811584
epoch 66000  training loss: 0.24477705359458923
epoch 66000  clean testing loss: 0.9735869765281677
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+00_invop1 ...
epoch 66100  training loss: 0.23787681758403778
epoch 66100  clean testing loss: 0.9749783277511597
epoch 66200  training loss: 0.2508572041988373
epoch 66200  clean testing loss: 0.9673891067504883
epoch 66300  training loss: 0.2539178431034088
epoch 66300  clean testing loss: 0.9693865180015564
epoch 66400  training loss: 0.24395005404949188
epoch 66400  clean testing loss: 0.9742366671562195
epoch 66500  training loss: 0.23544509708881378
epoch 66500  clean testing loss: 0.972506046295166
epoch 66600  training loss: 0.2615654766559601
epoch 66600  clean testing loss: 0.9673287868499756
epoch 66700  training loss: 0.2508576214313507
epoch 66700  clean testing loss: 0.9724347591400146
epoch 66800  training loss: 0.2610277831554413
epoch 66800  clean testing loss: 0.975209653377533
epoch 66900  training loss: 0.23455891013145447
epoch 66900  clean testing loss: 0.9956415295600891
epoch 67000  training loss: 0.2688632905483246
epoch 67000  clean testing loss: 1.0089235305786133
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+00_invop1 ...
epoch 67100  training loss: 0.24478529393672943
epoch 67100  clean testing loss: 1.0106773376464844
epoch 67200  training loss: 0.24982652068138123
epoch 67200  clean testing loss: 1.0153515338897705
epoch 67300  training loss: 0.2327999621629715
epoch 67300  clean testing loss: 1.012919306755066
epoch 67400  training loss: 0.23781484365463257

 23%|█████████████████▎                                                           | 67593/300000 [15:22<44:27, 87.13it/s]
epoch 67500  training loss: 0.24199308454990387

 23%|█████████████████▍                                                           | 67773/300000 [15:25<44:24, 87.15it/s]
epoch 67600  training loss: 0.24749919772148132
epoch 67600  clean testing loss: 1.0296214818954468
epoch 67700  training loss: 0.2554114758968353

 23%|█████████████████▍                                                           | 67944/300000 [15:27<44:16, 87.35it/s]
epoch 67800  training loss: 0.25344619154930115
epoch 67800  clean testing loss: 1.0164912939071655
epoch 67900  training loss: 0.24124452471733093

 23%|█████████████████▍                                                           | 68115/300000 [15:28<44:22, 87.10it/s]
epoch 68000  training loss: 0.2555506229400635
epoch 68000  clean testing loss: 1.028969168663025
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+00_invop1 ...
epoch 68100  training loss: 0.2586390972137451

 23%|█████████████████▌                                                           | 68295/300000 [15:31<44:23, 86.99it/s]
epoch 68200  training loss: 0.2647109031677246

 23%|█████████████████▌                                                           | 68466/300000 [15:33<44:10, 87.37it/s]
epoch 68300  training loss: 0.26578474044799805
epoch 68300  clean testing loss: 1.0110608339309692
epoch 68400  training loss: 0.26143360137939453

 23%|█████████████████▌                                                           | 68646/300000 [15:35<44:09, 87.33it/s]
epoch 68500  training loss: 0.2599806785583496
epoch 68500  clean testing loss: 0.9768263697624207
epoch 68600  training loss: 0.253969669342041

 23%|█████████████████▋                                                           | 68700/300000 [15:35<44:09, 87.30it/s]
epoch 68700  training loss: 0.267055481672287

 23%|█████████████████▋                                                           | 69123/300000 [15:42<46:43, 82.37it/s]
epoch 68800  training loss: 0.2614210247993469
epoch 68800  clean testing loss: 1.00108802318573
epoch 68900  training loss: 0.2603546977043152
epoch 68900  clean testing loss: 0.9948252439498901
epoch 69000  training loss: 0.26598334312438965
epoch 69000  clean testing loss: 1.0116064548492432
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+00_invop1 ...
epoch 69100  training loss: 0.26453012228012085

 23%|█████████████████▊                                                           | 69303/300000 [15:44<44:09, 87.06it/s]
epoch 69200  training loss: 0.26437219977378845
epoch 69200  clean testing loss: 1.0153120756149292
epoch 69300  training loss: 0.26815736293792725

 23%|█████████████████▊                                                           | 69474/300000 [15:46<43:59, 87.34it/s]
epoch 69400  training loss: 0.2687225043773651
epoch 69400  clean testing loss: 1.0201210975646973
epoch 69500  training loss: 0.26599597930908203

 23%|█████████████████▉                                                           | 69654/300000 [15:48<43:52, 87.49it/s]
epoch 69600  training loss: 0.2655675411224365

 23%|█████████████████▉                                                           | 69825/300000 [15:50<44:03, 87.07it/s]
epoch 69700  training loss: 0.24536775052547455
epoch 69700  clean testing loss: 1.057446837425232
epoch 69800  training loss: 0.2539791464805603

 23%|█████████████████▉                                                           | 70005/300000 [15:52<44:21, 86.42it/s]
epoch 69900  training loss: 0.2522856891155243
epoch 69900  clean testing loss: 1.0517431497573853
epoch 70000  training loss: 0.2652042508125305
epoch 70000  clean testing loss: 1.0694761276245117

 23%|██████████████████                                                           | 70176/300000 [15:54<43:53, 87.26it/s]
epoch 70100  training loss: 0.26459792256355286
epoch 70100  clean testing loss: 1.0485724210739136
epoch 70200  training loss: 0.25687113404273987

 23%|██████████████████                                                           | 70356/300000 [15:56<43:44, 87.49it/s]
epoch 70300  training loss: 0.2605603039264679

 24%|██████████████████                                                           | 70527/300000 [15:58<43:45, 87.41it/s]
epoch 70400  training loss: 0.24588808417320251
epoch 70400  clean testing loss: 1.0642942190170288
epoch 70500  training loss: 0.2591795027256012

 24%|██████████████████▏                                                          | 70707/300000 [16:00<43:57, 86.93it/s]
epoch 70600  training loss: 0.26374971866607666
epoch 70600  clean testing loss: 1.0617557764053345
epoch 70700  training loss: 0.25251534581184387

 24%|██████████████████▏                                                          | 70878/300000 [16:02<44:12, 86.39it/s]
epoch 70800  training loss: 0.2554003596305847
epoch 70800  clean testing loss: 1.053645372390747
epoch 70900  training loss: 0.25625133514404297

 24%|██████████████████▏                                                          | 71049/300000 [16:04<43:49, 87.06it/s]
epoch 71000  training loss: 0.2614099979400635
epoch 71000  clean testing loss: 1.0522302389144897

 24%|██████████████████▎                                                          | 71193/300000 [16:06<43:52, 86.93it/s]
epoch 71100  training loss: 0.2744235694408417
epoch 71100  clean testing loss: 1.0472993850708008
epoch 71200  training loss: 0.27695533633232117

 24%|██████████████████▍                                                          | 71751/300000 [16:12<43:44, 86.96it/s]
epoch 71300  training loss: 0.2633393704891205
epoch 71300  clean testing loss: 1.0289756059646606
epoch 71400  training loss: 0.2723829746246338
epoch 71400  clean testing loss: 1.0262508392333984
epoch 71500  training loss: 0.2683079242706299
epoch 71500  clean testing loss: 0.9999968409538269
epoch 71600  training loss: 0.2739883065223694
epoch 71600  clean testing loss: 0.9960416555404663
epoch 71700  training loss: 0.2667178809642792

 24%|██████████████████▍                                                          | 71922/300000 [16:14<43:35, 87.20it/s]
epoch 71800  training loss: 0.2804379463195801
epoch 71800  clean testing loss: 1.0076853036880493
epoch 71900  training loss: 0.2725234627723694

 24%|██████████████████▌                                                          | 72102/300000 [16:16<43:36, 87.08it/s]
epoch 72000  training loss: 0.2684202194213867
epoch 72000  clean testing loss: 0.9933655858039856
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+00_invop1 ...
epoch 72100  training loss: 0.2652570903301239

 24%|██████████████████▌                                                          | 72273/300000 [16:18<43:28, 87.29it/s]
epoch 72200  training loss: 0.275175005197525
epoch 72200  clean testing loss: 0.977092981338501
epoch 72300  training loss: 0.27721136808395386

 24%|██████████████████▌                                                          | 72453/300000 [16:20<43:23, 87.41it/s]
epoch 72400  training loss: 0.28488340973854065

 24%|██████████████████▋                                                          | 72624/300000 [16:22<43:28, 87.18it/s]
epoch 72500  training loss: 0.26767775416374207
epoch 72500  clean testing loss: 0.9806377291679382
epoch 72600  training loss: 0.28261950612068176

 24%|██████████████████▋                                                          | 72804/300000 [16:24<43:27, 87.13it/s]
epoch 72700  training loss: 0.28104016184806824
epoch 72700  clean testing loss: 0.9798183441162109
epoch 72800  training loss: 0.27833661437034607

 24%|██████████████████▋                                                          | 72975/300000 [16:26<43:19, 87.33it/s]
epoch 72900  training loss: 0.2752182185649872
epoch 72900  clean testing loss: 0.9627282619476318
epoch 73000  training loss: 0.28264546394348145
epoch 73000  clean testing loss: 0.9677546620368958

 24%|██████████████████▊                                                          | 73146/300000 [16:28<43:14, 87.44it/s]
epoch 73100  training loss: 0.2870423197746277

 24%|██████████████████▊                                                          | 73326/300000 [16:30<43:18, 87.22it/s]
epoch 73200  training loss: 0.2804761528968811
epoch 73200  clean testing loss: 0.9656348824501038
epoch 73300  training loss: 0.27423036098480225
epoch 73300  clean testing loss: 0.9725885391235352
epoch 73400  training loss: 0.2620494067668915
epoch 73400  clean testing loss: 0.9720693826675415
epoch 73500  training loss: 0.2656598687171936
epoch 73500  clean testing loss: 0.9741646647453308
Validation loss variation < 1e-6, trained to interpolation, stop

 24%|██████████████████▊                                                          | 73500/300000 [16:32<50:59, 74.04it/s]