
  0%|                                                                                            | 0/300000 [00:00<?, ?it/s]
epoch 0  training loss: 59.01002502441406
epoch 0  clean testing loss: 44.23716354370117
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop1 ...
epoch 100  training loss: 10.500258445739746


  0%|                                                                                | 272/300000 [00:04<1:06:21, 75.29it/s]
epoch 200  training loss: 8.895051002502441
epoch 200  clean testing loss: 0.5690065622329712
epoch 300  training loss: 8.610065460205078
epoch 300  clean testing loss: 0.6261690855026245
epoch 400  training loss: 8.488974571228027

  0%|                                                                                | 416/300000 [00:06<1:09:46, 71.57it/s]
epoch 500  training loss: 8.283220291137695

  0%|▏                                                                               | 560/300000 [00:08<1:09:50, 71.46it/s]
epoch 600  training loss: 8.12026596069336
epoch 600  clean testing loss: 0.9390383958816528
epoch 700  training loss: 7.9412150382995605

  0%|▏                                                                               | 695/300000 [00:10<1:15:26, 66.13it/s]
epoch 800  training loss: 7.794120788574219


  0%|▎                                                                               | 961/300000 [00:14<1:16:06, 65.48it/s]
epoch 900  training loss: 7.55428409576416
epoch 900  clean testing loss: 1.4128450155258179
epoch 1000  training loss: 7.296131610870361
epoch 1000  clean testing loss: 1.7073819637298584

  0%|▎                                                                               | 996/300000 [00:14<1:15:10, 66.30it/s]
epoch 1100  training loss: 6.981597900390625

  0%|▎                                                                              | 1121/300000 [00:19<1:17:35, 64.20it/s]
epoch 1200  training loss: 6.818592071533203

  0%|▎                                                                              | 1254/300000 [00:21<1:16:43, 64.90it/s]
epoch 1300  training loss: 6.605363368988037

  0%|▎                                                                              | 1380/300000 [00:23<1:16:10, 65.33it/s]
epoch 1400  training loss: 6.17473840713501
epoch 1400  clean testing loss: 3.0624279975891113
epoch 1500  training loss: 6.033602714538574

  1%|▍                                                                              | 1514/300000 [00:25<1:15:17, 66.07it/s]
epoch 1600  training loss: 5.977372646331787

  1%|▍                                                                              | 1647/300000 [00:27<1:16:17, 65.18it/s]
epoch 1700  training loss: 5.859175205230713

  1%|▍                                                                              | 1773/300000 [00:29<1:15:59, 65.41it/s]
epoch 1800  training loss: 5.6336350440979
epoch 1800  clean testing loss: 3.65670108795166
epoch 1900  training loss: 5.294748306274414

  1%|▌                                                                              | 1906/300000 [00:31<1:15:01, 66.22it/s]
epoch 2000  training loss: 5.272842884063721
epoch 2000  clean testing loss: 4.43651008605957

  1%|▌                                                                              | 2039/300000 [00:33<1:15:41, 65.61it/s]
epoch 2100  training loss: 5.066012859344482

  1%|▌                                                                              | 2172/300000 [00:35<1:15:18, 65.91it/s]
epoch 2200  training loss: 5.526548385620117
epoch 2200  clean testing loss: 4.483214378356934
epoch 2300  training loss: 5.287223815917969

  1%|▌                                                                              | 2305/300000 [00:37<1:16:25, 64.92it/s]
epoch 2400  training loss: 4.9385528564453125

  1%|▋                                                                              | 2431/300000 [00:39<1:15:39, 65.55it/s]
epoch 2500  training loss: 6.156341075897217

  1%|▋                                                                              | 2564/300000 [00:41<1:15:38, 65.54it/s]
epoch 2600  training loss: 5.45623779296875

  1%|▋                                                                              | 2697/300000 [00:43<1:15:34, 65.57it/s]
epoch 2700  training loss: 5.257317543029785
epoch 2700  clean testing loss: 5.058592319488525
epoch 2800  training loss: 5.19289493560791

  1%|▋                                                                              | 2823/300000 [00:45<1:16:38, 64.62it/s]
epoch 2900  training loss: 5.265644073486328

  1%|▊                                                                              | 2956/300000 [00:47<1:17:05, 64.23it/s]
epoch 3000  training loss: 4.709009647369385
epoch 3000  clean testing loss: 5.75624942779541

  1%|▊                                                                              | 3082/300000 [00:49<1:15:12, 65.80it/s]
epoch 3100  training loss: 4.487944602966309
epoch 3100  clean testing loss: 6.466829299926758
epoch 3200  training loss: 4.74711275100708

  1%|▊                                                                              | 3215/300000 [00:51<1:15:35, 65.44it/s]
epoch 3300  training loss: 4.90501594543457

  1%|▉                                                                              | 3348/300000 [00:53<1:15:15, 65.70it/s]
epoch 3400  training loss: 4.821837902069092
epoch 3400  clean testing loss: 5.657760143280029
epoch 3500  training loss: 4.666945457458496
epoch 3500  clean testing loss: 5.853338241577148
epoch 3600  training loss: 5.185243606567383
epoch 3600  clean testing loss: 5.224449634552002
epoch 3700  training loss: 4.789231777191162

  1%|▉                                                                              | 3790/300000 [00:59<1:14:44, 66.06it/s]
epoch 3800  training loss: 4.721481800079346
epoch 3800  clean testing loss: 6.126418113708496
epoch 3900  training loss: 4.0218939781188965

  1%|█                                                                              | 3923/300000 [01:01<1:15:35, 65.28it/s]
epoch 4000  training loss: 4.283308982849121
epoch 4000  clean testing loss: 6.203601360321045

  1%|█                                                                              | 4049/300000 [01:03<1:15:42, 65.15it/s]
epoch 4100  training loss: 4.277103424072266

  1%|█                                                                              | 4182/300000 [01:05<1:14:45, 65.94it/s]
epoch 4200  training loss: 4.0295562744140625
epoch 4200  clean testing loss: 6.990906715393066
epoch 4300  training loss: 4.34086799621582


  1%|█▏                                                                             | 4448/300000 [01:09<1:15:51, 64.94it/s]
epoch 4400  training loss: 4.117315769195557
epoch 4400  clean testing loss: 6.972488880157471
epoch 4500  training loss: 4.429548263549805


  2%|█▏                                                                             | 4707/300000 [01:13<1:15:03, 65.58it/s]
epoch 4600  training loss: 4.315454959869385
epoch 4600  clean testing loss: 7.188807964324951
epoch 4700  training loss: 3.91853928565979

  2%|█▎                                                                             | 4840/300000 [01:15<1:14:55, 65.65it/s]
epoch 4800  training loss: 4.137885093688965

  2%|█▎                                                                             | 4973/300000 [01:17<1:14:59, 65.57it/s]
epoch 4900  training loss: 4.116304874420166

  2%|█▎                                                                             | 5106/300000 [01:19<1:15:37, 64.99it/s]
epoch 5000  training loss: 3.9187018871307373
epoch 5000  clean testing loss: 7.416245460510254

  2%|█▍                                                                             | 5239/300000 [01:21<1:14:49, 65.66it/s]
epoch 5100  training loss: 3.94057035446167
epoch 5100  clean testing loss: 7.6291913986206055
epoch 5200  training loss: 4.016585350036621

  2%|█▍                                                                             | 5365/300000 [01:23<1:14:32, 65.88it/s]
epoch 5300  training loss: 4.049065589904785

  2%|█▍                                                                             | 5498/300000 [01:25<1:13:55, 66.40it/s]
epoch 5400  training loss: 3.9000937938690186
epoch 5400  clean testing loss: 7.788746356964111
epoch 5500  training loss: 3.980055332183838
epoch 5500  clean testing loss: 7.434222221374512
epoch 5600  training loss: 4.044140338897705


  2%|█▌                                                                             | 5729/300000 [01:29<1:14:38, 65.70it/s]
epoch 5700  training loss: 3.7458317279815674
epoch 5700  clean testing loss: 7.11337423324585
epoch 5800  training loss: 3.7447714805603027

  2%|█▌                                                                             | 5862/300000 [01:31<1:14:58, 65.39it/s]
epoch 5900  training loss: 3.440986394882202
epoch 5900  clean testing loss: 7.593074798583984
epoch 6000  training loss: 3.762688636779785
epoch 6000  clean testing loss: 8.11872386932373

  2%|█▌                                                                             | 5995/300000 [01:33<1:14:16, 65.98it/s]
epoch 6100  training loss: 3.7032406330108643

  2%|█▌                                                                             | 6121/300000 [01:35<1:13:59, 66.19it/s]
epoch 6200  training loss: 3.7460572719573975

  2%|█▋                                                                             | 6254/300000 [01:37<1:14:43, 65.51it/s]
epoch 6300  training loss: 3.801359176635742
epoch 6300  clean testing loss: 7.60167121887207
epoch 6400  training loss: 3.9557082653045654

  2%|█▋                                                                             | 6387/300000 [01:39<1:14:31, 65.66it/s]
epoch 6500  training loss: 3.594259023666382

  2%|█▋                                                                             | 6520/300000 [01:41<1:14:10, 65.94it/s]
epoch 6600  training loss: 3.6625235080718994

  2%|█▊                                                                             | 6654/300000 [01:43<1:10:35, 69.26it/s]
epoch 6700  training loss: 3.741763114929199
epoch 6700  clean testing loss: 6.609525680541992
epoch 6800  training loss: 3.642225742340088

  2%|█▊                                                                             | 6798/300000 [01:45<1:12:57, 66.99it/s]
epoch 6900  training loss: 3.7598493099212646

  2%|█▊                                                                             | 6924/300000 [01:47<1:15:04, 65.07it/s]
epoch 7000  training loss: 3.4680733680725098
epoch 7000  clean testing loss: 7.953271865844727

  2%|█▊                                                                             | 7036/300000 [01:49<1:25:54, 56.84it/s]
epoch 7100  training loss: 3.6546359062194824

  2%|█▉                                                                             | 7169/300000 [01:51<1:13:40, 66.24it/s]
epoch 7200  training loss: 3.625964641571045
epoch 7200  clean testing loss: 7.6042375564575195
epoch 7300  training loss: 5.849520683288574


  2%|█▉                                                                             | 7435/300000 [01:55<1:13:33, 66.28it/s]
epoch 7400  training loss: 4.7716474533081055

  3%|█▉                                                                             | 7561/300000 [01:57<1:13:02, 66.73it/s]
epoch 7500  training loss: 4.822432041168213

  3%|██                                                                             | 7697/300000 [01:59<1:06:19, 73.44it/s]
epoch 7600  training loss: 4.301164150238037
epoch 7600  clean testing loss: 5.380307674407959
epoch 7700  training loss: 4.126651763916016
epoch 7700  clean testing loss: 5.824871063232422
epoch 7800  training loss: 4.122915744781494
epoch 7800  clean testing loss: 6.2519378662109375
epoch 7900  training loss: 3.9909424781799316

  3%|██                                                                             | 7913/300000 [02:02<1:14:09, 65.65it/s]
epoch 8000  training loss: 4.005880355834961
epoch 8000  clean testing loss: 6.983908653259277


  3%|██▏                                                                            | 8179/300000 [02:06<1:13:48, 65.89it/s]
epoch 8100  training loss: 3.559455156326294

  3%|██▏                                                                            | 8312/300000 [02:08<1:14:42, 65.07it/s]
epoch 8200  training loss: 3.694024085998535

  3%|██▏                                                                            | 8438/300000 [02:10<1:14:07, 65.56it/s]
epoch 8300  training loss: 3.6249923706054688
epoch 8300  clean testing loss: 7.189780235290527
epoch 8400  training loss: 3.8309037685394287

  3%|██▎                                                                            | 8571/300000 [02:12<1:14:06, 65.53it/s]
epoch 8500  training loss: 3.6412551403045654

  3%|██▎                                                                            | 8704/300000 [02:14<1:14:03, 65.56it/s]
epoch 8600  training loss: 3.4716343879699707

  3%|██▎                                                                            | 8837/300000 [02:16<1:14:15, 65.36it/s]
epoch 8700  training loss: 3.505312204360962
epoch 8700  clean testing loss: 8.004831314086914
epoch 8800  training loss: 3.5769128799438477

  3%|██▎                                                                            | 8970/300000 [02:18<1:14:00, 65.54it/s]
epoch 8900  training loss: 3.3426458835601807

  3%|██▍                                                                            | 9103/300000 [02:20<1:13:51, 65.64it/s]
epoch 9000  training loss: 3.453477144241333
epoch 9000  clean testing loss: 7.745851039886475

  3%|██▍                                                                            | 9229/300000 [02:22<1:13:34, 65.86it/s]
epoch 9100  training loss: 3.400319814682007
epoch 9100  clean testing loss: 8.021672248840332
epoch 9200  training loss: 3.4098551273345947

  3%|██▍                                                                            | 9362/300000 [02:24<1:13:46, 65.66it/s]
epoch 9300  training loss: 3.759129762649536
epoch 9300  clean testing loss: 8.200051307678223
epoch 9400  training loss: 3.424617052078247
epoch 9400  clean testing loss: 8.343056678771973
epoch 9500  training loss: 3.1082189083099365
epoch 9500  clean testing loss: 8.367752075195312
epoch 9600  training loss: 3.5083189010620117
epoch 9600  clean testing loss: 8.447535514831543
epoch 9700  training loss: 3.4559106826782227

  3%|██▌                                                                            | 9726/300000 [02:29<1:14:11, 65.21it/s]
epoch 9800  training loss: 3.410360097885132

  3%|██▌                                                                            | 9859/300000 [02:31<1:13:14, 66.02it/s]
epoch 9900  training loss: 3.4952473640441895
epoch 9900  clean testing loss: 7.95860481262207
epoch 10000  training loss: 3.2133607864379883
epoch 10000  clean testing loss: 8.341198921203613

  3%|██▋                                                                            | 9992/300000 [02:33<1:13:11, 66.04it/s]
epoch 10100  training loss: 3.289872884750366

  3%|██▋                                                                           | 10118/300000 [02:35<1:14:28, 64.88it/s]
epoch 10200  training loss: 4.272517204284668

  3%|██▋                                                                           | 10251/300000 [02:37<1:13:52, 65.36it/s]
epoch 10300  training loss: 7.107840061187744

  3%|██▋                                                                           | 10384/300000 [02:39<1:14:11, 65.07it/s]
epoch 10400  training loss: 4.818568706512451
epoch 10400  clean testing loss: 5.357156276702881
epoch 10500  training loss: 4.613579273223877

  4%|██▋                                                                           | 10517/300000 [02:41<1:14:32, 64.73it/s]
epoch 10600  training loss: 4.589620113372803

  4%|██▊                                                                           | 10650/300000 [02:44<1:13:48, 65.34it/s]
epoch 10700  training loss: 4.291253566741943

  4%|██▊                                                                           | 10776/300000 [02:45<1:13:41, 65.41it/s]
epoch 10800  training loss: 3.948194980621338
epoch 10800  clean testing loss: 6.140384674072266
epoch 10900  training loss: 4.029869556427002

  4%|██▊                                                                           | 10909/300000 [02:47<1:13:18, 65.72it/s]
epoch 11000  training loss: 3.9351022243499756
epoch 11000  clean testing loss: 6.792489528656006


  4%|██▉                                                                           | 11175/300000 [02:52<1:13:06, 65.85it/s]
epoch 11100  training loss: 3.6503653526306152

  4%|██▉                                                                           | 11406/300000 [02:55<1:12:36, 66.24it/s]
epoch 11200  training loss: 4.155884742736816
epoch 11200  clean testing loss: 6.705621242523193
epoch 11300  training loss: 3.915057897567749
epoch 11300  clean testing loss: 6.8736491203308105
epoch 11400  training loss: 3.6560347080230713
epoch 11400  clean testing loss: 7.0631914138793945
epoch 11500  training loss: 3.8950116634368896

  4%|███                                                                           | 11539/300000 [02:57<1:13:48, 65.14it/s]
epoch 11600  training loss: 3.874924421310425

  4%|███                                                                           | 11665/300000 [02:59<1:13:30, 65.37it/s]
epoch 11700  training loss: 3.3548121452331543
epoch 11700  clean testing loss: 6.922445774078369
epoch 11800  training loss: 3.948652744293213

  4%|███                                                                           | 11798/300000 [03:01<1:12:14, 66.49it/s]
epoch 11900  training loss: 3.498751640319824

  4%|███                                                                           | 11931/300000 [03:03<1:13:30, 65.31it/s]
epoch 12000  training loss: 3.380239486694336
epoch 12000  clean testing loss: 7.129359722137451

  4%|███▏                                                                          | 12064/300000 [03:05<1:13:12, 65.55it/s]
epoch 12100  training loss: 3.4209134578704834
epoch 12100  clean testing loss: 7.415249347686768
epoch 12200  training loss: 3.6485514640808105

  4%|███▏                                                                          | 12197/300000 [03:07<1:13:41, 65.09it/s]
epoch 12300  training loss: 3.403301477432251

  4%|███▏                                                                          | 12323/300000 [03:09<1:13:00, 65.67it/s]
epoch 12400  training loss: 3.274012804031372

  4%|███▏                                                                          | 12456/300000 [03:11<1:12:43, 65.89it/s]
epoch 12500  training loss: 3.3795793056488037

  4%|███▎                                                                          | 12589/300000 [03:13<1:12:53, 65.71it/s]
epoch 12600  training loss: 3.2093851566314697
epoch 12600  clean testing loss: 8.05374526977539
epoch 12700  training loss: 3.2194440364837646

  4%|███▎                                                                          | 12722/300000 [03:15<1:13:48, 64.87it/s]
epoch 12800  training loss: 3.3678882122039795

  4%|███▎                                                                          | 12855/300000 [03:17<1:12:11, 66.29it/s]
epoch 12900  training loss: 2.969660520553589

  4%|███▍                                                                          | 12981/300000 [03:19<1:11:56, 66.50it/s]
epoch 13000  training loss: 2.9863860607147217
epoch 13000  clean testing loss: 8.393985748291016
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop1 ...
epoch 13100  training loss: 2.795682668685913

  4%|███▍                                                                          | 13114/300000 [03:21<1:17:14, 61.91it/s]
epoch 13200  training loss: 2.749765396118164

  4%|███▍                                                                          | 13240/300000 [03:23<1:13:44, 64.81it/s]
epoch 13300  training loss: 2.9734034538269043

  4%|███▍                                                                          | 13366/300000 [03:25<1:13:49, 64.71it/s]
epoch 13400  training loss: 3.1495509147644043
epoch 13400  clean testing loss: 7.735728740692139
epoch 13500  training loss: 3.0264835357666016

  4%|███▌                                                                          | 13499/300000 [03:27<1:12:49, 65.57it/s]
epoch 13600  training loss: 2.8876426219940186

  5%|███▌                                                                          | 13632/300000 [03:29<1:12:12, 66.10it/s]
epoch 13700  training loss: 3.0878872871398926

  5%|███▌                                                                          | 13772/300000 [03:31<1:00:44, 78.53it/s]
epoch 13800  training loss: 3.2328104972839355
epoch 13800  clean testing loss: 8.174886703491211
epoch 13900  training loss: 2.9537627696990967
epoch 13900  clean testing loss: 8.527425765991211
epoch 14000  training loss: 3.162199020385742
epoch 14000  clean testing loss: 8.234981536865234
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop1 ...
epoch 14100  training loss: 2.9800591468811035


  5%|███▋                                                                          | 14237/300000 [03:44<1:12:50, 65.38it/s]
epoch 14200  training loss: 2.857165575027466

  5%|███▋                                                                          | 14370/300000 [03:46<1:13:07, 65.10it/s]
epoch 14300  training loss: 2.8200924396514893

  5%|███▊                                                                          | 14496/300000 [03:48<1:12:04, 66.02it/s]
epoch 14400  training loss: 2.86310076713562
epoch 14400  clean testing loss: 7.957673072814941
epoch 14500  training loss: 2.921365976333618

  5%|███▊                                                                          | 14629/300000 [03:50<1:11:50, 66.21it/s]
epoch 14600  training loss: 2.8609495162963867
epoch 14600  clean testing loss: 8.49704647064209
epoch 14700  training loss: 2.937671422958374


  5%|███▊                                                                          | 14895/300000 [03:54<1:12:11, 65.83it/s]
epoch 14800  training loss: 2.8250088691711426

  5%|███▉                                                                          | 14993/300000 [03:56<1:11:53, 66.08it/s]
epoch 14900  training loss: 2.8343865871429443
epoch 14900  clean testing loss: 8.505126953125
epoch 15000  training loss: 2.8166391849517822
epoch 15000  clean testing loss: 8.766050338745117
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop1 ...
epoch 15100  training loss: 2.82228422164917

  5%|███▉                                                                          | 15106/300000 [03:59<1:14:35, 63.65it/s]
epoch 15200  training loss: 2.925579071044922

  5%|███▉                                                                          | 15239/300000 [04:01<1:11:52, 66.04it/s]
epoch 15300  training loss: 2.803635358810425

  5%|███▉                                                                          | 15365/300000 [04:03<1:11:47, 66.09it/s]
epoch 15400  training loss: 2.630535840988159
epoch 15400  clean testing loss: 8.979161262512207
epoch 15500  training loss: 2.4712038040161133

  5%|████                                                                          | 15498/300000 [04:05<1:12:07, 65.74it/s]
epoch 15600  training loss: 2.7009332180023193

  5%|████                                                                          | 15631/300000 [04:07<1:11:50, 65.98it/s]
epoch 15700  training loss: 2.5049421787261963

  5%|████                                                                          | 15764/300000 [04:09<1:12:35, 65.26it/s]
epoch 15800  training loss: 2.6069180965423584


  5%|████▏                                                                         | 16108/300000 [04:16<1:14:30, 63.51it/s]
epoch 15900  training loss: 2.537428379058838
epoch 15900  clean testing loss: 9.068071365356445
epoch 16000  training loss: 2.884700298309326
epoch 16000  clean testing loss: 9.470986366271973
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop1 ...
epoch 16100  training loss: 2.4328224658966064
epoch 16100  clean testing loss: 9.813736915588379
epoch 16200  training loss: 2.5935466289520264


  5%|████▎                                                                         | 16374/300000 [04:20<1:12:18, 65.37it/s]
epoch 16300  training loss: 2.873331308364868

  6%|████▎                                                                         | 16507/300000 [04:22<1:13:05, 64.65it/s]
epoch 16400  training loss: 2.7196691036224365
epoch 16400  clean testing loss: 9.828912734985352
epoch 16500  training loss: 2.809232473373413

  6%|████▎                                                                         | 16640/300000 [04:24<1:12:20, 65.29it/s]
epoch 16600  training loss: 2.6759166717529297

  6%|████▎                                                                         | 16766/300000 [04:26<1:11:26, 66.08it/s]
epoch 16700  training loss: 2.8165924549102783

  6%|████▍                                                                         | 16899/300000 [04:28<1:11:53, 65.64it/s]
epoch 16800  training loss: 2.9464564323425293

  6%|████▍                                                                         | 17032/300000 [04:30<1:11:22, 66.07it/s]
epoch 16900  training loss: 3.59661865234375
epoch 16900  clean testing loss: 9.887086868286133
epoch 17000  training loss: 3.354769706726074
epoch 17000  clean testing loss: 8.616155624389648

  6%|████▍                                                                         | 17165/300000 [04:32<1:12:05, 65.39it/s]
epoch 17100  training loss: 3.3645856380462646

  6%|████▍                                                                         | 17298/300000 [04:34<1:11:33, 65.84it/s]
epoch 17200  training loss: 3.619131326675415

  6%|████▌                                                                         | 17424/300000 [04:36<1:11:34, 65.80it/s]
epoch 17300  training loss: 3.483327865600586
epoch 17300  clean testing loss: 9.628217697143555
epoch 17400  training loss: 3.3573312759399414

  6%|████▌                                                                         | 17557/300000 [04:38<1:11:21, 65.97it/s]
epoch 17500  training loss: 3.733672857284546

  6%|████▌                                                                         | 17690/300000 [04:40<1:11:55, 65.42it/s]
epoch 17600  training loss: 4.042327880859375

  6%|████▋                                                                         | 17823/300000 [04:42<1:11:29, 65.78it/s]
epoch 17700  training loss: 3.698777914047241
epoch 17700  clean testing loss: 9.653359413146973
epoch 17800  training loss: 3.439358949661255

  6%|████▋                                                                         | 17956/300000 [04:44<1:11:10, 66.05it/s]
epoch 17900  training loss: 3.540523052215576

  6%|████▋                                                                         | 18082/300000 [04:46<1:12:50, 64.50it/s]
epoch 18000  training loss: 3.380800247192383
epoch 18000  clean testing loss: 10.166438102722168

  6%|████▋                                                                         | 18215/300000 [04:48<1:10:40, 66.45it/s]
epoch 18100  training loss: 3.5336880683898926
epoch 18100  clean testing loss: 9.670893669128418
epoch 18200  training loss: 3.9499871730804443

  6%|████▊                                                                         | 18348/300000 [04:50<1:10:54, 66.20it/s]
epoch 18300  training loss: 4.6676764488220215

  6%|████▊                                                                         | 18481/300000 [04:52<1:11:22, 65.74it/s]
epoch 18400  training loss: 5.587936878204346

  6%|████▊                                                                         | 18607/300000 [04:54<1:11:03, 66.01it/s]
epoch 18500  training loss: 5.778368949890137
epoch 18500  clean testing loss: 7.461152076721191
epoch 18600  training loss: 4.786811351776123

  6%|████▊                                                                         | 18740/300000 [04:56<1:11:09, 65.88it/s]
epoch 18700  training loss: 5.210165500640869
epoch 18700  clean testing loss: 6.963078498840332
epoch 18800  training loss: 4.4766411781311035
epoch 18800  clean testing loss: 7.019663333892822
epoch 18900  training loss: 4.34088134765625
epoch 18900  clean testing loss: 6.6568403244018555
epoch 19000  training loss: 4.479426860809326
epoch 19000  clean testing loss: 7.56219482421875
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop1 ...
epoch 19100  training loss: 4.90512752532959


  6%|████▉                                                                         | 19229/300000 [05:08<1:10:54, 65.99it/s]
epoch 19200  training loss: 4.630328178405762

  6%|█████                                                                         | 19362/300000 [05:10<1:10:59, 65.88it/s]
epoch 19300  training loss: 4.679375171661377

  6%|█████                                                                         | 19495/300000 [05:12<1:10:13, 66.57it/s]
epoch 19400  training loss: 5.050249099731445
epoch 19400  clean testing loss: 7.328047275543213
epoch 19500  training loss: 4.709141254425049

  7%|█████                                                                         | 19628/300000 [05:15<1:11:44, 65.13it/s]
epoch 19600  training loss: 4.645257472991943

  7%|█████▏                                                                        | 19754/300000 [05:16<1:11:49, 65.04it/s]
epoch 19700  training loss: 4.689903736114502

  7%|█████▏                                                                        | 19887/300000 [05:18<1:10:28, 66.24it/s]
epoch 19800  training loss: 4.383164405822754
epoch 19800  clean testing loss: 7.378439426422119
epoch 19900  training loss: 4.924311637878418

  7%|█████▏                                                                        | 19992/300000 [05:20<1:10:27, 66.23it/s]
epoch 20000  training loss: 4.444503307342529
epoch 20000  clean testing loss: 7.186756134033203
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop1 ...
epoch 20100  training loss: 4.137819766998291

  7%|█████▏                                                                        | 20113/300000 [05:23<1:12:10, 64.64it/s]
epoch 20200  training loss: 3.922027111053467

  7%|█████▎                                                                        | 20246/300000 [05:25<1:10:36, 66.03it/s]
epoch 20300  training loss: 3.8491804599761963

  7%|█████▎                                                                        | 20372/300000 [05:27<1:11:39, 65.03it/s]
epoch 20400  training loss: 4.031101226806641

  7%|█████▎                                                                        | 20505/300000 [05:29<1:10:31, 66.05it/s]
epoch 20500  training loss: 3.8611481189727783
epoch 20500  clean testing loss: 8.135708808898926
epoch 20600  training loss: 4.039383888244629

  7%|█████▎                                                                        | 20638/300000 [05:31<1:10:58, 65.60it/s]
epoch 20700  training loss: 3.9223780632019043

  7%|█████▍                                                                        | 20771/300000 [05:33<1:10:52, 65.66it/s]
epoch 20800  training loss: 4.0132246017456055


  7%|█████▍                                                                        | 21030/300000 [05:37<1:11:46, 64.78it/s]
epoch 20900  training loss: 4.119750022888184
epoch 20900  clean testing loss: 8.391947746276855
epoch 21000  training loss: 3.846709728240967
epoch 21000  clean testing loss: 8.32448673248291

  7%|█████▌                                                                        | 21163/300000 [05:39<1:10:48, 65.64it/s]
epoch 21100  training loss: 3.999133825302124

  7%|█████▌                                                                        | 21296/300000 [05:41<1:10:33, 65.83it/s]
epoch 21200  training loss: 3.9599313735961914

  7%|█████▌                                                                        | 21429/300000 [05:43<1:11:23, 65.03it/s]
epoch 21300  training loss: 4.022676944732666
epoch 21300  clean testing loss: 8.795143127441406
epoch 21400  training loss: 3.8717775344848633

  7%|█████▌                                                                        | 21562/300000 [05:45<1:10:32, 65.79it/s]
epoch 21500  training loss: 3.8193557262420654

  7%|█████▋                                                                        | 22017/300000 [05:52<1:11:27, 64.84it/s]
epoch 21600  training loss: 3.776634931564331
epoch 21600  clean testing loss: 8.98459243774414
epoch 21700  training loss: 3.8005387783050537
epoch 21700  clean testing loss: 9.10586166381836
epoch 21800  training loss: 3.627547025680542
epoch 21800  clean testing loss: 9.070150375366211
epoch 21900  training loss: 3.448737621307373
epoch 21900  clean testing loss: 9.260664939880371
epoch 22000  training loss: 3.4548258781433105
epoch 22000  clean testing loss: 9.305134773254395
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop1 ...
epoch 22100  training loss: 3.665057420730591

  7%|█████▊                                                                        | 22150/300000 [05:54<1:10:46, 65.44it/s]
epoch 22200  training loss: 3.6449573040008545
epoch 22200  clean testing loss: 9.334471702575684
epoch 22300  training loss: 3.405108690261841

  7%|█████▊                                                                        | 22283/300000 [05:56<1:09:26, 66.65it/s]
epoch 22400  training loss: 3.350714683532715


  8%|█████▉                                                                        | 22913/300000 [06:06<1:09:32, 66.40it/s]
epoch 22500  training loss: 3.7939605712890625
epoch 22500  clean testing loss: 9.773649215698242
epoch 22600  training loss: 3.683398485183716
epoch 22600  clean testing loss: 9.592795372009277
epoch 22700  training loss: 3.258104085922241
epoch 22700  clean testing loss: 9.227508544921875
epoch 22800  training loss: 3.4641900062561035
epoch 22800  clean testing loss: 9.6255464553833
epoch 22900  training loss: 3.2559726238250732

  8%|█████▉                                                                        | 23039/300000 [06:08<1:10:17, 65.67it/s]
epoch 23000  training loss: 3.4709131717681885
epoch 23000  clean testing loss: 9.21274471282959

  8%|██████                                                                        | 23172/300000 [06:10<1:09:36, 66.28it/s]
epoch 23100  training loss: 3.3592820167541504

  8%|██████                                                                        | 23305/300000 [06:12<1:10:52, 65.07it/s]
epoch 23200  training loss: 3.337533950805664
epoch 23200  clean testing loss: 9.548933029174805
epoch 23300  training loss: 3.325817584991455

  8%|██████                                                                        | 23438/300000 [06:14<1:09:39, 66.17it/s]
epoch 23400  training loss: 3.503298044204712

  8%|██████▏                                                                       | 23571/300000 [06:16<1:10:27, 65.39it/s]
epoch 23500  training loss: 3.560877561569214

  8%|██████▏                                                                       | 23704/300000 [06:18<1:09:40, 66.08it/s]
epoch 23600  training loss: 3.2773971557617188
epoch 23600  clean testing loss: 9.471854209899902
epoch 23700  training loss: 3.5838475227355957

  8%|██████▏                                                                       | 23830/300000 [06:20<1:10:10, 65.59it/s]
epoch 23800  training loss: 3.5436060428619385

  8%|██████▏                                                                       | 23963/300000 [06:22<1:09:52, 65.84it/s]
epoch 23900  training loss: 3.4296467304229736

  8%|██████▎                                                                       | 24096/300000 [06:24<1:09:55, 65.76it/s]
epoch 24000  training loss: 3.527146339416504
epoch 24000  clean testing loss: 9.68244743347168
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop1 ...
epoch 24100  training loss: 3.631519079208374

  8%|██████▎                                                                       | 24229/300000 [06:26<1:09:31, 66.12it/s]
epoch 24200  training loss: 3.538468599319458

  8%|██████▎                                                                       | 24355/300000 [06:28<1:12:02, 63.76it/s]
epoch 24300  training loss: 3.591552495956421

  8%|██████▎                                                                       | 24488/300000 [06:30<1:09:14, 66.31it/s]
epoch 24400  training loss: 3.5390734672546387
epoch 24400  clean testing loss: 10.02597713470459
epoch 24500  training loss: 3.2322144508361816

  8%|██████▍                                                                       | 24621/300000 [06:32<1:10:43, 64.90it/s]
epoch 24600  training loss: 3.3380942344665527

  8%|██████▍                                                                       | 24747/300000 [06:34<1:09:47, 65.74it/s]
epoch 24700  training loss: 3.455852508544922
epoch 24700  clean testing loss: 9.629053115844727
epoch 24800  training loss: 3.342829704284668

  8%|██████▍                                                                       | 24880/300000 [06:36<1:09:21, 66.12it/s]
epoch 24900  training loss: 3.7330617904663086
epoch 24900  clean testing loss: 9.627237319946289
epoch 25000  training loss: 3.436260938644409
epoch 25000  clean testing loss: 9.855653762817383


  8%|██████▌                                                                       | 25146/300000 [06:40<1:10:11, 65.26it/s]
epoch 25100  training loss: 3.3707780838012695

  8%|██████▌                                                                       | 25272/300000 [06:42<1:10:37, 64.84it/s]
epoch 25200  training loss: 3.305020809173584

  8%|██████▌                                                                       | 25405/300000 [06:44<1:09:41, 65.67it/s]
epoch 25300  training loss: 3.6356265544891357
epoch 25300  clean testing loss: 10.142793655395508
epoch 25400  training loss: 3.5148203372955322

  9%|██████▋                                                                       | 25538/300000 [06:46<1:09:55, 65.41it/s]
epoch 25500  training loss: 3.2967910766601562

  9%|██████▋                                                                       | 25671/300000 [06:48<1:09:08, 66.12it/s]
epoch 25600  training loss: 3.1865131855010986

  9%|██████▋                                                                       | 25797/300000 [06:50<1:09:40, 65.60it/s]
epoch 25700  training loss: 3.342705011367798
epoch 25700  clean testing loss: 10.020967483520508
epoch 25800  training loss: 3.1024739742279053

  9%|██████▋                                                                       | 25930/300000 [06:52<1:09:41, 65.55it/s]
epoch 25900  training loss: 3.3832950592041016

  9%|██████▊                                                                       | 26063/300000 [06:54<1:18:01, 58.52it/s]
epoch 26000  training loss: 3.4692535400390625
epoch 26000  clean testing loss: 9.746844291687012

  9%|██████▊                                                                       | 26189/300000 [06:56<1:09:51, 65.33it/s]
epoch 26100  training loss: 3.51019549369812

  9%|██████▊                                                                       | 26322/300000 [06:58<1:09:22, 65.74it/s]
epoch 26200  training loss: 3.287620782852173
epoch 26200  clean testing loss: 9.603789329528809
epoch 26300  training loss: 3.224590539932251

  9%|██████▉                                                                       | 26455/300000 [07:00<1:09:07, 65.96it/s]
epoch 26400  training loss: 3.2613272666931152

  9%|██████▉                                                                       | 26588/300000 [07:02<1:10:27, 64.68it/s]
epoch 26500  training loss: 3.358105421066284

  9%|██████▉                                                                       | 26714/300000 [07:04<1:08:41, 66.30it/s]
epoch 26600  training loss: 3.2827677726745605
epoch 26600  clean testing loss: 9.978017807006836
epoch 26700  training loss: 2.9163496494293213

  9%|██████▉                                                                       | 26847/300000 [07:06<1:09:08, 65.85it/s]
epoch 26800  training loss: 3.167051315307617

  9%|███████                                                                       | 26980/300000 [07:08<1:09:19, 65.63it/s]
epoch 26900  training loss: 3.2009334564208984

  9%|███████                                                                       | 27113/300000 [07:10<1:08:32, 66.35it/s]
epoch 27000  training loss: 3.101060152053833
epoch 27000  clean testing loss: 9.347220420837402
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop1 ...
epoch 27100  training loss: 3.199742317199707

  9%|███████                                                                       | 27246/300000 [07:12<1:09:22, 65.52it/s]
epoch 27200  training loss: 3.024442672729492

  9%|███████                                                                       | 27379/300000 [07:14<1:10:02, 64.88it/s]
epoch 27300  training loss: 3.060655355453491

  9%|███████▏                                                                      | 27505/300000 [07:16<1:10:02, 64.84it/s]
epoch 27400  training loss: 3.031813621520996
epoch 27400  clean testing loss: 9.388789176940918
epoch 27500  training loss: 2.9999899864196777

  9%|███████▏                                                                      | 27638/300000 [07:18<1:08:45, 66.02it/s]
epoch 27600  training loss: 3.1579322814941406
epoch 27600  clean testing loss: 9.60385513305664
epoch 27700  training loss: 3.009526014328003
epoch 27700  clean testing loss: 9.708063125610352
epoch 27800  training loss: 3.0441932678222656
epoch 27800  clean testing loss: 9.42653751373291
epoch 27900  training loss: 3.160985231399536
epoch 27900  clean testing loss: 9.313980102539062
epoch 28000  training loss: 3.3663010597229004
epoch 28000  clean testing loss: 9.95727825164795
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop1 ...
epoch 28100  training loss: 3.013091564178467

  9%|███████▎                                                                      | 28128/300000 [07:28<1:09:56, 64.79it/s]
epoch 28200  training loss: 3.119889736175537

  9%|███████▎                                                                      | 28254/300000 [07:30<1:08:53, 65.75it/s]
epoch 28300  training loss: 3.3487179279327393

  9%|███████▍                                                                      | 28387/300000 [07:32<1:08:34, 66.01it/s]
epoch 28400  training loss: 3.150982618331909
epoch 28400  clean testing loss: 9.97952938079834
epoch 28500  training loss: 3.148841381072998

 10%|███████▍                                                                      | 28520/300000 [07:34<1:09:19, 65.27it/s]
epoch 28600  training loss: 2.9361491203308105


 10%|███████▍                                                                      | 28786/300000 [07:39<1:09:06, 65.41it/s]
epoch 28700  training loss: 3.1758744716644287

 10%|███████▌                                                                      | 28919/300000 [07:41<1:08:26, 66.01it/s]
epoch 28800  training loss: 3.3210060596466064
epoch 28800  clean testing loss: 10.234672546386719
epoch 28900  training loss: 3.245065927505493

 10%|███████▌                                                                      | 29045/300000 [07:42<1:08:50, 65.60it/s]
epoch 29000  training loss: 3.339681625366211
epoch 29000  clean testing loss: 10.223498344421387

 10%|███████▌                                                                      | 29178/300000 [07:44<1:08:30, 65.88it/s]
epoch 29100  training loss: 3.1493771076202393

 10%|███████▌                                                                      | 29311/300000 [07:47<1:08:06, 66.25it/s]
epoch 29200  training loss: 3.0846314430236816
epoch 29200  clean testing loss: 9.876440048217773
epoch 29300  training loss: 3.266446113586426

 10%|███████▋                                                                      | 29444/300000 [07:49<1:08:28, 65.85it/s]
epoch 29400  training loss: 3.0806686878204346

 10%|███████▋                                                                      | 29542/300000 [07:50<1:09:50, 64.54it/s]
epoch 29500  training loss: 3.0428028106689453

 10%|███████▋                                                                      | 29668/300000 [07:52<1:09:04, 65.22it/s]
epoch 29600  training loss: 2.9947597980499268
epoch 29600  clean testing loss: 10.107807159423828
epoch 29700  training loss: 2.9124677181243896

 10%|███████▋                                                                      | 29801/300000 [07:54<1:09:02, 65.22it/s]
epoch 29800  training loss: 3.0589489936828613

 10%|███████▊                                                                      | 29934/300000 [07:56<1:09:14, 65.00it/s]
epoch 29900  training loss: 3.15238356590271

 10%|███████▊                                                                      | 30099/300000 [08:07<1:20:04, 56.18it/s]
epoch 30000  training loss: 2.943321943283081
epoch 30000  clean testing loss: 10.812732696533203

 10%|███████▊                                                                      | 30232/300000 [08:09<1:07:18, 66.80it/s]
epoch 30100  training loss: 2.9476020336151123
epoch 30100  clean testing loss: 10.826432228088379
epoch 30200  training loss: 2.928734064102173

 10%|███████▉                                                                      | 30365/300000 [08:11<1:08:44, 65.37it/s]
epoch 30300  training loss: 2.803687572479248

 10%|███████▉                                                                      | 30491/300000 [08:13<1:07:50, 66.22it/s]
epoch 30400  training loss: 3.0354082584381104

 10%|███████▉                                                                      | 30624/300000 [08:15<1:07:54, 66.11it/s]
epoch 30500  training loss: 3.1072652339935303
epoch 30500  clean testing loss: 10.637537956237793
epoch 30600  training loss: 3.230710506439209

 10%|███████▉                                                                      | 30757/300000 [08:17<1:09:35, 64.48it/s]
epoch 30700  training loss: 3.1966328620910645

 10%|████████                                                                      | 30890/300000 [08:19<1:07:54, 66.05it/s]
epoch 30800  training loss: 3.422328233718872
epoch 30800  clean testing loss: 10.0343599319458
epoch 30900  training loss: 3.1622705459594727
epoch 30900  clean testing loss: 9.926889419555664
epoch 31000  training loss: 3.218416690826416
epoch 31000  clean testing loss: 10.196065902709961
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop1 ...
epoch 31100  training loss: 2.892547607421875

 10%|████████                                                                      | 31114/300000 [08:26<1:10:11, 63.85it/s]
epoch 31200  training loss: 3.1430349349975586

 10%|████████                                                                      | 31247/300000 [08:28<1:08:41, 65.21it/s]
epoch 31300  training loss: 3.125223159790039

 10%|████████▏                                                                     | 31380/300000 [08:30<1:07:30, 66.31it/s]
epoch 31400  training loss: 2.689443588256836
epoch 31400  clean testing loss: 10.512392044067383
epoch 31500  training loss: 3.2164061069488525

 11%|████████▏                                                                     | 31513/300000 [08:32<1:08:15, 65.56it/s]
epoch 31600  training loss: 3.3409156799316406

 11%|████████▏                                                                     | 31639/300000 [08:34<1:07:46, 66.00it/s]
epoch 31700  training loss: 3.4063308238983154


 11%|████████▎                                                                     | 31905/300000 [08:38<1:08:06, 65.60it/s]
epoch 31800  training loss: 3.342433452606201

 11%|████████▎                                                                     | 32038/300000 [08:40<1:08:06, 65.58it/s]
epoch 31900  training loss: 3.3949315547943115
epoch 31900  clean testing loss: 9.79326057434082
epoch 32000  training loss: 3.2318382263183594
epoch 32000  clean testing loss: 9.757558822631836

 11%|████████▎                                                                     | 32164/300000 [08:42<1:08:04, 65.58it/s]
epoch 32100  training loss: 3.1035778522491455

 11%|████████▍                                                                     | 32297/300000 [08:44<1:07:41, 65.92it/s]
epoch 32200  training loss: 3.2230656147003174

 11%|████████▍                                                                     | 32430/300000 [08:46<1:07:50, 65.73it/s]
epoch 32300  training loss: 3.0776472091674805
epoch 32300  clean testing loss: 10.416141510009766
epoch 32400  training loss: 3.1046977043151855

 11%|████████▍                                                                     | 32563/300000 [08:48<1:08:07, 65.43it/s]
epoch 32500  training loss: 2.90958309173584
epoch 32500  clean testing loss: 10.501789093017578
epoch 32600  training loss: 2.791598081588745

 11%|████████▌                                                                     | 32696/300000 [08:50<1:07:05, 66.40it/s]
epoch 32700  training loss: 2.734973907470703
epoch 32700  clean testing loss: 10.33177375793457
epoch 32800  training loss: 2.8808298110961914
epoch 32800  clean testing loss: 10.144607543945312
epoch 32900  training loss: 3.0066521167755127
epoch 32900  clean testing loss: 10.306309700012207
epoch 33000  training loss: 2.690185070037842
epoch 33000  clean testing loss: 10.599610328674316

 11%|████████▌                                                                     | 33106/300000 [09:03<1:16:01, 58.51it/s]
epoch 33100  training loss: 2.9110636711120605
epoch 33100  clean testing loss: 10.474841117858887
epoch 33200  training loss: 2.9489309787750244


 11%|████████▋                                                                     | 33372/300000 [09:07<1:07:46, 65.56it/s]
epoch 33300  training loss: 2.759392023086548
epoch 33300  clean testing loss: 10.52673625946045
epoch 33400  training loss: 2.6749043464660645

 11%|████████▋                                                                     | 33505/300000 [09:09<1:07:54, 65.40it/s]
epoch 33500  training loss: 2.6617941856384277
epoch 33500  clean testing loss: 10.422916412353516
epoch 33600  training loss: 2.9059176445007324

 11%|████████▋                                                                     | 33638/300000 [09:11<1:07:49, 65.45it/s]
epoch 33700  training loss: 3.08982253074646

 11%|████████▊                                                                     | 33764/300000 [09:13<1:08:32, 64.74it/s]
epoch 33800  training loss: 2.7855217456817627

 11%|████████▊                                                                     | 33897/300000 [09:15<1:07:17, 65.91it/s]
epoch 33900  training loss: 2.822737455368042
epoch 33900  clean testing loss: 10.96580982208252
epoch 34000  training loss: 2.975482940673828
epoch 34000  clean testing loss: 10.9364013671875

 11%|████████▊                                                                     | 34030/300000 [09:17<1:08:34, 64.64it/s]
epoch 34100  training loss: 3.1122756004333496

 11%|████████▉                                                                     | 34163/300000 [09:19<1:07:46, 65.38it/s]
epoch 34200  training loss: 2.91947078704834

 11%|████████▉                                                                     | 34289/300000 [09:21<1:07:31, 65.59it/s]
epoch 34300  training loss: 2.988642692565918
epoch 34300  clean testing loss: 10.225664138793945
epoch 34400  training loss: 3.2134625911712646

 11%|████████▉                                                                     | 34422/300000 [09:23<1:07:36, 65.47it/s]
epoch 34500  training loss: 3.031029462814331

 12%|████████▉                                                                     | 34555/300000 [09:25<1:07:12, 65.83it/s]
epoch 34600  training loss: 2.8077549934387207

 12%|█████████                                                                     | 34688/300000 [09:27<1:07:02, 65.96it/s]
epoch 34700  training loss: 2.8463521003723145
epoch 34700  clean testing loss: 10.604830741882324
epoch 34800  training loss: 2.7299258708953857

 12%|█████████                                                                     | 34821/300000 [09:29<1:06:58, 66.00it/s]
epoch 34900  training loss: 2.978658676147461

 12%|█████████                                                                     | 34954/300000 [09:31<1:06:39, 66.27it/s]
epoch 35000  training loss: 2.835864782333374
epoch 35000  clean testing loss: 10.809151649475098
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop1 ...
epoch 35100  training loss: 2.898374080657959

 12%|█████████▏                                                                    | 35119/300000 [09:35<1:08:22, 64.56it/s]
epoch 35200  training loss: 2.689680576324463

 12%|█████████▏                                                                    | 35252/300000 [09:37<1:07:07, 65.74it/s]
epoch 35300  training loss: 2.796818256378174

 12%|█████████▏                                                                    | 35385/300000 [09:39<1:07:24, 65.42it/s]
epoch 35400  training loss: 2.7313923835754395

 12%|█████████▏                                                                    | 35518/300000 [09:41<1:06:27, 66.33it/s]
epoch 35500  training loss: 2.7791359424591064
epoch 35500  clean testing loss: 11.480810165405273
epoch 35600  training loss: 2.5881457328796387

 12%|█████████▎                                                                    | 35644/300000 [09:43<1:06:50, 65.92it/s]
epoch 35700  training loss: 2.5326485633850098

 12%|█████████▎                                                                    | 35777/300000 [09:45<1:06:27, 66.27it/s]
epoch 35800  training loss: 2.8585424423217773

 12%|█████████▎                                                                    | 35910/300000 [09:47<1:07:42, 65.01it/s]
epoch 35900  training loss: 2.708672046661377
epoch 35900  clean testing loss: 11.06222152709961
epoch 36000  training loss: 2.6956467628479004
epoch 36000  clean testing loss: 10.656488418579102

 12%|█████████▎                                                                    | 36043/300000 [09:49<1:07:20, 65.32it/s]
epoch 36100  training loss: 2.6211190223693848

 12%|█████████▍                                                                    | 36176/300000 [09:51<1:07:17, 65.35it/s]
epoch 36200  training loss: 2.7744596004486084

 12%|█████████▍                                                                    | 36309/300000 [09:53<1:06:53, 65.70it/s]
epoch 36300  training loss: 3.310482978820801
epoch 36300  clean testing loss: 10.711288452148438
epoch 36400  training loss: 2.7651288509368896

 12%|█████████▍                                                                    | 36435/300000 [09:55<1:07:06, 65.46it/s]
epoch 36500  training loss: 2.7055585384368896

 12%|█████████▌                                                                    | 36568/300000 [09:57<1:06:58, 65.55it/s]
epoch 36600  training loss: 2.678694486618042


 12%|█████████▋                                                                    | 37094/300000 [10:05<1:06:35, 65.80it/s]
epoch 36700  training loss: 2.690721035003662
epoch 36700  clean testing loss: 10.923250198364258
epoch 36800  training loss: 2.7538418769836426
epoch 36800  clean testing loss: 10.92276382446289
epoch 36900  training loss: 2.5810647010803223
epoch 36900  clean testing loss: 10.610793113708496
epoch 37000  training loss: 2.6528313159942627
epoch 37000  clean testing loss: 10.717345237731934
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop1 ...
epoch 37100  training loss: 2.4947726726531982

 12%|█████████▋                                                                    | 37227/300000 [10:07<1:06:51, 65.51it/s]
epoch 37200  training loss: 2.651503086090088

 12%|█████████▋                                                                    | 37360/300000 [10:09<1:06:03, 66.26it/s]
epoch 37300  training loss: 2.4835219383239746
epoch 37300  clean testing loss: 10.568717956542969
epoch 37400  training loss: 2.7095508575439453
epoch 37400  clean testing loss: 10.527188301086426
epoch 37500  training loss: 2.5705573558807373

 12%|█████████▋                                                                    | 37493/300000 [10:11<1:06:45, 65.54it/s]
epoch 37600  training loss: 2.441105842590332
epoch 37600  clean testing loss: 10.676543235778809
epoch 37700  training loss: 2.6721138954162598
epoch 37700  clean testing loss: 10.472199440002441
epoch 37800  training loss: 2.586068868637085
epoch 37800  clean testing loss: 10.34745979309082
epoch 37900  training loss: 2.7741618156433105
epoch 37900  clean testing loss: 10.52100658416748
epoch 38000  training loss: 2.6855854988098145
epoch 38000  clean testing loss: 10.622278213500977

 13%|█████████▉                                                                    | 38116/300000 [10:21<1:05:28, 66.66it/s]
epoch 38100  training loss: 2.387115716934204
epoch 38100  clean testing loss: 10.491644859313965
epoch 38200  training loss: 2.7007412910461426


 13%|█████████▉                                                                    | 38382/300000 [10:25<1:06:44, 65.33it/s]
epoch 38300  training loss: 2.8699772357940674

 13%|██████████                                                                    | 38515/300000 [10:27<1:06:48, 65.23it/s]
epoch 38400  training loss: 2.6546638011932373

 13%|██████████                                                                    | 38648/300000 [10:29<1:07:08, 64.88it/s]
epoch 38500  training loss: 2.6019599437713623
epoch 38500  clean testing loss: 10.289851188659668
epoch 38600  training loss: 2.827986478805542

 13%|██████████                                                                    | 38774/300000 [10:31<1:05:56, 66.03it/s]
epoch 38700  training loss: 2.6425511837005615

 13%|██████████                                                                    | 38907/300000 [10:33<1:06:11, 65.74it/s]
epoch 38800  training loss: 2.5185635089874268

 13%|██████████▏                                                                   | 39099/300000 [10:39<1:10:05, 62.04it/s]
epoch 38900  training loss: 2.6765329837799072
epoch 38900  clean testing loss: 10.372801780700684
epoch 39000  training loss: 2.7334165573120117
epoch 39000  clean testing loss: 10.327058792114258
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop1 ...
epoch 39100  training loss: 2.696411609649658

 13%|██████████▏                                                                   | 39232/300000 [10:41<1:05:59, 65.86it/s]
epoch 39200  training loss: 2.8510708808898926

 13%|██████████▏                                                                   | 39365/300000 [10:43<1:05:51, 65.95it/s]
epoch 39300  training loss: 2.6562130451202393

 13%|██████████▎                                                                   | 39498/300000 [10:45<1:05:48, 65.97it/s]
epoch 39400  training loss: 2.58076810836792
epoch 39400  clean testing loss: 10.646506309509277
epoch 39500  training loss: 2.7405056953430176

 13%|██████████▎                                                                   | 39624/300000 [10:47<1:06:30, 65.25it/s]
epoch 39600  training loss: 2.6530895233154297

 13%|██████████▎                                                                   | 39757/300000 [10:49<1:06:19, 65.39it/s]
epoch 39700  training loss: 2.6062049865722656

 13%|██████████▎                                                                   | 39890/300000 [10:51<1:05:29, 66.19it/s]
epoch 39800  training loss: 2.657372236251831

 13%|██████████▍                                                                   | 40023/300000 [10:53<1:07:17, 64.39it/s]
epoch 39900  training loss: 2.3781955242156982
epoch 39900  clean testing loss: 10.536795616149902
epoch 40000  training loss: 2.363412618637085
epoch 40000  clean testing loss: 10.563045501708984

 13%|██████████▍                                                                   | 40149/300000 [10:55<1:05:28, 66.14it/s]
epoch 40100  training loss: 2.478276491165161

 13%|██████████▍                                                                   | 40282/300000 [10:57<1:05:26, 66.15it/s]
epoch 40200  training loss: 2.6693785190582275

 13%|██████████▌                                                                   | 40415/300000 [10:59<1:04:56, 66.63it/s]
epoch 40300  training loss: 2.5086441040039062
epoch 40300  clean testing loss: 10.881491661071777
epoch 40400  training loss: 2.46588397026062

 14%|██████████▌                                                                   | 40548/300000 [11:01<1:05:30, 66.00it/s]
epoch 40500  training loss: 2.631417989730835

 14%|██████████▌                                                                   | 40681/300000 [11:03<1:05:25, 66.06it/s]
epoch 40600  training loss: 2.414477586746216

 14%|██████████▌                                                                   | 40814/300000 [11:05<1:05:00, 66.44it/s]
epoch 40700  training loss: 2.4753971099853516
epoch 40700  clean testing loss: 11.371013641357422
epoch 40800  training loss: 2.508605480194092

 14%|██████████▋                                                                   | 40940/300000 [11:07<1:06:26, 64.98it/s]
epoch 40900  training loss: 2.406717300415039

 14%|██████████▋                                                                   | 41073/300000 [11:09<1:05:41, 65.69it/s]
epoch 41000  training loss: 2.3504250049591064
epoch 41000  clean testing loss: 11.337946891784668

 14%|██████████▋                                                                   | 41206/300000 [11:11<1:06:07, 65.23it/s]
epoch 41100  training loss: 2.4319937229156494
epoch 41100  clean testing loss: 11.157431602478027
epoch 41200  training loss: 2.5480799674987793

 14%|██████████▋                                                                   | 41339/300000 [11:13<1:06:10, 65.14it/s]
epoch 41300  training loss: 2.6372525691986084

 14%|██████████▊                                                                   | 41472/300000 [11:15<1:05:42, 65.58it/s]
epoch 41400  training loss: 2.5220513343811035

 14%|██████████▊                                                                   | 41598/300000 [11:17<1:05:09, 66.09it/s]
epoch 41500  training loss: 2.6661460399627686
epoch 41500  clean testing loss: 11.598543167114258
epoch 41600  training loss: 2.4889986515045166

 14%|██████████▊                                                                   | 41731/300000 [11:19<1:05:21, 65.85it/s]
epoch 41700  training loss: 2.6127264499664307

 14%|██████████▉                                                                   | 41864/300000 [11:21<1:05:29, 65.69it/s]
epoch 41800  training loss: 2.2642569541931152

 14%|██████████▉                                                                   | 41997/300000 [11:23<1:05:06, 66.04it/s]
epoch 41900  training loss: 2.512880563735962

 14%|██████████▉                                                                   | 42123/300000 [11:25<1:05:19, 65.79it/s]
epoch 42000  training loss: 2.389176368713379
epoch 42000  clean testing loss: 11.593820571899414
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop1 ...
epoch 42100  training loss: 2.318908214569092

 14%|██████████▉                                                                   | 42256/300000 [11:27<1:05:40, 65.41it/s]
epoch 42200  training loss: 2.5461840629577637

 14%|███████████                                                                   | 42389/300000 [11:29<1:05:20, 65.71it/s]
epoch 42300  training loss: 2.639385938644409

 14%|███████████                                                                   | 42522/300000 [11:31<1:05:05, 65.93it/s]
epoch 42400  training loss: 2.5812277793884277
epoch 42400  clean testing loss: 11.876603126525879
epoch 42500  training loss: 2.3721842765808105

 14%|███████████                                                                   | 42655/300000 [11:33<1:04:42, 66.29it/s]
epoch 42600  training loss: 2.5545871257781982
epoch 42600  clean testing loss: 11.830975532531738
epoch 42700  training loss: 2.5133309364318848
epoch 42700  clean testing loss: 11.541476249694824
epoch 42800  training loss: 2.4674761295318604
epoch 42800  clean testing loss: 11.981660842895508
epoch 42900  training loss: 2.502382278442383
epoch 42900  clean testing loss: 11.86050033569336
epoch 43000  training loss: 2.5845839977264404
epoch 43000  clean testing loss: 11.786354064941406

 14%|███████████▏                                                                  | 43019/300000 [11:39<1:05:38, 65.25it/s]
epoch 43100  training loss: 2.6836776733398438

 14%|███████████▏                                                                  | 43145/300000 [11:41<1:05:01, 65.84it/s]
epoch 43200  training loss: 2.485952377319336

 14%|███████████▎                                                                  | 43278/300000 [11:43<1:04:48, 66.02it/s]
epoch 43300  training loss: 2.3533122539520264
epoch 43300  clean testing loss: 11.69487190246582
epoch 43400  training loss: 2.727337598800659

 14%|███████████▎                                                                  | 43411/300000 [11:45<1:04:23, 66.41it/s]
epoch 43500  training loss: 2.489410400390625

 15%|███████████▎                                                                  | 43544/300000 [11:47<1:04:58, 65.79it/s]
epoch 43600  training loss: 2.464611530303955

 15%|███████████▎                                                                  | 43677/300000 [11:49<1:05:18, 65.41it/s]
epoch 43700  training loss: 2.3124704360961914
epoch 43700  clean testing loss: 11.882938385009766
epoch 43800  training loss: 2.5511701107025146

 15%|███████████▍                                                                  | 43810/300000 [11:51<1:05:15, 65.44it/s]
epoch 43900  training loss: 2.414590835571289

 15%|███████████▍                                                                  | 43943/300000 [11:53<1:05:00, 65.65it/s]
epoch 44000  training loss: 2.499636650085449
epoch 44000  clean testing loss: 11.763609886169434

 15%|███████████▍                                                                  | 44069/300000 [11:55<1:04:33, 66.08it/s]
epoch 44100  training loss: 2.615659475326538
epoch 44100  clean testing loss: 11.684762001037598
epoch 44200  training loss: 2.5905518531799316

 15%|███████████▍                                                                  | 44202/300000 [11:57<1:05:03, 65.53it/s]
epoch 44300  training loss: 2.5256154537200928

 15%|███████████▌                                                                  | 44335/300000 [11:59<1:04:38, 65.92it/s]
epoch 44400  training loss: 2.2208707332611084

 15%|███████████▌                                                                  | 44468/300000 [12:01<1:04:12, 66.33it/s]
epoch 44500  training loss: 2.4187843799591064

 15%|███████████▌                                                                  | 44601/300000 [12:03<1:05:39, 64.83it/s]
epoch 44600  training loss: 2.411334276199341
epoch 44600  clean testing loss: 11.518026351928711
epoch 44700  training loss: 2.4052231311798096


 15%|███████████▋                                                                  | 45028/300000 [12:09<1:06:01, 64.37it/s]
epoch 44800  training loss: 2.4457173347473145
epoch 44800  clean testing loss: 11.373186111450195
epoch 44900  training loss: 2.3694570064544678
epoch 44900  clean testing loss: 11.2044095993042
epoch 45000  training loss: 2.3472506999969482
epoch 45000  clean testing loss: 11.345124244689941

 15%|███████████▋                                                                  | 45154/300000 [12:11<1:05:08, 65.20it/s]
epoch 45100  training loss: 2.522491216659546

 15%|███████████▊                                                                  | 45287/300000 [12:13<1:04:09, 66.17it/s]
epoch 45200  training loss: 2.7301037311553955
epoch 45200  clean testing loss: 11.200400352478027
epoch 45300  training loss: 2.5054938793182373

 15%|███████████▊                                                                  | 45420/300000 [12:15<1:05:10, 65.10it/s]
epoch 45400  training loss: 2.690338373184204

 15%|███████████▊                                                                  | 45553/300000 [12:17<1:03:54, 66.36it/s]
epoch 45500  training loss: 2.6116108894348145

 15%|███████████▉                                                                  | 45686/300000 [12:19<1:03:53, 66.35it/s]
epoch 45600  training loss: 2.538787603378296
epoch 45600  clean testing loss: 11.340396881103516
epoch 45700  training loss: 2.6582369804382324

 15%|███████████▉                                                                  | 45819/300000 [12:21<1:05:12, 64.96it/s]
epoch 45800  training loss: 2.5972580909729004

 15%|███████████▉                                                                  | 45952/300000 [12:23<1:05:10, 64.96it/s]
epoch 45900  training loss: 2.851177930831909

 15%|███████████▉                                                                  | 46078/300000 [12:25<1:04:54, 65.20it/s]
epoch 46000  training loss: 2.718062162399292
epoch 46000  clean testing loss: 11.010051727294922
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop1 ...
epoch 46100  training loss: 2.895565986633301

 15%|████████████                                                                  | 46211/300000 [12:27<1:05:19, 64.74it/s]
epoch 46200  training loss: 2.822751522064209

 15%|████████████                                                                  | 46344/300000 [12:29<1:03:31, 66.55it/s]
epoch 46300  training loss: 2.7936437129974365
epoch 46300  clean testing loss: 11.029474258422852
epoch 46400  training loss: 2.879046678543091

 15%|████████████                                                                  | 46477/300000 [12:31<1:04:11, 65.83it/s]
epoch 46500  training loss: 2.7861080169677734
epoch 46500  clean testing loss: 10.634636878967285
epoch 46600  training loss: 2.64888334274292

 16%|████████████                                                                  | 46603/300000 [12:33<1:04:21, 65.62it/s]
epoch 46700  training loss: 2.6292030811309814

 16%|████████████▏                                                                 | 46736/300000 [12:35<1:03:59, 65.96it/s]
epoch 46800  training loss: 2.7453579902648926

 16%|████████████▏                                                                 | 46869/300000 [12:37<1:03:59, 65.92it/s]
epoch 46900  training loss: 2.754307270050049
epoch 46900  clean testing loss: 10.387858390808105
epoch 47000  training loss: 2.8981189727783203
epoch 47000  clean testing loss: 10.435359001159668

 16%|████████████▏                                                                 | 47002/300000 [12:39<1:04:21, 65.51it/s]
epoch 47100  training loss: 2.9545209407806396

 16%|████████████▎                                                                 | 47135/300000 [12:42<1:03:54, 65.95it/s]
epoch 47200  training loss: 3.2032179832458496


 16%|████████████▍                                                                 | 47758/300000 [12:51<1:03:50, 65.85it/s]
epoch 47300  training loss: 3.115520477294922
epoch 47300  clean testing loss: 11.230998039245605
epoch 47400  training loss: 3.3230111598968506
epoch 47400  clean testing loss: 11.015006065368652
epoch 47500  training loss: 3.38651967048645
epoch 47500  clean testing loss: 11.206674575805664
epoch 47600  training loss: 3.602423906326294
epoch 47600  clean testing loss: 11.120437622070312
epoch 47700  training loss: 4.009040355682373

 16%|████████████▍                                                                 | 47891/300000 [12:53<1:03:47, 65.87it/s]
epoch 47800  training loss: 4.0598835945129395

 16%|████████████▍                                                                 | 48024/300000 [12:55<1:04:48, 64.79it/s]
epoch 47900  training loss: 3.7076358795166016
epoch 47900  clean testing loss: 11.13062572479248
epoch 48000  training loss: 3.825545072555542
epoch 48000  clean testing loss: 11.1477689743042

 16%|████████████▌                                                                 | 48157/300000 [12:57<1:03:47, 65.80it/s]
epoch 48100  training loss: 3.5945534706115723

 16%|████████████▌                                                                 | 48283/300000 [12:59<1:03:01, 66.57it/s]
epoch 48200  training loss: 3.759761333465576

 16%|████████████▌                                                                 | 48416/300000 [13:01<1:03:15, 66.29it/s]
epoch 48300  training loss: 3.8584861755371094
epoch 48300  clean testing loss: 11.333956718444824
epoch 48400  training loss: 3.771646738052368
epoch 48400  clean testing loss: 11.231731414794922
epoch 48500  training loss: 3.8557076454162598
epoch 48500  clean testing loss: 10.957319259643555
epoch 48600  training loss: 3.563103199005127
epoch 48600  clean testing loss: 10.86914348602295
epoch 48700  training loss: 3.4819235801696777
epoch 48700  clean testing loss: 10.819137573242188
epoch 48800  training loss: 3.3876802921295166
epoch 48800  clean testing loss: 10.795012474060059
epoch 48900  training loss: 3.4688146114349365
epoch 48900  clean testing loss: 11.069866180419922
epoch 49000  training loss: 3.3363351821899414
epoch 49000  clean testing loss: 11.01917839050293
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop1 ...
epoch 49100  training loss: 3.34401273727417

 16%|████████████▊                                                                 | 49119/300000 [13:13<1:04:00, 65.32it/s]
epoch 49200  training loss: 3.3066186904907227

 16%|████████████▊                                                                 | 49245/300000 [13:15<1:03:30, 65.81it/s]
epoch 49300  training loss: 3.231011390686035

 16%|████████████▊                                                                 | 49378/300000 [13:17<1:03:29, 65.78it/s]
epoch 49400  training loss: 3.2637007236480713

 17%|████████████▊                                                                 | 49511/300000 [13:19<1:04:15, 64.96it/s]
epoch 49500  training loss: 3.1913177967071533
epoch 49500  clean testing loss: 11.270235061645508
epoch 49600  training loss: 3.097180128097534

 17%|████████████▉                                                                 | 49644/300000 [13:21<1:03:02, 66.18it/s]
epoch 49700  training loss: 3.043701648712158

 17%|█████████████                                                                 | 50071/300000 [13:27<1:03:04, 66.04it/s]
epoch 49800  training loss: 3.0965802669525146
epoch 49800  clean testing loss: 11.248983383178711
epoch 49900  training loss: 3.2309792041778564
epoch 49900  clean testing loss: 11.233312606811523
epoch 50000  training loss: 3.278982400894165
epoch 50000  clean testing loss: 11.607841491699219

 17%|█████████████                                                                 | 50204/300000 [13:29<1:03:33, 65.50it/s]
epoch 50100  training loss: 3.171851634979248
epoch 50100  clean testing loss: 11.450501441955566
epoch 50200  training loss: 3.0582082271575928

 17%|█████████████                                                                 | 50337/300000 [13:31<1:03:26, 65.58it/s]
epoch 50300  training loss: 3.1639764308929443

 17%|█████████████                                                                 | 50470/300000 [13:33<1:03:02, 65.97it/s]
epoch 50400  training loss: 3.1854324340820312

 17%|█████████████▏                                                                | 50603/300000 [13:35<1:03:26, 65.52it/s]
epoch 50500  training loss: 3.329171657562256
epoch 50500  clean testing loss: 11.17519760131836
epoch 50600  training loss: 3.113997220993042

 17%|█████████████▏                                                                | 50729/300000 [13:37<1:03:34, 65.35it/s]
epoch 50700  training loss: 3.14428973197937

 17%|█████████████▏                                                                | 50863/300000 [13:39<1:03:12, 65.69it/s]
epoch 50800  training loss: 3.317603826522827

 17%|█████████████▎                                                                | 50996/300000 [13:41<1:02:56, 65.94it/s]
epoch 50900  training loss: 2.94720721244812
epoch 50900  clean testing loss: 10.951457023620605
epoch 51000  training loss: 2.965501308441162
epoch 51000  clean testing loss: 11.121474266052246

 17%|█████████████▎                                                                | 51129/300000 [13:43<1:02:40, 66.19it/s]
epoch 51100  training loss: 3.19820499420166

 17%|█████████████▎                                                                | 51262/300000 [13:45<1:03:16, 65.53it/s]
epoch 51200  training loss: 3.0376689434051514

 17%|█████████████▍                                                                | 51619/300000 [13:51<1:03:18, 65.39it/s]
epoch 51300  training loss: 2.96417498588562
epoch 51300  clean testing loss: 10.888680458068848
epoch 51400  training loss: 3.1372764110565186
epoch 51400  clean testing loss: 10.848428726196289
epoch 51500  training loss: 2.963420867919922
epoch 51500  clean testing loss: 10.768457412719727
epoch 51600  training loss: 2.966834545135498
epoch 51600  clean testing loss: 10.473775863647461
epoch 51700  training loss: 3.1538565158843994


 17%|█████████████▍                                                                | 51878/300000 [13:55<1:03:17, 65.33it/s]
epoch 51800  training loss: 3.0864434242248535
epoch 51800  clean testing loss: 10.652140617370605
epoch 51900  training loss: 3.010896921157837
epoch 51900  clean testing loss: 10.520752906799316
epoch 52000  training loss: 2.9442272186279297
epoch 52000  clean testing loss: 10.430127143859863


 17%|█████████████▌                                                                | 52137/300000 [13:59<1:07:32, 61.16it/s]
epoch 52100  training loss: 3.075500726699829

 17%|█████████████▌                                                                | 52270/300000 [14:01<1:03:38, 64.87it/s]
epoch 52200  training loss: 2.9129161834716797

 17%|█████████████▌                                                                | 52403/300000 [14:03<1:02:33, 65.96it/s]
epoch 52300  training loss: 2.95223069190979
epoch 52300  clean testing loss: 10.486614227294922
epoch 52400  training loss: 2.805851697921753

 18%|█████████████▋                                                                | 52529/300000 [14:05<1:02:36, 65.89it/s]
epoch 52500  training loss: 3.0038843154907227

 18%|█████████████▋                                                                | 52662/300000 [14:07<1:02:54, 65.53it/s]
epoch 52600  training loss: 3.0087785720825195

 18%|█████████████▋                                                                | 52795/300000 [14:09<1:02:40, 65.73it/s]
epoch 52700  training loss: 3.120377540588379
epoch 52700  clean testing loss: 10.597627639770508
epoch 52800  training loss: 2.8236546516418457

 18%|█████████████▊                                                                | 52928/300000 [14:11<1:02:17, 66.11it/s]
epoch 52900  training loss: 2.8873848915100098

 18%|██████████████▏                                                                 | 52997/300000 [14:12<53:36, 76.80it/s]
epoch 53000  training loss: 2.9476983547210693
epoch 53000  clean testing loss: 10.566625595092773

 18%|█████████████▊                                                                | 53103/300000 [14:23<1:13:42, 55.83it/s]
epoch 53100  training loss: 3.1819629669189453

 18%|█████████████▊                                                                | 53236/300000 [14:25<1:03:05, 65.18it/s]
epoch 53200  training loss: 3.0982320308685303
epoch 53200  clean testing loss: 10.81731128692627
epoch 53300  training loss: 2.942589521408081


 18%|█████████████▉                                                                | 53502/300000 [14:29<1:02:52, 65.34it/s]
epoch 53400  training loss: 2.9701030254364014
epoch 53400  clean testing loss: 10.688572883605957
epoch 53500  training loss: 3.253215789794922

 18%|█████████████▉                                                                | 53628/300000 [14:31<1:02:09, 66.07it/s]
epoch 53600  training loss: 3.1544806957244873

 18%|█████████████▉                                                                | 53761/300000 [14:33<1:02:49, 65.33it/s]
epoch 53700  training loss: 3.2253308296203613

 18%|██████████████                                                                | 53894/300000 [14:35<1:02:01, 66.13it/s]
epoch 53800  training loss: 3.164532423019409
epoch 53800  clean testing loss: 11.102240562438965
epoch 53900  training loss: 2.8963351249694824

 18%|██████████████                                                                | 54119/300000 [14:47<1:06:48, 61.34it/s]
epoch 54000  training loss: 3.0493767261505127
epoch 54000  clean testing loss: 11.012503623962402
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop1 ...
epoch 54100  training loss: 3.057002544403076

 18%|██████████████                                                                | 54252/300000 [14:49<1:02:25, 65.61it/s]
epoch 54200  training loss: 3.0488834381103516

 18%|██████████████▏                                                               | 54378/300000 [14:51<1:03:00, 64.96it/s]
epoch 54300  training loss: 2.8450093269348145

 18%|██████████████▏                                                               | 54511/300000 [14:53<1:02:06, 65.88it/s]
epoch 54400  training loss: 2.944340467453003
epoch 54400  clean testing loss: 10.743545532226562
epoch 54500  training loss: 2.9506449699401855
epoch 54500  clean testing loss: 10.699932098388672
epoch 54600  training loss: 2.921628713607788

 18%|██████████████▏                                                               | 54644/300000 [14:55<1:01:51, 66.10it/s]
epoch 54700  training loss: 2.9771132469177246

 18%|██████████████▏                                                               | 54777/300000 [14:57<1:01:32, 66.42it/s]
epoch 54800  training loss: 3.098414182662964
epoch 54800  clean testing loss: 10.881227493286133
epoch 54900  training loss: 3.070924997329712

 18%|██████████████▎                                                               | 54910/300000 [15:00<1:02:39, 65.20it/s]
epoch 55000  training loss: 3.0924830436706543
epoch 55000  clean testing loss: 11.027604103088379

 18%|██████████████▎                                                               | 55036/300000 [15:01<1:02:01, 65.82it/s]
epoch 55100  training loss: 3.006251573562622

 18%|██████████████▎                                                               | 55169/300000 [15:03<1:01:47, 66.04it/s]
epoch 55200  training loss: 2.998087167739868
epoch 55200  clean testing loss: 11.081624984741211
epoch 55300  training loss: 3.061809778213501

 18%|██████████████▍                                                               | 55302/300000 [15:05<1:02:11, 65.58it/s]
epoch 55400  training loss: 3.2802891731262207

 18%|██████████████▍                                                               | 55435/300000 [15:08<1:02:16, 65.46it/s]
epoch 55500  training loss: 3.295992374420166

 19%|██████████████▍                                                               | 55568/300000 [15:10<1:01:46, 65.94it/s]
epoch 55600  training loss: 3.339463949203491
epoch 55600  clean testing loss: 10.994965553283691
epoch 55700  training loss: 3.3550474643707275

 19%|██████████████▍                                                               | 55701/300000 [15:12<1:01:49, 65.86it/s]
epoch 55800  training loss: 3.3166558742523193

 19%|██████████████▌                                                               | 55827/300000 [15:13<1:02:42, 64.90it/s]
epoch 55900  training loss: 3.3115317821502686

 19%|██████████████▌                                                               | 55960/300000 [15:15<1:02:10, 65.41it/s]
epoch 56000  training loss: 3.1315290927886963
epoch 56000  clean testing loss: 10.810738563537598

 19%|██████████████▌                                                               | 56093/300000 [15:18<1:02:19, 65.23it/s]
epoch 56100  training loss: 2.942152261734009
epoch 56100  clean testing loss: 10.757172584533691
epoch 56200  training loss: 3.1226818561553955

 19%|██████████████▌                                                               | 56226/300000 [15:20<1:01:42, 65.84it/s]
epoch 56300  training loss: 3.0593152046203613

 19%|██████████████▋                                                               | 56359/300000 [15:22<1:01:14, 66.30it/s]
epoch 56400  training loss: 3.083277940750122

 19%|██████████████▋                                                               | 56492/300000 [15:24<1:02:09, 65.29it/s]
epoch 56500  training loss: 2.9724435806274414
epoch 56500  clean testing loss: 10.788185119628906
epoch 56600  training loss: 3.148131847381592

 19%|██████████████▋                                                               | 56618/300000 [15:26<1:01:31, 65.92it/s]
epoch 56700  training loss: 3.1176910400390625

 19%|██████████████▊                                                               | 56751/300000 [15:28<1:02:49, 64.53it/s]
epoch 56800  training loss: 3.10461163520813

 19%|██████████████▊                                                               | 56884/300000 [15:30<1:00:49, 66.61it/s]
epoch 56900  training loss: 3.260592222213745
epoch 56900  clean testing loss: 10.609299659729004
epoch 57000  training loss: 3.0278990268707275
epoch 57000  clean testing loss: 10.726101875305176
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop1 ...
epoch 57100  training loss: 3.2167160511016846


 19%|██████████████▉                                                               | 57260/300000 [15:44<1:01:03, 66.26it/s]
epoch 57200  training loss: 3.0920591354370117

 19%|██████████████▉                                                               | 57393/300000 [15:46<1:01:36, 65.63it/s]
epoch 57300  training loss: 3.1627702713012695
epoch 57300  clean testing loss: 10.476349830627441
epoch 57400  training loss: 2.9433205127716064
epoch 57400  clean testing loss: 10.650412559509277
epoch 57500  training loss: 2.8963675498962402

 19%|██████████████▉                                                               | 57519/300000 [15:48<1:01:46, 65.42it/s]
epoch 57600  training loss: 3.292187213897705


 19%|███████████████                                                               | 57785/300000 [15:52<1:02:34, 64.51it/s]
epoch 57700  training loss: 3.13698410987854

 19%|███████████████                                                               | 57911/300000 [15:54<1:01:58, 65.10it/s]
epoch 57800  training loss: 3.1862242221832275
epoch 57800  clean testing loss: 10.616705894470215
epoch 57900  training loss: 2.85901141166687

 19%|███████████████▍                                                                | 57996/300000 [15:55<58:22, 69.09it/s]
epoch 58000  training loss: 3.3263957500457764
epoch 58000  clean testing loss: 10.632439613342285

 19%|███████████████                                                               | 58115/300000 [15:58<1:02:00, 65.01it/s]
epoch 58100  training loss: 2.9372920989990234

 19%|███████████████▏                                                              | 58241/300000 [16:00<1:00:54, 66.16it/s]
epoch 58200  training loss: 2.9456729888916016

 19%|███████████████▏                                                              | 58374/300000 [16:02<1:01:52, 65.09it/s]
epoch 58300  training loss: 2.9204294681549072

 20%|███████████████▏                                                              | 58507/300000 [16:04<1:01:53, 65.03it/s]
epoch 58400  training loss: 2.919017791748047
epoch 58400  clean testing loss: 10.496588706970215
epoch 58500  training loss: 2.858999729156494

 20%|███████████████▏                                                              | 58640/300000 [16:06<1:01:31, 65.37it/s]
epoch 58600  training loss: 3.305002212524414

 20%|███████████████▎                                                              | 58773/300000 [16:08<1:01:25, 65.45it/s]
epoch 58700  training loss: 2.9996988773345947

 20%|███████████████▎                                                              | 58899/300000 [16:10<1:01:55, 64.88it/s]
epoch 58800  training loss: 3.2182457447052

 20%|███████████████▎                                                              | 59032/300000 [16:12<1:01:51, 64.92it/s]
epoch 58900  training loss: 2.8340723514556885
epoch 58900  clean testing loss: 11.110708236694336
epoch 59000  training loss: 3.1019434928894043
epoch 59000  clean testing loss: 10.742155075073242

 20%|███████████████▍                                                              | 59165/300000 [16:14<1:01:26, 65.32it/s]
epoch 59100  training loss: 3.0528957843780518

 20%|███████████████▍                                                              | 59291/300000 [16:16<1:00:49, 65.95it/s]
epoch 59200  training loss: 3.19571852684021

 20%|███████████████▍                                                              | 59424/300000 [16:18<1:00:44, 66.02it/s]
epoch 59300  training loss: 3.3201236724853516
epoch 59300  clean testing loss: 10.692461013793945
epoch 59400  training loss: 3.103809118270874

 20%|███████████████▍                                                              | 59557/300000 [16:20<1:00:28, 66.27it/s]
epoch 59500  training loss: 3.0739293098449707
epoch 59500  clean testing loss: 11.086719512939453
epoch 59600  training loss: 3.1716554164886475
epoch 59600  clean testing loss: 11.203076362609863
epoch 59700  training loss: 3.269791603088379
epoch 59700  clean testing loss: 11.294231414794922
epoch 59800  training loss: 3.3162636756896973
epoch 59800  clean testing loss: 11.254859924316406
epoch 59900  training loss: 3.285944938659668
epoch 59900  clean testing loss: 11.255522727966309
epoch 60000  training loss: 3.6065571308135986
epoch 60000  clean testing loss: 11.199258804321289
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop1 ...
epoch 60100  training loss: 3.4862706661224365

 20%|███████████████▋                                                              | 60127/300000 [16:31<1:01:25, 65.09it/s]
epoch 60200  training loss: 3.554002523422241

 20%|███████████████▋                                                              | 60260/300000 [16:33<1:00:26, 66.12it/s]
epoch 60300  training loss: 3.4122393131256104

 20%|███████████████▋                                                              | 60386/300000 [16:35<1:00:50, 65.63it/s]
epoch 60400  training loss: 3.2776777744293213
epoch 60400  clean testing loss: 11.472484588623047
epoch 60500  training loss: 3.184431314468384

 20%|███████████████▋                                                              | 60519/300000 [16:37<1:00:53, 65.55it/s]
epoch 60600  training loss: 3.561878204345703

 20%|████████████████▏                                                               | 60652/300000 [16:39<59:54, 66.58it/s]
epoch 60700  training loss: 3.403796434402466

 20%|███████████████▊                                                              | 60785/300000 [16:41<1:00:12, 66.21it/s]
epoch 60800  training loss: 3.271186351776123
epoch 60800  clean testing loss: 11.640449523925781
epoch 60900  training loss: 3.2034506797790527

 20%|███████████████▊                                                              | 60918/300000 [16:43<1:00:41, 65.66it/s]
epoch 61000  training loss: 3.244112968444824
epoch 61000  clean testing loss: 11.601962089538574

 20%|███████████████▊                                                              | 61044/300000 [16:45<1:01:51, 64.38it/s]
epoch 61100  training loss: 3.076242446899414

 20%|███████████████▉                                                              | 61149/300000 [16:46<1:00:27, 65.84it/s]
epoch 61200  training loss: 3.2584009170532227
epoch 61200  clean testing loss: 11.343249320983887
epoch 61300  training loss: 3.517359972000122

 20%|███████████████▉                                                              | 61275/300000 [16:48<1:00:44, 65.49it/s]
epoch 61400  training loss: 3.441433906555176


 21%|████████████████                                                              | 61541/300000 [16:52<1:00:19, 65.88it/s]
epoch 61500  training loss: 3.3164710998535156
epoch 61500  clean testing loss: 11.317639350891113
epoch 61600  training loss: 3.2817976474761963

 21%|████████████████                                                              | 61674/300000 [16:54<1:00:12, 65.97it/s]
epoch 61700  training loss: 3.3171398639678955
epoch 61700  clean testing loss: 11.06666088104248
epoch 61800  training loss: 3.2818737030029297
epoch 61800  clean testing loss: 11.076119422912598
epoch 61900  training loss: 3.3905704021453857
epoch 61900  clean testing loss: 10.785567283630371
epoch 62000  training loss: 3.633526086807251
epoch 62000  clean testing loss: 11.061066627502441

 21%|████████████████▏                                                             | 62038/300000 [17:00<1:00:09, 65.92it/s]
epoch 62100  training loss: 3.5641591548919678

 21%|████████████████▌                                                               | 62164/300000 [17:02<59:52, 66.20it/s]
epoch 62200  training loss: 3.396517753601074
epoch 62200  clean testing loss: 11.32760238647461
epoch 62300  training loss: 3.5773720741271973

 21%|████████████████▏                                                             | 62297/300000 [17:04<1:00:22, 65.62it/s]
epoch 62400  training loss: 3.533336639404297

 21%|████████████████▏                                                             | 62430/300000 [17:06<1:00:06, 65.88it/s]
epoch 62500  training loss: 3.471038818359375

 21%|████████████████▎                                                             | 62563/300000 [17:08<1:00:10, 65.77it/s]
epoch 62600  training loss: 3.2008602619171143
epoch 62600  clean testing loss: 11.198029518127441
epoch 62700  training loss: 3.4538795948028564

 21%|████████████████▎                                                             | 62696/300000 [17:10<1:00:08, 65.77it/s]
epoch 62800  training loss: 3.58974027633667

 21%|████████████████▎                                                             | 62822/300000 [17:12<1:00:33, 65.28it/s]
epoch 62900  training loss: 3.5352914333343506

 21%|████████████████▎                                                             | 62955/300000 [17:14<1:00:03, 65.79it/s]
epoch 63000  training loss: 3.644101142883301
epoch 63000  clean testing loss: 11.58761215209961
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop1 ...
epoch 63100  training loss: 3.53385329246521

 21%|████████████████▍                                                             | 63088/300000 [17:16<1:00:27, 65.30it/s]
epoch 63200  training loss: 3.5226426124572754

 21%|████████████████▊                                                               | 63221/300000 [17:18<59:45, 66.04it/s]
epoch 63300  training loss: 3.7570059299468994

 21%|████████████████▉                                                               | 63354/300000 [17:20<59:50, 65.90it/s]
epoch 63400  training loss: 3.795337200164795
epoch 63400  clean testing loss: 11.489809036254883
epoch 63500  training loss: 3.5802764892578125

 21%|████████████████▌                                                             | 63480/300000 [17:22<1:00:01, 65.68it/s]
epoch 63600  training loss: 3.545069932937622

 21%|████████████████▉                                                               | 63613/300000 [17:24<59:54, 65.76it/s]
epoch 63700  training loss: 3.556781530380249

 21%|████████████████▉                                                               | 63746/300000 [17:26<59:32, 66.13it/s]
epoch 63800  training loss: 3.5160627365112305

 21%|█████████████████                                                               | 63879/300000 [17:28<59:33, 66.07it/s]
epoch 63900  training loss: 3.684177875518799
epoch 63900  clean testing loss: 11.601502418518066
epoch 64000  training loss: 3.4407379627227783
epoch 64000  clean testing loss: 11.598953247070312

 21%|████████████████▋                                                             | 64012/300000 [17:30<1:01:04, 64.40it/s]
epoch 64100  training loss: 3.309112548828125

 21%|█████████████████                                                               | 64138/300000 [17:32<59:55, 65.60it/s]
epoch 64200  training loss: 3.518669605255127

 21%|█████████████████▏                                                              | 64271/300000 [17:34<59:33, 65.97it/s]
epoch 64300  training loss: 3.2299816608428955
epoch 64300  clean testing loss: 11.909096717834473
epoch 64400  training loss: 3.6300482749938965

 21%|█████████████████▏                                                              | 64404/300000 [17:36<59:41, 65.78it/s]
epoch 64500  training loss: 3.4453532695770264
epoch 64500  clean testing loss: 12.213138580322266
epoch 64600  training loss: 3.4353737831115723
epoch 64600  clean testing loss: 12.001741409301758
epoch 64700  training loss: 3.3630013465881348
epoch 64700  clean testing loss: 12.172379493713379
epoch 64800  training loss: 3.4921083450317383
epoch 64800  clean testing loss: 12.214969635009766
epoch 64900  training loss: 3.3312408924102783
epoch 64900  clean testing loss: 12.06702709197998
epoch 65000  training loss: 3.3767361640930176
epoch 65000  clean testing loss: 12.131634712219238
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop1 ...
epoch 65100  training loss: 3.4409117698669434

 22%|████████████████▉                                                             | 65105/300000 [17:49<1:01:58, 63.16it/s]
epoch 65200  training loss: 3.2657887935638428

 22%|█████████████████▍                                                              | 65238/300000 [17:51<59:22, 65.90it/s]
epoch 65300  training loss: 3.404348850250244

 22%|█████████████████▍                                                              | 65371/300000 [17:53<59:32, 65.67it/s]
epoch 65400  training loss: 3.4261350631713867
epoch 65400  clean testing loss: 12.089590072631836
epoch 65500  training loss: 3.512014865875244

 22%|█████████████████▍                                                              | 65497/300000 [17:54<59:05, 66.14it/s]
epoch 65600  training loss: 3.4246699810028076

 22%|█████████████████▌                                                              | 65630/300000 [17:57<59:37, 65.51it/s]
epoch 65700  training loss: 3.5016584396362305

 22%|█████████████████▌                                                              | 65763/300000 [17:59<59:22, 65.75it/s]
epoch 65800  training loss: 3.6143503189086914
epoch 65800  clean testing loss: 12.331765174865723
epoch 65900  training loss: 3.4073386192321777

 22%|█████████████████▏                                                            | 65896/300000 [18:01<1:00:21, 64.65it/s]
epoch 66000  training loss: 3.802957773208618
epoch 66000  clean testing loss: 12.164170265197754

 22%|█████████████████▌                                                              | 66022/300000 [18:03<59:43, 65.30it/s]
epoch 66100  training loss: 3.3887429237365723

 22%|█████████████████▋                                                              | 66155/300000 [18:05<59:41, 65.29it/s]
epoch 66200  training loss: 3.7122902870178223
epoch 66200  clean testing loss: 12.134587287902832
epoch 66300  training loss: 3.4652140140533447

 22%|█████████████████▋                                                              | 66288/300000 [18:07<58:49, 66.22it/s]
epoch 66400  training loss: 3.5399529933929443

 22%|█████████████████▋                                                              | 66421/300000 [18:09<58:50, 66.17it/s]
epoch 66500  training loss: 3.499380588531494

 22%|█████████████████▋                                                              | 66554/300000 [18:11<58:51, 66.10it/s]
epoch 66600  training loss: 3.5126590728759766

 22%|█████████████████▊                                                              | 66680/300000 [18:13<58:38, 66.31it/s]
epoch 66700  training loss: 3.5433456897735596
epoch 66700  clean testing loss: 12.32151985168457
epoch 66800  training loss: 3.735558032989502

 22%|█████████████████▊                                                              | 66813/300000 [18:15<59:45, 65.04it/s]
epoch 66900  training loss: 3.6727166175842285


 22%|█████████████████▍                                                            | 67118/300000 [18:29<1:04:33, 60.12it/s]
epoch 67000  training loss: 3.9418654441833496
epoch 67000  clean testing loss: 12.24522590637207
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop1 ...
epoch 67100  training loss: 3.3287041187286377

 22%|█████████████████▉                                                              | 67244/300000 [18:31<59:59, 64.66it/s]
epoch 67200  training loss: 3.4275622367858887

 22%|█████████████████▉                                                              | 67377/300000 [18:33<58:21, 66.43it/s]
epoch 67300  training loss: 3.4858956336975098

 23%|██████████████████                                                              | 67510/300000 [18:35<58:52, 65.81it/s]
epoch 67400  training loss: 3.5043692588806152
epoch 67400  clean testing loss: 11.762378692626953
epoch 67500  training loss: 3.658298969268799

 23%|██████████████████                                                              | 67839/300000 [18:40<58:26, 66.20it/s]
epoch 67600  training loss: 3.761042833328247
epoch 67600  clean testing loss: 11.702696800231934
epoch 67700  training loss: 3.772026538848877
epoch 67700  clean testing loss: 11.624574661254883
epoch 67800  training loss: 3.81626296043396

 23%|██████████████████▏                                                             | 67972/300000 [18:42<58:35, 66.00it/s]
epoch 67900  training loss: 3.6236977577209473

 23%|██████████████████▏                                                             | 68098/300000 [18:44<58:29, 66.08it/s]
epoch 68000  training loss: 3.7740354537963867
epoch 68000  clean testing loss: 11.558890342712402
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop1 ...
epoch 68100  training loss: 3.6276237964630127

 23%|██████████████████▏                                                             | 68231/300000 [18:46<59:42, 64.69it/s]
epoch 68200  training loss: 3.6934192180633545

 23%|██████████████████▏                                                             | 68364/300000 [18:48<58:34, 65.90it/s]
epoch 68300  training loss: 3.575138807296753
epoch 68300  clean testing loss: 11.349488258361816
epoch 68400  training loss: 3.3997368812561035

 23%|██████████████████▎                                                             | 68497/300000 [18:50<59:18, 65.06it/s]
epoch 68500  training loss: 3.669072151184082

 23%|██████████████████▎                                                             | 68623/300000 [18:52<59:38, 64.66it/s]
epoch 68600  training loss: 3.481295108795166

 23%|██████████████████▎                                                             | 68756/300000 [18:54<58:42, 65.65it/s]
epoch 68700  training loss: 3.725081205368042

 23%|██████████████████▎                                                             | 68889/300000 [18:56<58:46, 65.54it/s]
epoch 68800  training loss: 3.653195381164551
epoch 68800  clean testing loss: 11.205910682678223
epoch 68900  training loss: 3.713400363922119

 23%|██████████████████▍                                                             | 69022/300000 [18:58<58:36, 65.69it/s]
epoch 69000  training loss: 3.692944049835205
epoch 69000  clean testing loss: 11.272749900817871

 23%|██████████████████▍                                                             | 69155/300000 [19:00<59:01, 65.18it/s]
epoch 69100  training loss: 3.6399810314178467

 23%|██████████████████▍                                                             | 69281/300000 [19:02<59:56, 64.15it/s]
epoch 69200  training loss: 3.942699432373047
epoch 69200  clean testing loss: 11.183030128479004
epoch 69300  training loss: 3.9783129692077637

 23%|██████████████████▌                                                             | 69414/300000 [19:04<58:07, 66.12it/s]
epoch 69400  training loss: 4.186818599700928
epoch 69400  clean testing loss: 11.167716026306152
epoch 69500  training loss: 3.9199836254119873
epoch 69500  clean testing loss: 11.043682098388672
epoch 69600  training loss: 4.011831283569336
epoch 69600  clean testing loss: 11.09572982788086
epoch 69700  training loss: 4.124428749084473
epoch 69700  clean testing loss: 11.140656471252441
epoch 69800  training loss: 3.9740827083587646
epoch 69800  clean testing loss: 11.093291282653809
epoch 69900  training loss: 3.9161911010742188
epoch 69900  clean testing loss: 10.97840404510498
epoch 70000  training loss: 4.00759220123291
epoch 70000  clean testing loss: 10.841114044189453
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop1 ...
epoch 70100  training loss: 4.100747108459473

 23%|██████████████████▋                                                             | 70112/300000 [19:15<58:45, 65.21it/s]
epoch 70200  training loss: 4.161097049713135


 23%|██████████████████▊                                                             | 70476/300000 [19:21<58:04, 65.86it/s]
epoch 70300  training loss: 4.32753324508667
epoch 70300  clean testing loss: 10.805542945861816
epoch 70400  training loss: 4.517698287963867

 24%|██████████████████▊                                                             | 70609/300000 [19:23<57:43, 66.23it/s]
epoch 70500  training loss: 4.497931480407715
epoch 70500  clean testing loss: 11.035797119140625
epoch 70600  training loss: 4.304800033569336

 24%|██████████████████▊                                                             | 70742/300000 [19:25<57:41, 66.22it/s]
epoch 70700  training loss: 4.372023105621338

 24%|██████████████████▉                                                             | 70868/300000 [19:27<59:36, 64.06it/s]
epoch 70800  training loss: 4.270110130310059

 24%|██████████████████▉                                                             | 70995/300000 [19:29<56:14, 67.87it/s]
epoch 70900  training loss: 4.438492774963379
epoch 70900  clean testing loss: 11.040857315063477
epoch 71000  training loss: 4.428745269775391
epoch 71000  clean testing loss: 10.927873611450195


 24%|██████████████████▉                                                             | 71198/300000 [19:41<59:08, 64.49it/s]
epoch 71100  training loss: 4.551120281219482

 24%|███████████████████                                                             | 71331/300000 [19:43<58:16, 65.40it/s]
epoch 71200  training loss: 4.403900146484375
epoch 71200  clean testing loss: 11.024637222290039
epoch 71300  training loss: 4.433096885681152

 24%|███████████████████                                                             | 71464/300000 [19:45<58:29, 65.12it/s]
epoch 71400  training loss: 4.2693257331848145
epoch 71400  clean testing loss: 11.181119918823242
epoch 71500  training loss: 4.392175197601318


 24%|███████████████████▏                                                            | 71723/300000 [19:49<58:59, 64.50it/s]
epoch 71600  training loss: 4.378663063049316
epoch 71600  clean testing loss: 11.264643669128418
epoch 71700  training loss: 4.171084403991699

 24%|███████████████████▏                                                            | 71856/300000 [19:51<58:05, 65.45it/s]
epoch 71800  training loss: 4.223977565765381

 24%|███████████████████▏                                                            | 71982/300000 [19:53<58:10, 65.33it/s]
epoch 71900  training loss: 4.183070659637451
epoch 71900  clean testing loss: 11.088578224182129
epoch 72000  training loss: 4.084219455718994
epoch 72000  clean testing loss: 10.852219581604004


 24%|███████████████████▏                                                            | 72122/300000 [19:57<59:07, 64.23it/s]
epoch 72100  training loss: 4.157493591308594

 24%|███████████████████▎                                                            | 72248/300000 [19:59<57:38, 65.86it/s]
epoch 72200  training loss: 4.239465713500977

 24%|███████████████████▎                                                            | 72381/300000 [20:01<57:19, 66.17it/s]
epoch 72300  training loss: 4.366180896759033
epoch 72300  clean testing loss: 10.81844425201416
epoch 72400  training loss: 4.042667388916016
epoch 72400  clean testing loss: 10.917890548706055
epoch 72500  training loss: 3.814241886138916
epoch 72500  clean testing loss: 11.04226016998291
epoch 72600  training loss: 4.133296966552734

 24%|███████████████████▎                                                            | 72612/300000 [20:04<58:21, 64.95it/s]
epoch 72700  training loss: 3.945809841156006

 24%|███████████████████▍                                                            | 72745/300000 [20:06<57:23, 66.00it/s]
epoch 72800  training loss: 3.881714105606079

 24%|███████████████████▍                                                            | 72878/300000 [20:09<57:55, 65.34it/s]
epoch 72900  training loss: 3.7405991554260254
epoch 72900  clean testing loss: 11.086403846740723
epoch 73000  training loss: 3.7836828231811523
epoch 73000  clean testing loss: 10.932409286499023

 24%|███████████████████▍                                                            | 73004/300000 [20:10<58:38, 64.52it/s]
epoch 73100  training loss: 4.195220470428467

 24%|███████████████████▌                                                            | 73137/300000 [20:12<57:51, 65.34it/s]
epoch 73200  training loss: 3.7970235347747803

 24%|███████████████████▌                                                            | 73270/300000 [20:14<58:18, 64.80it/s]
epoch 73300  training loss: 3.9646449089050293

 24%|███████████████████▌                                                            | 73403/300000 [20:17<57:37, 65.54it/s]
epoch 73400  training loss: 3.773934841156006
epoch 73400  clean testing loss: 11.04117202758789
epoch 73500  training loss: 3.9220526218414307

 25%|███████████████████▌                                                            | 73536/300000 [20:19<57:04, 66.13it/s]
epoch 73600  training loss: 3.8762643337249756

 25%|███████████████████▋                                                            | 73669/300000 [20:21<57:57, 65.09it/s]
epoch 73700  training loss: 4.116943359375

 25%|███████████████████▋                                                            | 73795/300000 [20:22<57:39, 65.39it/s]
epoch 73800  training loss: 3.7921979427337646
epoch 73800  clean testing loss: 10.991721153259277
epoch 73900  training loss: 3.893444538116455

 25%|███████████████████▋                                                            | 73928/300000 [20:25<57:12, 65.86it/s]
epoch 74000  training loss: 4.020630359649658
epoch 74000  clean testing loss: 11.194894790649414


 25%|███████████████████▎                                                          | 74071/300000 [20:37<2:01:14, 31.06it/s]
epoch 74100  training loss: 3.9409914016723633

 25%|███████████████████▊                                                            | 74204/300000 [20:39<57:16, 65.71it/s]
epoch 74200  training loss: 3.9084420204162598
epoch 74200  clean testing loss: 11.235213279724121
epoch 74300  training loss: 3.7604150772094727

 25%|███████████████████▊                                                            | 74330/300000 [20:41<57:06, 65.85it/s]
epoch 74400  training loss: 3.8432741165161133

 25%|███████████████████▊                                                            | 74435/300000 [20:42<57:27, 65.43it/s]
epoch 74500  training loss: 3.886744260787964

 25%|███████████████████▉                                                            | 74561/300000 [20:44<57:00, 65.91it/s]
epoch 74600  training loss: 3.89723801612854
epoch 74600  clean testing loss: 11.12870979309082
epoch 74700  training loss: 3.764228582382202

 25%|███████████████████▉                                                            | 74694/300000 [20:46<56:40, 66.26it/s]
epoch 74800  training loss: 3.7777764797210693

 25%|███████████████████▉                                                            | 74827/300000 [20:48<57:33, 65.21it/s]
epoch 74900  training loss: 3.7433295249938965
epoch 74900  clean testing loss: 11.229493141174316
epoch 75000  training loss: 3.6905505657196045
epoch 75000  clean testing loss: 11.23414134979248
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop1 ...
epoch 75100  training loss: 3.7619388103485107

 25%|████████████████████                                                            | 75128/300000 [20:53<56:42, 66.08it/s]
epoch 75200  training loss: 3.776041030883789


 25%|████████████████████                                                            | 75394/300000 [20:57<56:38, 66.08it/s]
epoch 75300  training loss: 3.6603336334228516
epoch 75300  clean testing loss: 11.081032752990723
epoch 75400  training loss: 3.6855292320251465
epoch 75400  clean testing loss: 11.188841819763184
epoch 75500  training loss: 3.7166061401367188


 25%|████████████████████▏                                                           | 75653/300000 [21:01<56:25, 66.26it/s]
epoch 75600  training loss: 3.656506299972534

 25%|████████████████████▏                                                           | 75786/300000 [21:03<56:26, 66.21it/s]
epoch 75700  training loss: 3.6294894218444824

 25%|████████████████████▏                                                           | 75919/300000 [21:05<56:38, 65.93it/s]
epoch 75800  training loss: 3.4881246089935303
epoch 75800  clean testing loss: 11.33216667175293
epoch 75900  training loss: 3.67396879196167

 25%|████████████████████▎                                                           | 76052/300000 [21:07<57:33, 64.86it/s]
epoch 76000  training loss: 3.527477741241455
epoch 76000  clean testing loss: 11.223710060119629
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop1 ...
epoch 76100  training loss: 3.594116687774658

 25%|████████████████████▎                                                           | 76185/300000 [21:09<56:29, 66.03it/s]
epoch 76200  training loss: 3.4526724815368652
epoch 76200  clean testing loss: 11.279325485229492
epoch 76300  training loss: 3.6898834705352783

 25%|████████████████████▎                                                           | 76283/300000 [21:11<56:41, 65.77it/s]
epoch 76400  training loss: 3.5429556369781494


 26%|████████████████████▍                                                           | 76542/300000 [21:15<56:04, 66.42it/s]
epoch 76500  training loss: 3.680809259414673

 26%|████████████████████▌                                                           | 77120/300000 [21:25<57:21, 64.76it/s]
epoch 76600  training loss: 3.8902390003204346
epoch 76600  clean testing loss: 11.255425453186035
epoch 76700  training loss: 3.895780563354492
epoch 76700  clean testing loss: 11.249417304992676
epoch 76800  training loss: 3.6561696529388428
epoch 76800  clean testing loss: 11.33090877532959
epoch 76900  training loss: 3.607412815093994
epoch 76900  clean testing loss: 11.293622016906738
epoch 77000  training loss: 3.7489678859710693
epoch 77000  clean testing loss: 11.19002628326416
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop1 ...
epoch 77100  training loss: 3.6394312381744385
epoch 77100  clean testing loss: 11.252463340759277
epoch 77200  training loss: 3.4981234073638916

 26%|████████████████████▌                                                           | 77253/300000 [21:27<56:35, 65.59it/s]
epoch 77300  training loss: 3.8338937759399414

 26%|████████████████████▋                                                           | 77386/300000 [21:29<56:12, 66.01it/s]
epoch 77400  training loss: 3.569340467453003
epoch 77400  clean testing loss: 11.189923286437988
epoch 77500  training loss: 3.604254961013794

 26%|████████████████████▋                                                           | 77519/300000 [21:31<56:43, 65.36it/s]
epoch 77600  training loss: 3.6054608821868896

 26%|████████████████████▋                                                           | 77645/300000 [21:33<56:59, 65.02it/s]
epoch 77700  training loss: 3.6073110103607178

 26%|████████████████████▋                                                           | 77778/300000 [21:35<56:26, 65.61it/s]
epoch 77800  training loss: 3.481558084487915
epoch 77800  clean testing loss: 11.275760650634766
epoch 77900  training loss: 3.55120849609375
epoch 77900  clean testing loss: 11.403910636901855
epoch 78000  training loss: 3.5182278156280518
epoch 78000  clean testing loss: 11.341748237609863
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop1 ...
epoch 78100  training loss: 3.4801392555236816

 26%|████████████████████▊                                                           | 78121/300000 [21:42<56:59, 64.88it/s]
epoch 78200  training loss: 3.4787745475769043

 26%|████████████████████▊                                                           | 78247/300000 [21:44<55:57, 66.05it/s]
epoch 78300  training loss: 3.6155683994293213

 26%|████████████████████▉                                                           | 78380/300000 [21:46<55:53, 66.09it/s]
epoch 78400  training loss: 3.6134138107299805

 26%|████████████████████▉                                                           | 78513/300000 [21:48<56:24, 65.45it/s]
epoch 78500  training loss: 3.570016622543335
epoch 78500  clean testing loss: 11.589495658874512
epoch 78600  training loss: 3.3826725482940674

 26%|████████████████████▉                                                           | 78646/300000 [21:50<55:50, 66.08it/s]
epoch 78700  training loss: 3.5159947872161865

 26%|█████████████████████                                                           | 78779/300000 [21:52<56:19, 65.46it/s]
epoch 78800  training loss: 3.4608259201049805

 26%|█████████████████████                                                           | 78912/300000 [21:54<56:06, 65.67it/s]
epoch 78900  training loss: 3.6325719356536865
epoch 78900  clean testing loss: 11.553449630737305
epoch 79000  training loss: 3.6315975189208984
epoch 79000  clean testing loss: 11.523690223693848

 26%|█████████████████████                                                           | 79038/300000 [21:56<55:52, 65.91it/s]
epoch 79100  training loss: 3.6468658447265625

 26%|█████████████████████                                                           | 79171/300000 [21:58<55:58, 65.75it/s]
epoch 79200  training loss: 3.837886095046997


 26%|█████████████████████▏                                                          | 79437/300000 [22:02<56:18, 65.29it/s]
epoch 79300  training loss: 3.5438549518585205
epoch 79300  clean testing loss: 11.54808235168457
epoch 79400  training loss: 3.5576539039611816

 27%|█████████████████████▎                                                          | 79962/300000 [22:10<55:31, 66.05it/s]
epoch 79500  training loss: 3.8482630252838135
epoch 79500  clean testing loss: 11.537842750549316
epoch 79600  training loss: 3.717480421066284
epoch 79600  clean testing loss: 11.48434829711914
epoch 79700  training loss: 3.333444118499756
epoch 79700  clean testing loss: 11.45665454864502
epoch 79800  training loss: 3.586390733718872
epoch 79800  clean testing loss: 11.508645057678223
epoch 79900  training loss: 3.941176176071167

 27%|█████████████████████▎                                                          | 80095/300000 [22:12<55:58, 65.47it/s]
epoch 80000  training loss: 3.659905433654785
epoch 80000  clean testing loss: 11.616305351257324

 27%|█████████████████████▍                                                          | 80221/300000 [22:14<55:31, 65.97it/s]
epoch 80100  training loss: 3.510380506515503
epoch 80100  clean testing loss: 11.523612976074219
epoch 80200  training loss: 3.388624668121338

 27%|█████████████████████▍                                                          | 80354/300000 [22:16<55:20, 66.15it/s]
epoch 80300  training loss: 3.657740831375122

 27%|█████████████████████▍                                                          | 80452/300000 [22:18<55:31, 65.90it/s]
epoch 80400  training loss: 3.6160032749176025

 27%|█████████████████████▋                                                          | 81108/300000 [22:30<57:28, 63.48it/s]
epoch 80500  training loss: 3.4212090969085693
epoch 80500  clean testing loss: 11.076586723327637
epoch 80600  training loss: 3.317190647125244
epoch 80600  clean testing loss: 11.011239051818848
epoch 80700  training loss: 3.6083340644836426
epoch 80700  clean testing loss: 11.049049377441406
epoch 80800  training loss: 3.472503185272217
epoch 80800  clean testing loss: 11.108556747436523
epoch 80900  training loss: 3.3721625804901123
epoch 80900  clean testing loss: 11.011645317077637
epoch 81000  training loss: 3.4673731327056885
epoch 81000  clean testing loss: 11.089326858520508
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop1 ...
epoch 81100  training loss: 3.571713447570801

 27%|█████████████████████▋                                                          | 81241/300000 [22:33<55:22, 65.83it/s]
epoch 81200  training loss: 3.359171152114868
epoch 81200  clean testing loss: 10.964401245117188
epoch 81300  training loss: 3.4870309829711914
epoch 81300  clean testing loss: 11.016465187072754
epoch 81400  training loss: 3.206016778945923

 27%|█████████████████████▋                                                          | 81437/300000 [22:35<55:40, 65.44it/s]
epoch 81500  training loss: 3.149320125579834


 27%|█████████████████████▊                                                          | 81696/300000 [22:39<55:19, 65.76it/s]
epoch 81600  training loss: 3.1662535667419434
epoch 81600  clean testing loss: 11.116496086120605
epoch 81700  training loss: 3.383359670639038

 27%|█████████████████████▊                                                          | 81829/300000 [22:41<55:23, 65.65it/s]
epoch 81800  training loss: 3.4064016342163086

 27%|█████████████████████▊                                                          | 81962/300000 [22:44<54:37, 66.53it/s]
epoch 81900  training loss: 3.585578203201294

 27%|█████████████████████▉                                                          | 82095/300000 [22:46<55:24, 65.55it/s]
epoch 82000  training loss: 3.4284605979919434
epoch 82000  clean testing loss: 11.164600372314453
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop1 ...
epoch 82100  training loss: 3.6305530071258545

 27%|█████████████████████▉                                                          | 82228/300000 [22:48<55:30, 65.39it/s]
epoch 82200  training loss: 3.551262617111206

 28%|██████████████████████                                                          | 82788/300000 [22:56<55:09, 65.63it/s]
epoch 82300  training loss: 3.513988494873047
epoch 82300  clean testing loss: 11.128395080566406
epoch 82400  training loss: 3.649010181427002
epoch 82400  clean testing loss: 11.072181701660156
epoch 82500  training loss: 3.496690034866333
epoch 82500  clean testing loss: 11.140262603759766
epoch 82600  training loss: 3.3855080604553223
epoch 82600  clean testing loss: 11.178910255432129
epoch 82700  training loss: 3.3576409816741943
epoch 82700  clean testing loss: 11.342607498168945
epoch 82800  training loss: 3.716505527496338
epoch 82800  clean testing loss: 11.194876670837402
epoch 82900  training loss: 3.7319648265838623

 28%|██████████████████████                                                          | 82914/300000 [22:58<55:34, 65.09it/s]
epoch 83000  training loss: 3.575377941131592
epoch 83000  clean testing loss: 11.238778114318848


 28%|██████████████████████▏                                                         | 83180/300000 [23:02<55:23, 65.23it/s]
epoch 83100  training loss: 3.7491724491119385
epoch 83100  clean testing loss: 11.271554946899414
epoch 83200  training loss: 3.5171356201171875
epoch 83200  clean testing loss: 11.252601623535156
epoch 83300  training loss: 3.6920197010040283

 28%|██████████████████████▏                                                         | 83306/300000 [23:04<55:02, 65.62it/s]
epoch 83400  training loss: 3.478508234024048

 28%|██████████████████████▎                                                         | 83439/300000 [23:06<55:33, 64.97it/s]
epoch 83500  training loss: 3.5637528896331787

 28%|██████████████████████▎                                                         | 83572/300000 [23:08<54:43, 65.92it/s]
epoch 83600  training loss: 3.556893825531006
epoch 83600  clean testing loss: 11.184355735778809
epoch 83700  training loss: 3.4438693523406982

 28%|██████████████████████▎                                                         | 83705/300000 [23:10<54:25, 66.23it/s]
epoch 83800  training loss: 3.3728127479553223

 28%|██████████████████████▎                                                         | 83838/300000 [23:12<54:35, 66.00it/s]
epoch 83900  training loss: 3.471928358078003

 28%|██████████████████████▍                                                         | 83971/300000 [23:14<55:26, 64.95it/s]
epoch 84000  training loss: 3.3498165607452393
epoch 84000  clean testing loss: 10.97052001953125
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop1 ...
epoch 84100  training loss: 3.563260555267334

 28%|██████████████████████▍                                                         | 84097/300000 [23:16<56:03, 64.19it/s]
epoch 84200  training loss: 3.293261766433716

 28%|██████████████████████▍                                                         | 84230/300000 [23:18<53:40, 66.99it/s]
epoch 84300  training loss: 3.5195746421813965

 28%|██████████████████████▍                                                         | 84363/300000 [23:20<56:20, 63.80it/s]
epoch 84400  training loss: 3.4707672595977783

 28%|██████████████████████▌                                                         | 84496/300000 [23:22<54:36, 65.78it/s]
epoch 84500  training loss: 3.3510313034057617
epoch 84500  clean testing loss: 10.97838020324707
epoch 84600  training loss: 3.252436399459839

 28%|██████████████████████▌                                                         | 84629/300000 [23:24<54:07, 66.33it/s]
epoch 84700  training loss: 3.3888165950775146
epoch 84700  clean testing loss: 10.97640323638916
epoch 84800  training loss: 3.0448169708251953

 28%|██████████████████████▌                                                         | 84825/300000 [23:27<55:28, 64.65it/s]
epoch 84900  training loss: 3.1696383953094482


 28%|██████████████████████▋                                                         | 85084/300000 [23:31<54:13, 66.06it/s]
epoch 85000  training loss: 3.3817811012268066
epoch 85000  clean testing loss: 10.888565063476562

 28%|██████████████████████▋                                                         | 85217/300000 [23:33<54:28, 65.70it/s]
epoch 85100  training loss: 3.3037099838256836
epoch 85100  clean testing loss: 10.91636848449707
epoch 85200  training loss: 3.1066601276397705

 28%|██████████████████████▊                                                         | 85350/300000 [23:35<54:09, 66.05it/s]
epoch 85300  training loss: 3.226160764694214

 29%|██████████████████████▉                                                         | 86008/300000 [23:45<54:31, 65.42it/s]
epoch 85400  training loss: 3.3636510372161865
epoch 85400  clean testing loss: 11.091377258300781
epoch 85500  training loss: 3.194734811782837
epoch 85500  clean testing loss: 11.010416030883789
epoch 85600  training loss: 3.1811113357543945
epoch 85600  clean testing loss: 10.975689888000488
epoch 85700  training loss: 3.2060656547546387
epoch 85700  clean testing loss: 11.002979278564453
epoch 85800  training loss: 3.297527551651001
epoch 85800  clean testing loss: 10.96464729309082
epoch 85900  training loss: 3.1550934314727783
epoch 85900  clean testing loss: 10.90956974029541
epoch 86000  training loss: 3.2270586490631104
epoch 86000  clean testing loss: 10.94682502746582
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop1 ...
epoch 86100  training loss: 3.2073447704315186

 29%|██████████████████████▉                                                         | 86134/300000 [23:47<54:22, 65.55it/s]
epoch 86200  training loss: 3.480821371078491

 29%|███████████████████████                                                         | 86267/300000 [23:49<54:50, 64.95it/s]
epoch 86300  training loss: 3.30452561378479

 29%|███████████████████████                                                         | 86400/300000 [23:51<53:29, 66.56it/s]
epoch 86400  training loss: 3.4022796154022217
epoch 86400  clean testing loss: 10.909113883972168
epoch 86500  training loss: 3.447922468185425

 29%|███████████████████████                                                         | 86533/300000 [23:53<54:23, 65.41it/s]
epoch 86600  training loss: 3.108584403991699

 29%|███████████████████████                                                         | 86666/300000 [23:55<54:13, 65.56it/s]
epoch 86700  training loss: 3.1795313358306885

 29%|███████████████████████▏                                                        | 86792/300000 [23:57<54:03, 65.74it/s]
epoch 86800  training loss: 3.2738630771636963
epoch 86800  clean testing loss: 10.932236671447754
epoch 86900  training loss: 3.136655330657959

 29%|███████████████████████▏                                                        | 86925/300000 [23:59<54:52, 64.71it/s]
epoch 87000  training loss: 3.262946844100952
epoch 87000  clean testing loss: 10.83553409576416

 29%|███████████████████████▏                                                        | 86997/300000 [24:00<45:01, 78.83it/s]
epoch 87100  training loss: 2.9771902561187744

 29%|███████████████████████▏                                                        | 87103/300000 [24:04<56:30, 62.79it/s]
epoch 87200  training loss: 3.336301803588867

 29%|███████████████████████▎                                                        | 87236/300000 [24:06<53:48, 65.90it/s]
epoch 87300  training loss: 3.3706538677215576

 29%|███████████████████████▎                                                        | 87369/300000 [24:08<53:54, 65.75it/s]
epoch 87400  training loss: 3.2293496131896973

 29%|███████████████████████▎                                                        | 87502/300000 [24:10<53:40, 65.98it/s]
epoch 87500  training loss: 3.152165651321411
epoch 87500  clean testing loss: 10.837923049926758
epoch 87600  training loss: 3.1832332611083984

 29%|███████████████████████▍                                                        | 87831/300000 [24:15<54:26, 64.96it/s]
epoch 87700  training loss: 3.088163137435913
epoch 87700  clean testing loss: 10.846083641052246
epoch 87800  training loss: 3.196519374847412

 29%|███████████████████████▍                                                        | 87964/300000 [24:17<53:30, 66.04it/s]
epoch 87900  training loss: 3.1438350677490234

 29%|██████████████████████▉                                                       | 88035/300000 [24:19<1:24:15, 41.93it/s]
epoch 88000  training loss: 3.2690961360931396
epoch 88000  clean testing loss: 10.786422729492188
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop1 ...
epoch 88100  training loss: 3.1718990802764893

 29%|███████████████████████▌                                                        | 88168/300000 [24:21<53:35, 65.88it/s]
epoch 88200  training loss: 3.2906675338745117

 29%|███████████████████████▌                                                        | 88301/300000 [24:23<53:22, 66.11it/s]
epoch 88300  training loss: 3.345658779144287
epoch 88300  clean testing loss: 10.854111671447754
epoch 88400  training loss: 3.2307286262512207


 30%|███████████████████████▌                                                        | 88560/300000 [24:27<53:58, 65.29it/s]
epoch 88500  training loss: 3.1143712997436523

 30%|███████████████████████▋                                                        | 88693/300000 [24:29<53:23, 65.96it/s]
epoch 88600  training loss: 3.2922732830047607

 30%|███████████████████████▊                                                        | 89100/300000 [24:36<54:11, 64.86it/s]
epoch 88700  training loss: 3.3057515621185303
epoch 88700  clean testing loss: 10.960659980773926
epoch 88800  training loss: 3.3983216285705566
epoch 88800  clean testing loss: 10.950296401977539
epoch 88900  training loss: 3.1123600006103516
epoch 88900  clean testing loss: 10.944605827331543
epoch 89000  training loss: 3.1320388317108154
epoch 89000  clean testing loss: 10.97120189666748
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop1 ...
epoch 89100  training loss: 3.2900006771087646
epoch 89100  clean testing loss: 11.057514190673828
epoch 89200  training loss: 3.0928966999053955

 30%|███████████████████████▊                                                        | 89233/300000 [24:38<53:27, 65.71it/s]
epoch 89300  training loss: 3.406527519226074


 30%|███████████████████████▊                                                        | 89499/300000 [24:42<53:50, 65.17it/s]
epoch 89400  training loss: 3.3416919708251953
epoch 89400  clean testing loss: 11.098109245300293
epoch 89500  training loss: 3.3680498600006104
epoch 89500  clean testing loss: 11.08022403717041
epoch 89600  training loss: 3.2151143550872803
epoch 89600  clean testing loss: 11.064981460571289
epoch 89700  training loss: 3.382659912109375
epoch 89700  clean testing loss: 11.128426551818848
epoch 89800  training loss: 3.192673444747925
epoch 89800  clean testing loss: 11.08255672454834
epoch 89900  training loss: 3.1781320571899414
epoch 89900  clean testing loss: 11.174649238586426
epoch 90000  training loss: 3.212280035018921
epoch 90000  clean testing loss: 11.102062225341797

 30%|████████████████████████                                                        | 90059/300000 [24:50<53:30, 65.40it/s]
epoch 90100  training loss: 3.361823797225952

 30%|████████████████████████                                                        | 90185/300000 [24:52<53:11, 65.74it/s]
epoch 90200  training loss: 3.3833250999450684
epoch 90200  clean testing loss: 10.947240829467773
epoch 90300  training loss: 3.124051809310913

 30%|████████████████████████                                                        | 90318/300000 [24:54<53:41, 65.08it/s]
epoch 90400  training loss: 3.214108943939209

 30%|████████████████████████                                                        | 90451/300000 [24:56<53:18, 65.52it/s]
epoch 90500  training loss: 3.203277826309204

 30%|████████████████████████▏                                                       | 90584/300000 [24:58<53:35, 65.13it/s]
epoch 90600  training loss: 3.243863344192505
epoch 90600  clean testing loss: 11.176013946533203
epoch 90700  training loss: 3.233884334564209


 30%|████████████████████████▏                                                       | 90913/300000 [25:04<53:06, 65.61it/s]
epoch 90800  training loss: 3.259993314743042
epoch 90800  clean testing loss: 11.271355628967285
epoch 90900  training loss: 3.4136176109313965
epoch 90900  clean testing loss: 11.251236915588379
epoch 91000  training loss: 3.428459644317627
epoch 91000  clean testing loss: 11.045527458190918


 30%|████████████████████████▎                                                       | 91172/300000 [25:07<52:42, 66.04it/s]
epoch 91100  training loss: 3.280837059020996

 30%|████████████████████████▎                                                       | 91305/300000 [25:09<52:18, 66.50it/s]
epoch 91200  training loss: 3.1480157375335693

 30%|████████████████████████▍                                                       | 91438/300000 [25:12<52:53, 65.72it/s]
epoch 91300  training loss: 3.304698944091797
epoch 91300  clean testing loss: 11.064385414123535
epoch 91400  training loss: 3.26316237449646

 31%|████████████████████████▍                                                       | 91571/300000 [25:14<53:04, 65.45it/s]
epoch 91500  training loss: 3.3056695461273193

 31%|████████████████████████▍                                                       | 91704/300000 [25:16<52:41, 65.89it/s]
epoch 91600  training loss: 3.203552484512329

 31%|████████████████████████▍                                                       | 91837/300000 [25:18<52:38, 65.91it/s]
epoch 91700  training loss: 3.127561092376709
epoch 91700  clean testing loss: 10.879444122314453
epoch 91800  training loss: 3.2425403594970703

 31%|████████████████████████▌                                                       | 91963/300000 [25:20<52:27, 66.10it/s]
epoch 91900  training loss: 3.383741617202759

 31%|███████████████████████▉                                                      | 92011/300000 [25:22<2:54:56, 19.82it/s]
epoch 92000  training loss: 3.1904006004333496
epoch 92000  clean testing loss: 10.950246810913086

 31%|████████████████████████▌                                                       | 92144/300000 [25:24<52:09, 66.43it/s]
epoch 92100  training loss: 3.3296730518341064

 31%|████████████████████████▌                                                       | 92277/300000 [25:26<51:57, 66.63it/s]
epoch 92200  training loss: 3.1761832237243652

 31%|████████████████████████▋                                                       | 92410/300000 [25:28<52:21, 66.08it/s]
epoch 92300  training loss: 3.1936986446380615

 31%|████████████████████████▋                                                       | 92543/300000 [25:30<52:49, 65.46it/s]
epoch 92400  training loss: 3.1627132892608643
epoch 92400  clean testing loss: 10.925137519836426
epoch 92500  training loss: 3.2631959915161133

 31%|████████████████████████▋                                                       | 92676/300000 [25:32<53:01, 65.16it/s]
epoch 92600  training loss: 3.363311767578125

 31%|████████████████████████▋                                                       | 92802/300000 [25:34<52:24, 65.89it/s]
epoch 92700  training loss: 3.16367244720459

 31%|████████████████████████▊                                                       | 92935/300000 [25:36<52:12, 66.10it/s]
epoch 92800  training loss: 3.4520630836486816
epoch 92800  clean testing loss: 10.809730529785156
epoch 92900  training loss: 3.3703434467315674

 31%|████████████████████████▊                                                       | 92998/300000 [25:37<51:55, 66.45it/s]
epoch 93000  training loss: 3.1962363719940186
epoch 93000  clean testing loss: 10.842886924743652

 31%|████████████████████████▊                                                       | 93117/300000 [25:40<53:25, 64.55it/s]
epoch 93100  training loss: 3.254077434539795

 31%|████████████████████████▊                                                       | 93250/300000 [25:42<52:34, 65.54it/s]
epoch 93200  training loss: 3.2484993934631348

 31%|████████████████████████▉                                                       | 93383/300000 [25:44<52:41, 65.35it/s]
epoch 93300  training loss: 3.454829216003418
epoch 93300  clean testing loss: 10.776571273803711
epoch 93400  training loss: 3.2779600620269775
epoch 93400  clean testing loss: 10.6541748046875
epoch 93500  training loss: 3.3190550804138184
epoch 93500  clean testing loss: 10.718478202819824
epoch 93600  training loss: 3.340336322784424

 31%|████████████████████████▉                                                       | 93649/300000 [25:48<52:09, 65.94it/s]
epoch 93700  training loss: 3.361360549926758


 31%|█████████████████████████                                                       | 93908/300000 [25:52<52:53, 64.94it/s]
epoch 93800  training loss: 3.1463778018951416
epoch 93800  clean testing loss: 10.757804870605469
epoch 93900  training loss: 3.2835733890533447

 31%|█████████████████████████                                                       | 94000/300000 [25:53<49:47, 68.96it/s]
epoch 94000  training loss: 3.2089784145355225
epoch 94000  clean testing loss: 10.751134872436523
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop1 ...
epoch 94100  training loss: 3.485050916671753

 31%|█████████████████████████                                                       | 94105/300000 [25:58<55:59, 61.29it/s]
epoch 94200  training loss: 3.0590333938598633

 31%|█████████████████████████▏                                                      | 94238/300000 [26:00<52:11, 65.71it/s]
epoch 94300  training loss: 3.411677598953247


 32%|█████████████████████████▎                                                      | 94931/300000 [26:11<52:30, 65.08it/s]
epoch 94400  training loss: 3.2742903232574463
epoch 94400  clean testing loss: 10.763392448425293
epoch 94500  training loss: 3.2644460201263428
epoch 94500  clean testing loss: 10.77054500579834
epoch 94600  training loss: 3.2787325382232666
epoch 94600  clean testing loss: 10.760573387145996
epoch 94700  training loss: 3.301873207092285
epoch 94700  clean testing loss: 10.715995788574219
epoch 94800  training loss: 3.3814175128936768
epoch 94800  clean testing loss: 10.76273250579834
epoch 94900  training loss: 3.5474660396575928

 32%|█████████████████████████▎                                                      | 95057/300000 [26:13<52:19, 65.27it/s]
epoch 95000  training loss: 3.291727304458618
epoch 95000  clean testing loss: 10.727871894836426

 32%|█████████████████████████▍                                                      | 95190/300000 [26:15<52:46, 64.69it/s]
epoch 95100  training loss: 3.2625551223754883
epoch 95100  clean testing loss: 10.76153564453125
epoch 95200  training loss: 3.2464711666107178

 32%|█████████████████████████▍                                                      | 95323/300000 [26:17<52:00, 65.60it/s]
epoch 95300  training loss: 3.4005322456359863

 32%|█████████████████████████▍                                                      | 95456/300000 [26:19<51:24, 66.31it/s]
epoch 95400  training loss: 3.4624507427215576

 32%|█████████████████████████▍                                                      | 95582/300000 [26:21<52:05, 65.41it/s]
epoch 95500  training loss: 3.556607246398926

 32%|█████████████████████████▌                                                      | 95715/300000 [26:23<51:25, 66.20it/s]
epoch 95600  training loss: 3.3473784923553467
epoch 95600  clean testing loss: 10.858145713806152
epoch 95700  training loss: 3.3557281494140625

 32%|█████████████████████████▌                                                      | 95848/300000 [26:25<51:55, 65.53it/s]
epoch 95800  training loss: 3.426266670227051

 32%|█████████████████████████▌                                                      | 95981/300000 [26:27<52:07, 65.24it/s]
epoch 95900  training loss: 3.460334062576294

 32%|█████████████████████████▋                                                      | 96114/300000 [26:29<51:28, 66.02it/s]
epoch 96000  training loss: 3.5238213539123535
epoch 96000  clean testing loss: 11.126469612121582
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop1 ...
epoch 96100  training loss: 3.599114418029785

 32%|█████████████████████████▋                                                      | 96240/300000 [26:31<51:53, 65.45it/s]
epoch 96200  training loss: 3.629267692565918

 32%|█████████████████████████▋                                                      | 96373/300000 [26:33<51:20, 66.10it/s]
epoch 96300  training loss: 3.465130567550659

 32%|█████████████████████████▋                                                      | 96506/300000 [26:35<51:29, 65.86it/s]
epoch 96400  training loss: 3.589266538619995
epoch 96400  clean testing loss: 11.029622077941895
epoch 96500  training loss: 3.6928327083587646

 32%|█████████████████████████▊                                                      | 96639/300000 [26:37<52:10, 64.96it/s]
epoch 96600  training loss: 3.409268856048584

 32%|█████████████████████████▊                                                      | 96765/300000 [26:39<51:23, 65.91it/s]
epoch 96700  training loss: 3.459059715270996

 32%|█████████████████████████▊                                                      | 96898/300000 [26:41<51:30, 65.72it/s]
epoch 96800  training loss: 3.603623628616333
epoch 96800  clean testing loss: 11.122438430786133
epoch 96900  training loss: 3.5586330890655518

 32%|█████████████████████████▊                                                      | 97031/300000 [26:43<51:19, 65.92it/s]
epoch 97000  training loss: 3.5424153804779053
epoch 97000  clean testing loss: 11.134346008300781

 32%|█████████████████████████▉                                                      | 97164/300000 [26:45<50:59, 66.30it/s]
epoch 97100  training loss: 3.4068117141723633

 32%|█████████████████████████▉                                                      | 97297/300000 [26:47<52:00, 64.96it/s]
epoch 97200  training loss: 3.4965624809265137

 32%|█████████████████████████▉                                                      | 97423/300000 [26:49<51:40, 65.33it/s]
epoch 97300  training loss: 3.5135836601257324
epoch 97300  clean testing loss: 11.306388854980469
epoch 97400  training loss: 3.620358467102051

 33%|██████████████████████████                                                      | 97556/300000 [26:51<51:01, 66.12it/s]
epoch 97500  training loss: 3.5127649307250977

 33%|██████████████████████████                                                      | 97689/300000 [26:53<51:20, 65.68it/s]
epoch 97600  training loss: 3.4733619689941406

 33%|██████████████████████████                                                      | 97822/300000 [26:55<50:11, 67.14it/s]
epoch 97700  training loss: 3.4926772117614746
epoch 97700  clean testing loss: 11.337482452392578
epoch 97800  training loss: 3.594594955444336

 33%|██████████████████████████                                                      | 97955/300000 [26:57<51:16, 65.67it/s]
epoch 97900  training loss: 3.5561137199401855

 33%|██████████████████████████▏                                                     | 98088/300000 [26:59<51:07, 65.81it/s]
epoch 98000  training loss: 3.6012487411499023
epoch 98000  clean testing loss: 11.315574645996094

 33%|██████████████████████████▏                                                     | 98221/300000 [27:01<51:01, 65.91it/s]
epoch 98100  training loss: 3.5142366886138916
epoch 98100  clean testing loss: 11.34543228149414
epoch 98200  training loss: 3.381990909576416

 33%|██████████████████████████▏                                                     | 98347/300000 [27:03<51:12, 65.64it/s]
epoch 98300  training loss: 3.587519407272339

 33%|██████████████████████████▎                                                     | 98480/300000 [27:05<51:25, 65.32it/s]
epoch 98400  training loss: 3.4693844318389893

 33%|██████████████████████████▎                                                     | 98613/300000 [27:07<51:09, 65.61it/s]
epoch 98500  training loss: 3.7494587898254395
epoch 98500  clean testing loss: 11.323357582092285
epoch 98600  training loss: 3.4271178245544434

 33%|██████████████████████████▎                                                     | 98746/300000 [27:09<51:25, 65.23it/s]
epoch 98700  training loss: 3.70997953414917
epoch 98700  clean testing loss: 11.251297950744629
epoch 98800  training loss: 3.595377206802368
epoch 98800  clean testing loss: 11.24535846710205
epoch 98900  training loss: 3.5289011001586914
epoch 98900  clean testing loss: 11.28458023071289
epoch 99000  training loss: 3.473191499710083
epoch 99000  clean testing loss: 11.256009101867676

 33%|██████████████████████████▍                                                     | 99040/300000 [27:13<51:11, 65.43it/s]
epoch 99100  training loss: 3.635712146759033

 33%|██████████████████████████▍                                                     | 99173/300000 [27:15<51:01, 65.59it/s]
epoch 99200  training loss: 3.663667917251587
epoch 99200  clean testing loss: 11.351582527160645
epoch 99300  training loss: 3.7384719848632812

 33%|██████████████████████████▍                                                     | 99306/300000 [27:17<50:59, 65.59it/s]
epoch 99400  training loss: 3.3885021209716797

 33%|██████████████████████████▌                                                     | 99439/300000 [27:20<51:12, 65.27it/s]
epoch 99500  training loss: 3.570108413696289

 33%|██████████████████████████▌                                                     | 99565/300000 [27:21<51:24, 64.99it/s]
epoch 99600  training loss: 3.421292543411255

 33%|██████████████████████████▌                                                     | 99698/300000 [27:23<50:15, 66.43it/s]
epoch 99700  training loss: 3.5301167964935303
epoch 99700  clean testing loss: 11.438053131103516
epoch 99800  training loss: 3.593498706817627

 33%|██████████████████████████▌                                                     | 99831/300000 [27:25<50:58, 65.44it/s]
epoch 99900  training loss: 3.617757797241211

 33%|██████████████████████████▋                                                     | 99968/300000 [27:28<45:49, 72.76it/s]
epoch 100000  training loss: 3.6004605293273926
epoch 100000  clean testing loss: 11.356119155883789


 33%|█████████████████████████▋                                                   | 100097/300000 [27:49<1:09:17, 48.08it/s]
epoch 100100  training loss: 3.633927822113037

 33%|██████████████████████████▍                                                    | 100230/300000 [27:51<50:20, 66.15it/s]
epoch 100200  training loss: 3.562786340713501

 33%|██████████████████████████▍                                                    | 100363/300000 [27:53<50:42, 65.62it/s]
epoch 100300  training loss: 3.6501846313476562

 33%|██████████████████████████▍                                                    | 100489/300000 [27:54<50:14, 66.18it/s]
epoch 100400  training loss: 3.4840331077575684
epoch 100400  clean testing loss: 11.169561386108398
epoch 100500  training loss: 3.4736266136169434

 34%|██████████████████████████▍                                                    | 100622/300000 [27:57<50:16, 66.09it/s]
epoch 100600  training loss: 3.8103044033050537

 34%|██████████████████████████▌                                                    | 100755/300000 [27:59<50:18, 66.00it/s]
epoch 100700  training loss: 3.7794289588928223

 34%|██████████████████████████▌                                                    | 100888/300000 [28:01<50:24, 65.84it/s]
epoch 100800  training loss: 3.542383909225464
epoch 100800  clean testing loss: 11.200909614562988
epoch 100900  training loss: 3.5652730464935303

 34%|██████████████████████████▌                                                    | 101000/300000 [28:02<50:34, 65.57it/s]
epoch 101000  training loss: 3.6602721214294434
epoch 101000  clean testing loss: 11.318313598632812

 34%|██████████████████████████▋                                                    | 101110/300000 [28:09<54:29, 60.83it/s]
epoch 101100  training loss: 3.662325620651245

 34%|██████████████████████████▋                                                    | 101243/300000 [28:11<50:38, 65.41it/s]
epoch 101200  training loss: 3.603680372238159
epoch 101200  clean testing loss: 11.270215034484863
epoch 101300  training loss: 3.599681854248047


 34%|██████████████████████████▋                                                    | 101502/300000 [28:15<51:24, 64.35it/s]
epoch 101400  training loss: 3.53073787689209

 34%|██████████████████████████▊                                                    | 101628/300000 [28:17<56:07, 58.91it/s]
epoch 101500  training loss: 3.565821409225464
epoch 101500  clean testing loss: 11.22635555267334
epoch 101600  training loss: 3.8010332584381104

 34%|██████████████████████████▊                                                    | 101761/300000 [28:19<50:38, 65.25it/s]
epoch 101700  training loss: 3.63523530960083

 34%|██████████████████████████▊                                                    | 101894/300000 [28:21<49:43, 66.41it/s]
epoch 101800  training loss: 3.5772507190704346

 34%|██████████████████████████▊                                                    | 102027/300000 [28:23<49:45, 66.31it/s]
epoch 101900  training loss: 3.657341480255127
epoch 101900  clean testing loss: 11.211832046508789
epoch 102000  training loss: 3.512798547744751
epoch 102000  clean testing loss: 11.1294527053833

 34%|██████████████████████████▉                                                    | 102153/300000 [28:25<50:41, 65.06it/s]
epoch 102100  training loss: 3.740095376968384

 34%|██████████████████████████▉                                                    | 102286/300000 [28:27<50:09, 65.70it/s]
epoch 102200  training loss: 3.443934440612793

 34%|██████████████████████████▉                                                    | 102419/300000 [28:29<49:59, 65.87it/s]
epoch 102300  training loss: 3.5845768451690674
epoch 102300  clean testing loss: 11.088822364807129
epoch 102400  training loss: 3.514399528503418

 34%|███████████████████████████                                                    | 102552/300000 [28:31<50:07, 65.66it/s]
epoch 102500  training loss: 3.6425487995147705

 34%|███████████████████████████                                                    | 102720/300000 [28:34<50:19, 65.34it/s]
epoch 102600  training loss: 3.6683623790740967
epoch 102600  clean testing loss: 11.035360336303711
epoch 102700  training loss: 3.574716091156006

 34%|███████████████████████████                                                    | 102846/300000 [28:36<49:33, 66.29it/s]
epoch 102800  training loss: 3.612368583679199
epoch 102800  clean testing loss: 11.071873664855957
epoch 102900  training loss: 3.736400604248047
epoch 102900  clean testing loss: 11.024002075195312
epoch 103000  training loss: 3.5271942615509033
epoch 103000  clean testing loss: 11.03157901763916

 34%|███████████████████████████                                                    | 102979/300000 [28:38<49:51, 65.87it/s]
epoch 103100  training loss: 3.5895919799804688

 34%|███████████████████████████▏                                                   | 103120/300000 [28:41<50:44, 64.68it/s]
epoch 103200  training loss: 3.657028913497925

 34%|███████████████████████████▏                                                   | 103253/300000 [28:43<49:54, 65.71it/s]
epoch 103300  training loss: 3.665156841278076


 35%|███████████████████████████▎                                                   | 103519/300000 [28:47<50:05, 65.38it/s]
epoch 103400  training loss: 3.6278293132781982
epoch 103400  clean testing loss: 10.98462200164795
epoch 103500  training loss: 3.6877822875976562

 35%|███████████████████████████▎                                                   | 103645/300000 [28:49<49:29, 66.11it/s]
epoch 103600  training loss: 3.573265314102173
epoch 103600  clean testing loss: 10.961404800415039
epoch 103700  training loss: 3.8292534351348877

 35%|███████████████████████████▎                                                   | 103778/300000 [28:51<49:56, 65.48it/s]
epoch 103800  training loss: 3.6618762016296387
epoch 103800  clean testing loss: 11.020272254943848
epoch 103900  training loss: 3.7468178272247314

 35%|███████████████████████████▎                                                   | 103911/300000 [28:53<49:55, 65.45it/s]
epoch 104000  training loss: 3.647055149078369
epoch 104000  clean testing loss: 10.956315994262695


 35%|██████████████████████████▋                                                  | 104079/300000 [29:03<1:11:10, 45.88it/s]
epoch 104100  training loss: 3.571514844894409

 35%|███████████████████████████▍                                                   | 104212/300000 [29:05<49:33, 65.85it/s]
epoch 104200  training loss: 3.66342830657959
epoch 104200  clean testing loss: 11.016179084777832
epoch 104300  training loss: 3.5070548057556152

 35%|███████████████████████████▍                                                   | 104345/300000 [29:07<49:47, 65.49it/s]
epoch 104400  training loss: 3.5954298973083496

 35%|███████████████████████████▌                                                   | 104471/300000 [29:09<49:49, 65.40it/s]
epoch 104500  training loss: 3.6007461547851562

 35%|███████████████████████████▌                                                   | 104604/300000 [29:11<50:26, 64.57it/s]
epoch 104600  training loss: 3.578585147857666
epoch 104600  clean testing loss: 10.985349655151367
epoch 104700  training loss: 3.6553127765655518

 35%|███████████████████████████▌                                                   | 104737/300000 [29:13<49:13, 66.12it/s]
epoch 104800  training loss: 3.589935779571533

 35%|███████████████████████████▌                                                   | 104870/300000 [29:15<49:58, 65.07it/s]
epoch 104900  training loss: 3.6954615116119385

 35%|███████████████████████████▋                                                   | 104996/300000 [29:17<49:01, 66.30it/s]
epoch 105000  training loss: 3.5580127239227295
epoch 105000  clean testing loss: 11.005435943603516

 35%|███████████████████████████▋                                                   | 105122/300000 [29:23<50:56, 63.75it/s]
epoch 105100  training loss: 3.657121419906616

 35%|███████████████████████████▋                                                   | 105255/300000 [29:25<48:43, 66.62it/s]
epoch 105200  training loss: 3.61063289642334

 35%|███████████████████████████▊                                                   | 105388/300000 [29:27<49:30, 65.51it/s]
epoch 105300  training loss: 3.758260726928711

 35%|███████████████████████████▊                                                   | 105521/300000 [29:29<49:04, 66.05it/s]
epoch 105400  training loss: 3.647155523300171
epoch 105400  clean testing loss: 10.91323471069336
epoch 105500  training loss: 3.5243451595306396

 35%|███████████████████████████▊                                                   | 105654/300000 [29:31<49:22, 65.60it/s]
epoch 105600  training loss: 3.6761882305145264

 35%|███████████████████████████▊                                                   | 105800/300000 [29:33<47:35, 68.00it/s]
epoch 105700  training loss: 3.57060170173645
epoch 105700  clean testing loss: 10.863341331481934
epoch 105800  training loss: 3.698637008666992
epoch 105800  clean testing loss: 10.847634315490723
epoch 105900  training loss: 3.4635722637176514

 35%|███████████████████████████▉                                                   | 105926/300000 [29:35<49:19, 65.59it/s]
epoch 106000  training loss: 3.802382469177246
epoch 106000  clean testing loss: 10.833159446716309

 35%|███████████████████████████▉                                                   | 106017/300000 [29:37<49:20, 65.53it/s]
epoch 106100  training loss: 3.5539796352386475
epoch 106100  clean testing loss: 10.899765968322754
epoch 106200  training loss: 3.6500589847564697
epoch 106200  clean testing loss: 10.806882858276367
epoch 106300  training loss: 3.6047146320343018
epoch 106300  clean testing loss: 10.87317180633545
epoch 106400  training loss: 3.7288365364074707
epoch 106400  clean testing loss: 10.857076644897461
epoch 106500  training loss: 3.7862937450408936
epoch 106500  clean testing loss: 10.815596580505371
epoch 106600  training loss: 3.7226362228393555
epoch 106600  clean testing loss: 10.865891456604004
epoch 106700  training loss: 3.584752082824707

 36%|███████████████████████████                                                 | 106765/300000 [46:59<80:57:25,  1.51s/it]
epoch 106800  training loss: 3.70365834236145

 36%|████████████████████████████▏                                                  | 106893/300000 [47:00<58:39, 54.87it/s]
epoch 106900  training loss: 3.653475284576416
epoch 106900  clean testing loss: 10.863751411437988
epoch 107000  training loss: 3.6727232933044434
epoch 107000  clean testing loss: 10.887088775634766

 36%|████████████████████████████▏                                                  | 107019/300000 [47:02<51:20, 62.65it/s]
epoch 107100  training loss: 3.522385835647583

 36%|████████████████████████████▏                                                  | 107152/300000 [47:04<49:13, 65.29it/s]
epoch 107200  training loss: 3.7375473976135254

 36%|████████████████████████████▎                                                  | 107280/300000 [47:06<47:31, 67.58it/s]
epoch 107300  training loss: 3.576220989227295

 36%|████████████████████████████▎                                                  | 107413/300000 [47:08<48:57, 65.55it/s]
epoch 107400  training loss: 3.5043625831604004
epoch 107400  clean testing loss: 10.829943656921387
epoch 107500  training loss: 3.4162769317626953

 36%|████████████████████████████▎                                                  | 107539/300000 [47:10<51:58, 61.72it/s]
epoch 107600  training loss: 3.425267219543457

 36%|████████████████████████████▎                                                  | 107672/300000 [47:12<49:20, 64.97it/s]
epoch 107700  training loss: 3.5969889163970947

 36%|████████████████████████████▍                                                  | 107805/300000 [47:15<49:25, 64.82it/s]
epoch 107800  training loss: 3.427363395690918
epoch 107800  clean testing loss: 10.860288619995117
epoch 107900  training loss: 3.516735553741455

 36%|████████████████████████████▍                                                  | 108128/300000 [47:21<49:15, 64.92it/s]
epoch 108000  training loss: 3.568481922149658
epoch 108000  clean testing loss: 10.846465110778809
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop1 ...
epoch 108100  training loss: 3.454688787460327

 36%|████████████████████████████▌                                                  | 108254/300000 [47:22<48:46, 65.53it/s]
epoch 108200  training loss: 3.438295602798462

 36%|████████████████████████████▌                                                  | 108387/300000 [47:25<48:52, 65.34it/s]
epoch 108300  training loss: 3.478621482849121


 36%|████████████████████████████▌                                                  | 108646/300000 [47:29<49:27, 64.48it/s]
epoch 108400  training loss: 3.309107780456543
epoch 108400  clean testing loss: 10.822291374206543
epoch 108500  training loss: 3.41403865814209
epoch 108500  clean testing loss: 10.774930000305176
epoch 108600  training loss: 3.5321011543273926

 36%|████████████████████████████▋                                                  | 108779/300000 [47:31<48:40, 65.48it/s]
epoch 108700  training loss: 3.505695104598999

 36%|████████████████████████████▋                                                  | 108912/300000 [47:33<48:28, 65.69it/s]
epoch 108800  training loss: 3.4928805828094482
epoch 108800  clean testing loss: 10.841734886169434
epoch 108900  training loss: 3.384491443634033

 36%|████████████████████████████▋                                                  | 109038/300000 [47:35<49:00, 64.94it/s]
epoch 109000  training loss: 3.403851270675659
epoch 109000  clean testing loss: 10.871241569519043

 36%|████████████████████████████▋                                                  | 109171/300000 [47:37<48:39, 65.37it/s]
epoch 109100  training loss: 3.4061312675476074

 36%|████████████████████████████▊                                                  | 109304/300000 [47:39<48:49, 65.09it/s]
epoch 109200  training loss: 3.4884660243988037
epoch 109200  clean testing loss: 10.892168045043945
epoch 109300  training loss: 3.4862637519836426

 36%|████████████████████████████▊                                                  | 109437/300000 [47:41<48:20, 65.71it/s]
epoch 109400  training loss: 3.4316036701202393

 37%|████████████████████████████▊                                                  | 109570/300000 [47:43<48:09, 65.90it/s]
epoch 109500  training loss: 3.3420703411102295

 37%|████████████████████████████▉                                                  | 109696/300000 [47:45<48:03, 66.00it/s]
epoch 109600  training loss: 3.3179845809936523
epoch 109600  clean testing loss: 10.890311241149902
epoch 109700  training loss: 3.373342990875244

 37%|████████████████████████████▉                                                  | 109829/300000 [47:47<48:21, 65.53it/s]
epoch 109800  training loss: 3.4779815673828125
epoch 109800  clean testing loss: 10.902671813964844
epoch 109900  training loss: 3.431401252746582
epoch 109900  clean testing loss: 10.881322860717773
epoch 110000  training loss: 3.4736640453338623
epoch 110000  clean testing loss: 10.869256973266602


 37%|█████████████████████████████                                                  | 110221/300000 [47:53<48:16, 65.51it/s]
epoch 110100  training loss: 3.435927629470825

 37%|█████████████████████████████                                                  | 110354/300000 [47:55<48:04, 65.74it/s]
epoch 110200  training loss: 3.3872451782226562
epoch 110200  clean testing loss: 10.811774253845215
epoch 110300  training loss: 3.463033676147461

 37%|█████████████████████████████                                                  | 110480/300000 [47:57<47:42, 66.21it/s]
epoch 110400  training loss: 3.4313390254974365

 37%|█████████████████████████████▏                                                 | 110718/300000 [48:00<48:29, 65.05it/s]
epoch 110500  training loss: 3.4750163555145264
epoch 110500  clean testing loss: 10.786075592041016
epoch 110600  training loss: 3.3466906547546387
epoch 110600  clean testing loss: 10.778436660766602
epoch 110700  training loss: 3.291677713394165

 37%|█████████████████████████████▏                                                 | 110844/300000 [48:02<47:51, 65.88it/s]
epoch 110800  training loss: 3.3650734424591064
epoch 110800  clean testing loss: 10.801459312438965
epoch 110900  training loss: 3.464421272277832

 37%|█████████████████████████████▏                                                 | 110978/300000 [48:04<46:37, 67.57it/s]
epoch 111000  training loss: 3.3506293296813965
epoch 111000  clean testing loss: 10.812699317932129

 37%|█████████████████████████████▏                                                 | 110994/300000 [48:04<43:50, 71.85it/s]
epoch 111100  training loss: 3.403850793838501

 37%|█████████████████████████████▎                                                 | 111100/300000 [48:13<54:28, 57.79it/s]
epoch 111200  training loss: 3.342081069946289

 37%|█████████████████████████████▎                                                 | 111233/300000 [48:15<48:50, 64.42it/s]
epoch 111300  training loss: 3.4721601009368896

 37%|█████████████████████████████▎                                                 | 111359/300000 [48:17<47:56, 65.57it/s]
epoch 111400  training loss: 3.388906955718994

 37%|█████████████████████████████▎                                                 | 111492/300000 [48:19<47:55, 65.57it/s]
epoch 111500  training loss: 3.399714469909668
epoch 111500  clean testing loss: 10.798370361328125
epoch 111600  training loss: 3.2955095767974854

 37%|█████████████████████████████▍                                                 | 111625/300000 [48:21<49:19, 63.65it/s]
epoch 111700  training loss: 3.444629669189453

 37%|█████████████████████████████▍                                                 | 111751/300000 [48:23<48:16, 64.98it/s]
epoch 111800  training loss: 3.455993890762329

 37%|█████████████████████████████▍                                                 | 111884/300000 [48:25<47:38, 65.82it/s]
epoch 111900  training loss: 3.272461175918579
epoch 111900  clean testing loss: 10.815875053405762
epoch 112000  training loss: 3.4367246627807617
epoch 112000  clean testing loss: 10.823840141296387

 37%|█████████████████████████████▍                                                 | 112017/300000 [48:27<49:50, 62.87it/s]
epoch 112100  training loss: 3.3945791721343994

 37%|█████████████████████████████▌                                                 | 112143/300000 [48:29<49:26, 63.33it/s]
epoch 112200  training loss: 3.447113513946533

 37%|█████████████████████████████▌                                                 | 112276/300000 [48:31<47:07, 66.39it/s]
epoch 112300  training loss: 3.3961875438690186
epoch 112300  clean testing loss: 10.775721549987793
epoch 112400  training loss: 3.3761208057403564

 37%|█████████████████████████████▌                                                 | 112409/300000 [48:33<47:25, 65.92it/s]
epoch 112500  training loss: 3.3955986499786377

 38%|█████████████████████████████▋                                                 | 112542/300000 [48:35<47:28, 65.81it/s]
epoch 112600  training loss: 3.399505138397217

 38%|█████████████████████████████▋                                                 | 112668/300000 [48:37<47:25, 65.83it/s]
epoch 112700  training loss: 3.4952409267425537
epoch 112700  clean testing loss: 10.746689796447754
epoch 112800  training loss: 3.388216972351074

 38%|█████████████████████████████▋                                                 | 112801/300000 [48:39<47:28, 65.71it/s]
epoch 112900  training loss: 3.2854273319244385

 38%|█████████████████████████████▋                                                 | 112934/300000 [48:41<47:13, 66.02it/s]
epoch 113000  training loss: 3.2727904319763184
epoch 113000  clean testing loss: 10.759653091430664
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop1 ...
epoch 113100  training loss: 3.3381118774414062


 38%|█████████████████████████████▊                                                 | 113250/300000 [48:48<47:19, 65.77it/s]
epoch 113200  training loss: 3.288414716720581
epoch 113200  clean testing loss: 10.760873794555664
epoch 113300  training loss: 3.309452772140503

 38%|█████████████████████████████▊                                                 | 113383/300000 [48:50<46:55, 66.27it/s]
epoch 113400  training loss: 3.2587103843688965
epoch 113400  clean testing loss: 10.754842758178711
epoch 113500  training loss: 3.3407652378082275
epoch 113500  clean testing loss: 10.72851848602295
epoch 113600  training loss: 3.189547300338745
epoch 113600  clean testing loss: 10.74147891998291
epoch 113700  training loss: 3.228562593460083
epoch 113700  clean testing loss: 10.752717018127441
epoch 113800  training loss: 3.196197509765625
epoch 113800  clean testing loss: 10.7384033203125
epoch 113900  training loss: 3.2771079540252686

 38%|█████████████████████████████▉                                                 | 113915/300000 [48:58<47:10, 65.74it/s]
epoch 114000  training loss: 3.1980514526367188
epoch 114000  clean testing loss: 10.758315086364746


 38%|██████████████████████████████                                                 | 114181/300000 [49:02<46:56, 65.98it/s]
epoch 114100  training loss: 3.193267345428467

 38%|██████████████████████████████                                                 | 114307/300000 [49:04<47:11, 65.59it/s]
epoch 114200  training loss: 3.281186580657959
epoch 114200  clean testing loss: 10.725008010864258
epoch 114300  training loss: 3.1990461349487305

 38%|██████████████████████████████▏                                                | 114440/300000 [49:06<47:25, 65.22it/s]
epoch 114400  training loss: 3.1612305641174316

 38%|██████████████████████████████▏                                                | 114573/300000 [49:08<46:29, 66.48it/s]
epoch 114500  training loss: 3.362373113632202

 38%|██████████████████████████████▏                                                | 114706/300000 [49:10<46:30, 66.40it/s]
epoch 114600  training loss: 3.2585391998291016
epoch 114600  clean testing loss: 10.769312858581543
epoch 114700  training loss: 3.3296866416931152
epoch 114700  clean testing loss: 10.775008201599121
epoch 114800  training loss: 3.2089672088623047

 38%|██████████████████████████████▏                                                | 114839/300000 [49:12<47:10, 65.41it/s]
epoch 114900  training loss: 3.412971019744873

 38%|██████████████████████████████▎                                                | 114972/300000 [49:14<46:48, 65.87it/s]
epoch 115000  training loss: 3.2374911308288574
epoch 115000  clean testing loss: 10.745105743408203


 38%|██████████████████████████████▎                                                | 115231/300000 [49:18<47:16, 65.15it/s]
epoch 115100  training loss: 3.260484457015991
epoch 115100  clean testing loss: 10.75033187866211
epoch 115200  training loss: 3.431447744369507

 38%|██████████████████████████████▍                                                | 115364/300000 [49:20<46:49, 65.71it/s]
epoch 115300  training loss: 3.4215548038482666
epoch 115300  clean testing loss: 10.755197525024414
epoch 115400  training loss: 3.3999314308166504


 39%|██████████████████████████████▍                                                | 115623/300000 [49:24<47:18, 64.95it/s]
epoch 115500  training loss: 3.2262041568756104
epoch 115500  clean testing loss: 10.766345977783203
epoch 115600  training loss: 3.2700212001800537

 39%|██████████████████████████████▍                                                | 115756/300000 [49:26<46:57, 65.40it/s]
epoch 115700  training loss: 3.3420252799987793
epoch 115700  clean testing loss: 10.769143104553223
epoch 115800  training loss: 3.2900168895721436


 39%|██████████████████████████████▌                                                | 116022/300000 [49:30<46:52, 65.41it/s]
epoch 115900  training loss: 3.2550723552703857
epoch 115900  clean testing loss: 10.745882987976074
epoch 116000  training loss: 3.3371741771698
epoch 116000  clean testing loss: 10.762693405151367

 39%|██████████████████████████████▌                                                | 116155/300000 [49:32<46:36, 65.74it/s]
epoch 116100  training loss: 3.2006657123565674
epoch 116100  clean testing loss: 10.769157409667969
epoch 116200  training loss: 3.2624709606170654

 39%|██████████████████████████████▌                                                | 116288/300000 [49:34<46:37, 65.68it/s]
epoch 116300  training loss: 3.1491973400115967
epoch 116300  clean testing loss: 10.76443099975586
epoch 116400  training loss: 3.4835264682769775


 39%|██████████████████████████████▋                                                | 116547/300000 [49:38<46:30, 65.73it/s]
epoch 116500  training loss: 3.310163736343384
epoch 116500  clean testing loss: 10.783148765563965
epoch 116600  training loss: 3.3992927074432373

 39%|██████████████████████████████▋                                                | 116680/300000 [49:40<47:01, 64.97it/s]
epoch 116700  training loss: 3.3297603130340576
epoch 116700  clean testing loss: 10.76185417175293
epoch 116800  training loss: 3.3824305534362793

 39%|██████████████████████████████▊                                                | 116813/300000 [49:42<46:34, 65.54it/s]
epoch 116900  training loss: 3.231487512588501

 39%|██████████████████████████████▊                                                | 116946/300000 [49:44<46:18, 65.88it/s]
epoch 117000  training loss: 3.305399179458618
epoch 117000  clean testing loss: 10.774420738220215

 39%|██████████████████████████████▊                                                | 117105/300000 [49:51<50:45, 60.06it/s]
epoch 117100  training loss: 3.2939553260803223

 39%|██████████████████████████████▊                                                | 117231/300000 [49:53<46:38, 65.30it/s]
epoch 117200  training loss: 3.331624984741211

 39%|██████████████████████████████▉                                                | 117364/300000 [49:55<46:16, 65.79it/s]
epoch 117300  training loss: 3.3197572231292725

 39%|██████████████████████████████▉                                                | 117497/300000 [49:57<46:15, 65.75it/s]
epoch 117400  training loss: 3.2972707748413086
epoch 117400  clean testing loss: 10.803560256958008
epoch 117500  training loss: 3.353949546813965

 39%|██████████████████████████████▉                                                | 117623/300000 [49:59<46:41, 65.10it/s]
epoch 117600  training loss: 3.468655824661255

 39%|███████████████████████████████                                                | 117756/300000 [50:01<46:20, 65.53it/s]
epoch 117700  training loss: 3.514648914337158

 39%|███████████████████████████████                                                | 117889/300000 [50:03<46:48, 64.85it/s]
epoch 117800  training loss: 3.313143253326416
epoch 117800  clean testing loss: 10.729910850524902
epoch 117900  training loss: 3.423341989517212

 39%|███████████████████████████████                                                | 118120/300000 [50:13<47:58, 63.18it/s]
epoch 118000  training loss: 3.253661632537842
epoch 118000  clean testing loss: 10.723320007324219
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop1 ...
epoch 118100  training loss: 3.3534438610076904

 39%|███████████████████████████████▏                                               | 118246/300000 [50:15<46:17, 65.43it/s]
epoch 118200  training loss: 3.325897455215454

 39%|███████████████████████████████▏                                               | 118379/300000 [50:17<46:21, 65.29it/s]
epoch 118300  training loss: 3.407956123352051

 40%|███████████████████████████████▏                                               | 118512/300000 [50:19<46:09, 65.54it/s]
epoch 118400  training loss: 3.335710048675537
epoch 118400  clean testing loss: 10.72569465637207
epoch 118500  training loss: 3.352783203125

 40%|███████████████████████████████▏                                               | 118645/300000 [50:21<45:51, 65.91it/s]
epoch 118600  training loss: 3.5272419452667236

 40%|███████████████████████████████▎                                               | 118771/300000 [50:23<46:00, 65.66it/s]
epoch 118700  training loss: 3.480803966522217

 40%|███████████████████████████████▎                                               | 118904/300000 [50:25<45:54, 65.73it/s]
epoch 118800  training loss: 3.3433234691619873

 40%|███████████████████████████████▎                                               | 118996/300000 [50:27<44:38, 67.57it/s]
epoch 118900  training loss: 3.2907724380493164
epoch 118900  clean testing loss: 10.706178665161133
epoch 119000  training loss: 3.2571465969085693
epoch 119000  clean testing loss: 10.717694282531738

 40%|███████████████████████████████▎                                               | 119115/300000 [50:37<50:30, 59.69it/s]
epoch 119100  training loss: 3.3196699619293213

 40%|███████████████████████████████▍                                               | 119241/300000 [50:39<46:42, 64.50it/s]
epoch 119200  training loss: 3.448794364929199
epoch 119200  clean testing loss: 10.6508207321167
epoch 119300  training loss: 3.294635772705078

 40%|███████████████████████████████▍                                               | 119374/300000 [50:41<45:19, 66.43it/s]
epoch 119400  training loss: 3.3873493671417236
epoch 119400  clean testing loss: 10.672698020935059
epoch 119500  training loss: 3.337064027786255

 40%|███████████████████████████████▍                                               | 119577/300000 [50:44<45:56, 65.46it/s]
epoch 119600  training loss: 3.309143304824829
epoch 119600  clean testing loss: 10.641580581665039
epoch 119700  training loss: 3.3722057342529297

 40%|███████████████████████████████▌                                               | 119703/300000 [50:46<45:30, 66.02it/s]
epoch 119800  training loss: 3.402965784072876

 40%|███████████████████████████████▌                                               | 119836/300000 [50:48<45:45, 65.61it/s]
epoch 119900  training loss: 3.288184642791748


 40%|███████████████████████████████▋                                               | 120102/300000 [50:52<46:08, 64.99it/s]
epoch 120000  training loss: 3.3001935482025146
epoch 120000  clean testing loss: 10.62662124633789
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop1 ...
epoch 120100  training loss: 3.445500612258911
epoch 120100  clean testing loss: 10.594642639160156
epoch 120200  training loss: 3.4176690578460693


 40%|███████████████████████████████▋                                               | 120361/300000 [50:56<46:03, 65.00it/s]
epoch 120300  training loss: 3.3982784748077393

 40%|███████████████████████████████▋                                               | 120494/300000 [50:58<45:50, 65.27it/s]
epoch 120400  training loss: 3.3612775802612305
epoch 120400  clean testing loss: 10.666990280151367
epoch 120500  training loss: 3.287365198135376

 40%|███████████████████████████████▊                                               | 120627/300000 [51:00<45:23, 65.87it/s]
epoch 120600  training loss: 3.3905582427978516

 40%|███████████████████████████████▊                                               | 120760/300000 [51:02<44:50, 66.61it/s]
epoch 120700  training loss: 3.4231503009796143

 40%|███████████████████████████████▊                                               | 120893/300000 [51:04<45:45, 65.24it/s]
epoch 120800  training loss: 3.36334490776062
epoch 120800  clean testing loss: 10.683494567871094
epoch 120900  training loss: 3.438903331756592

 40%|███████████████████████████████▊                                               | 121019/300000 [51:06<45:44, 65.22it/s]
epoch 121000  training loss: 3.3017995357513428
epoch 121000  clean testing loss: 10.739043235778809

 40%|███████████████████████████████▉                                               | 121152/300000 [51:08<45:14, 65.88it/s]
epoch 121100  training loss: 3.2595715522766113

 40%|███████████████████████████████▉                                               | 121285/300000 [51:10<44:58, 66.22it/s]
epoch 121200  training loss: 3.2964494228363037
epoch 121200  clean testing loss: 10.722057342529297
epoch 121300  training loss: 3.279381036758423

 41%|████████████████████████████████                                               | 121579/300000 [51:15<45:50, 64.88it/s]
epoch 121400  training loss: 3.2658064365386963
epoch 121400  clean testing loss: 10.723357200622559
epoch 121500  training loss: 3.2829766273498535

 41%|████████████████████████████████                                               | 121712/300000 [51:17<45:02, 65.97it/s]
epoch 121600  training loss: 3.354443073272705
epoch 121600  clean testing loss: 10.639860153198242
epoch 121700  training loss: 3.3729357719421387

 41%|████████████████████████████████                                               | 121845/300000 [51:19<45:22, 65.45it/s]
epoch 121800  training loss: 3.431461811065674

 41%|████████████████████████████████▏                                              | 122034/300000 [51:22<46:59, 63.12it/s]
epoch 121900  training loss: 3.2379369735717773
epoch 121900  clean testing loss: 10.670451164245605
epoch 122000  training loss: 3.254549026489258
epoch 122000  clean testing loss: 10.623830795288086

 41%|████████████████████████████████▏                                              | 122167/300000 [51:24<44:47, 66.18it/s]
epoch 122100  training loss: 3.3548531532287598
epoch 122100  clean testing loss: 10.632709503173828
epoch 122200  training loss: 3.2332472801208496
epoch 122200  clean testing loss: 10.607564926147461
epoch 122300  training loss: 3.3035504817962646

 41%|████████████████████████████████▏                                              | 122300/300000 [51:26<44:53, 65.98it/s]
epoch 122400  training loss: 3.354623794555664

 41%|████████████████████████████████▏                                              | 122433/300000 [51:28<46:26, 63.73it/s]
epoch 122500  training loss: 3.3032476902008057

 41%|████████████████████████████████▎                                              | 122566/300000 [51:30<44:54, 65.84it/s]
epoch 122600  training loss: 3.3229262828826904
epoch 122600  clean testing loss: 10.618364334106445
epoch 122700  training loss: 3.442349910736084

 41%|████████████████████████████████▎                                              | 122692/300000 [51:32<44:36, 66.23it/s]
epoch 122800  training loss: 3.380197763442993


 41%|████████████████████████████████▍                                              | 123099/300000 [51:43<51:36, 57.12it/s]
epoch 122900  training loss: 3.4524497985839844
epoch 122900  clean testing loss: 10.625996589660645
epoch 123000  training loss: 3.2407140731811523
epoch 123000  clean testing loss: 10.650982856750488
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop1 ...
epoch 123100  training loss: 3.3226425647735596

 41%|████████████████████████████████▍                                              | 123232/300000 [51:45<44:33, 66.11it/s]
epoch 123200  training loss: 3.46124529838562

 41%|████████████████████████████████▍                                              | 123358/300000 [51:47<44:47, 65.72it/s]
epoch 123300  training loss: 3.406709671020508

 41%|████████████████████████████████▌                                              | 123491/300000 [51:49<44:51, 65.58it/s]
epoch 123400  training loss: 3.4461240768432617
epoch 123400  clean testing loss: 10.649683952331543
epoch 123500  training loss: 3.2878577709198

 41%|████████████████████████████████▌                                              | 123624/300000 [51:51<44:38, 65.85it/s]
epoch 123600  training loss: 3.465787649154663

 41%|████████████████████████████████▌                                              | 123757/300000 [51:53<44:34, 65.91it/s]
epoch 123700  training loss: 3.3011364936828613

 41%|████████████████████████████████▌                                              | 123883/300000 [51:55<44:44, 65.61it/s]
epoch 123800  training loss: 3.3447999954223633
epoch 123800  clean testing loss: 10.670461654663086
epoch 123900  training loss: 3.3423960208892822
epoch 123900  clean testing loss: 10.667122840881348
epoch 124000  training loss: 3.291991949081421
epoch 124000  clean testing loss: 10.6665620803833

 41%|████████████████████████████████▋                                              | 124016/300000 [51:57<44:29, 65.92it/s]
epoch 124100  training loss: 3.475860834121704


 41%|████████████████████████████████▋                                              | 124282/300000 [52:01<44:24, 65.94it/s]
epoch 124200  training loss: 3.322185754776001
epoch 124200  clean testing loss: 10.673611640930176
epoch 124300  training loss: 3.4520480632781982
epoch 124300  clean testing loss: 10.677861213684082
epoch 124400  training loss: 3.2506680488586426
epoch 124400  clean testing loss: 10.659262657165527
epoch 124500  training loss: 3.2563164234161377
epoch 124500  clean testing loss: 10.642340660095215
epoch 124600  training loss: 3.3214612007141113

 42%|████████████████████████████████▊                                              | 124674/300000 [52:07<44:35, 65.52it/s]
epoch 124700  training loss: 3.279374599456787

 42%|████████████████████████████████▊                                              | 124807/300000 [52:09<44:29, 65.62it/s]
epoch 124800  training loss: 3.383277416229248
epoch 124800  clean testing loss: 10.640750885009766
epoch 124900  training loss: 3.2495229244232178

 42%|████████████████████████████████▉                                              | 124940/300000 [52:12<44:33, 65.48it/s]
epoch 125000  training loss: 3.489783525466919
epoch 125000  clean testing loss: 10.649391174316406

 42%|████████████████████████████████▉                                              | 125066/300000 [52:13<44:12, 65.94it/s]
epoch 125100  training loss: 3.459686040878296

 42%|████████████████████████████████▉                                              | 125199/300000 [52:15<44:38, 65.26it/s]
epoch 125200  training loss: 3.2961349487304688
epoch 125200  clean testing loss: 10.663057327270508
epoch 125300  training loss: 3.5865490436553955

 42%|█████████████████████████████████                                              | 125332/300000 [52:18<44:31, 65.38it/s]
epoch 125400  training loss: 3.399759531021118

 42%|█████████████████████████████████                                              | 125465/300000 [52:20<44:24, 65.51it/s]
epoch 125500  training loss: 3.38114595413208

 42%|█████████████████████████████████                                              | 125598/300000 [52:22<44:25, 65.44it/s]
epoch 125600  training loss: 3.529181718826294
epoch 125600  clean testing loss: 10.675033569335938
epoch 125700  training loss: 3.396937370300293
epoch 125700  clean testing loss: 10.660355567932129
epoch 125800  training loss: 3.483133316040039
epoch 125800  clean testing loss: 10.681872367858887
epoch 125900  training loss: 3.4444146156311035

 42%|█████████████████████████████████▏                                             | 125990/300000 [52:28<43:49, 66.18it/s]
epoch 126000  training loss: 3.4535655975341797
epoch 126000  clean testing loss: 10.690561294555664

 42%|████████████████████████████████▎                                            | 126017/300000 [52:30<2:20:13, 20.68it/s]
epoch 126100  training loss: 3.4547696113586426

 42%|█████████████████████████████████▏                                             | 126150/300000 [52:32<44:25, 65.23it/s]
epoch 126200  training loss: 3.435199499130249

 42%|█████████████████████████████████▎                                             | 126283/300000 [52:34<43:52, 65.99it/s]
epoch 126300  training loss: 3.433558702468872
epoch 126300  clean testing loss: 10.676166534423828
epoch 126400  training loss: 3.392789125442505

 42%|█████████████████████████████████▎                                             | 126416/300000 [52:36<43:40, 66.24it/s]
epoch 126500  training loss: 3.3936190605163574

 42%|█████████████████████████████████▎                                             | 126549/300000 [52:38<44:13, 65.36it/s]
epoch 126600  training loss: 3.4451968669891357

 42%|█████████████████████████████████▎                                             | 126675/300000 [52:40<44:30, 64.89it/s]
epoch 126700  training loss: 3.5679211616516113
epoch 126700  clean testing loss: 10.660558700561523
epoch 126800  training loss: 3.51625394821167

 42%|█████████████████████████████████▍                                             | 126808/300000 [52:42<43:57, 65.67it/s]
epoch 126900  training loss: 3.4745049476623535
epoch 126900  clean testing loss: 10.67702579498291
epoch 127000  training loss: 3.479555130004883
epoch 127000  clean testing loss: 10.664534568786621
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop1 ...
epoch 127100  training loss: 3.4110026359558105


 42%|█████████████████████████████████▌                                             | 127233/300000 [52:50<44:20, 64.93it/s]
epoch 127200  training loss: 3.3738696575164795

 42%|█████████████████████████████████▌                                             | 127366/300000 [52:52<43:56, 65.47it/s]
epoch 127300  training loss: 3.5115644931793213

 42%|█████████████████████████████████▌                                             | 127499/300000 [52:54<44:08, 65.13it/s]
epoch 127400  training loss: 3.4803578853607178

 43%|█████████████████████████████████▌                                             | 127625/300000 [52:56<43:21, 66.25it/s]
epoch 127500  training loss: 3.3442506790161133
epoch 127500  clean testing loss: 10.661584854125977
epoch 127600  training loss: 3.431657552719116

 43%|█████████████████████████████████▋                                             | 127758/300000 [52:58<43:28, 66.02it/s]
epoch 127700  training loss: 3.2792515754699707

 43%|█████████████████████████████████▋                                             | 127891/300000 [53:00<43:20, 66.18it/s]
epoch 127800  training loss: 3.4086060523986816

 43%|█████████████████████████████████▋                                             | 128024/300000 [53:02<43:24, 66.02it/s]
epoch 127900  training loss: 3.370633840560913
epoch 127900  clean testing loss: 10.647482872009277
epoch 128000  training loss: 3.3714308738708496
epoch 128000  clean testing loss: 10.651623725891113

 43%|█████████████████████████████████▋                                             | 128157/300000 [53:04<43:14, 66.23it/s]
epoch 128100  training loss: 3.4138381481170654

 43%|█████████████████████████████████▊                                             | 128290/300000 [53:06<43:30, 65.78it/s]
epoch 128200  training loss: 3.440786600112915

 43%|█████████████████████████████████▊                                             | 128549/300000 [53:10<43:59, 64.95it/s]
epoch 128300  training loss: 3.452559471130371
epoch 128300  clean testing loss: 10.650209426879883
epoch 128400  training loss: 3.311947822570801
epoch 128400  clean testing loss: 10.686267852783203
epoch 128500  training loss: 3.399639368057251

 43%|█████████████████████████████████▉                                             | 128682/300000 [53:12<43:35, 65.51it/s]
epoch 128600  training loss: 3.3638205528259277
epoch 128600  clean testing loss: 10.672202110290527
epoch 128700  training loss: 3.3866991996765137
epoch 128700  clean testing loss: 10.672774314880371
epoch 128800  training loss: 3.363955497741699

 43%|█████████████████████████████████▉                                             | 128815/300000 [53:14<43:29, 65.61it/s]
epoch 128900  training loss: 3.347119092941284

 43%|█████████████████████████████████▉                                             | 128941/300000 [53:16<43:10, 66.03it/s]
epoch 129000  training loss: 3.4418351650238037
epoch 129000  clean testing loss: 10.672966957092285

 43%|█████████████████████████████████▉                                             | 129074/300000 [53:18<43:19, 65.75it/s]
epoch 129100  training loss: 3.4545578956604004
epoch 129100  clean testing loss: 10.68774700164795
epoch 129200  training loss: 3.2979607582092285

 43%|██████████████████████████████████                                             | 129207/300000 [53:20<44:01, 64.66it/s]
epoch 129300  training loss: 3.4442477226257324

 43%|██████████████████████████████████                                             | 129340/300000 [53:22<42:59, 66.17it/s]
epoch 129400  training loss: 3.4669487476348877

 43%|██████████████████████████████████                                             | 129473/300000 [53:24<42:31, 66.82it/s]
epoch 129500  training loss: 3.422632932662964

 43%|██████████████████████████████████▏                                            | 129599/300000 [53:26<42:52, 66.24it/s]
epoch 129600  training loss: 3.4578208923339844
epoch 129600  clean testing loss: 10.693975448608398
epoch 129700  training loss: 3.4551384449005127

 43%|██████████████████████████████████▏                                            | 129732/300000 [53:28<43:04, 65.89it/s]
epoch 129800  training loss: 3.4270341396331787
epoch 129800  clean testing loss: 10.695096015930176
epoch 129900  training loss: 3.450695753097534
epoch 129900  clean testing loss: 10.693069458007812
epoch 130000  training loss: 3.5704240798950195
epoch 130000  clean testing loss: 10.689180374145508
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop1 ...
epoch 130100  training loss: 3.400115489959717

 43%|██████████████████████████████████▎                                            | 130109/300000 [53:38<44:53, 63.06it/s]
epoch 130200  training loss: 3.358492374420166

 43%|██████████████████████████████████▎                                            | 130242/300000 [53:40<43:04, 65.69it/s]
epoch 130300  training loss: 3.495272159576416

 43%|██████████████████████████████████▎                                            | 130375/300000 [53:42<43:07, 65.56it/s]
epoch 130400  training loss: 3.4433205127716064
epoch 130400  clean testing loss: 10.687756538391113
epoch 130500  training loss: 3.4643468856811523

 44%|██████████████████████████████████▎                                            | 130508/300000 [53:44<42:57, 65.75it/s]
epoch 130600  training loss: 3.4017889499664307

 44%|██████████████████████████████████▍                                            | 130634/300000 [53:46<42:45, 66.01it/s]
epoch 130700  training loss: 3.4277331829071045


 44%|██████████████████████████████████▌                                            | 131033/300000 [53:52<43:03, 65.40it/s]
epoch 130800  training loss: 3.514780044555664
epoch 130800  clean testing loss: 10.69225025177002
epoch 130900  training loss: 3.3636367321014404
epoch 130900  clean testing loss: 10.690266609191895
epoch 131000  training loss: 3.460970640182495
epoch 131000  clean testing loss: 10.691890716552734

 44%|██████████████████████████████████▌                                            | 131166/300000 [53:54<42:47, 65.75it/s]
epoch 131100  training loss: 3.3654236793518066
epoch 131100  clean testing loss: 10.6835298538208
epoch 131200  training loss: 3.4167580604553223
epoch 131200  clean testing loss: 10.70616626739502
epoch 131300  training loss: 3.3260915279388428

 44%|██████████████████████████████████▌                                            | 131299/300000 [53:56<42:29, 66.16it/s]
epoch 131400  training loss: 3.3834638595581055

 44%|██████████████████████████████████▌                                            | 131425/300000 [53:58<42:55, 65.46it/s]
epoch 131500  training loss: 3.439775228500366

 44%|██████████████████████████████████▋                                            | 131558/300000 [54:00<42:30, 66.03it/s]
epoch 131600  training loss: 3.387702226638794

 44%|██████████████████████████████████▋                                            | 131691/300000 [54:02<42:29, 66.02it/s]
epoch 131700  training loss: 3.470052480697632
epoch 131700  clean testing loss: 10.723691940307617
epoch 131800  training loss: 3.3808891773223877

 44%|██████████████████████████████████▋                                            | 131824/300000 [54:04<42:52, 65.39it/s]
epoch 131900  training loss: 3.370572805404663

 44%|██████████████████████████████████▋                                            | 131950/300000 [54:06<42:30, 65.88it/s]
epoch 132000  training loss: 3.3830080032348633
epoch 132000  clean testing loss: 10.760272026062012

 44%|██████████████████████████████████▊                                            | 132083/300000 [54:08<42:22, 66.03it/s]
epoch 132100  training loss: 3.512939691543579
epoch 132100  clean testing loss: 10.74409294128418
epoch 132200  training loss: 3.394319534301758

 44%|██████████████████████████████████▊                                            | 132216/300000 [54:10<42:17, 66.13it/s]
epoch 132300  training loss: 3.2962725162506104

 44%|██████████████████████████████████▊                                            | 132349/300000 [54:12<42:27, 65.82it/s]
epoch 132400  training loss: 3.418977737426758

 44%|██████████████████████████████████▉                                            | 132482/300000 [54:14<42:32, 65.63it/s]
epoch 132500  training loss: 3.4545743465423584
epoch 132500  clean testing loss: 10.728852272033691
epoch 132600  training loss: 3.4606237411499023

 44%|██████████████████████████████████▉                                            | 132615/300000 [54:16<42:06, 66.26it/s]
epoch 132700  training loss: 3.3944714069366455
epoch 132700  clean testing loss: 10.755325317382812
epoch 132800  training loss: 3.202385425567627
epoch 132800  clean testing loss: 10.760112762451172
epoch 132900  training loss: 3.4366633892059326

 44%|███████████████████████████████████                                            | 133007/300000 [54:22<42:45, 65.09it/s]
epoch 133000  training loss: 3.455064535140991
epoch 133000  clean testing loss: 10.75777816772461
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop1 ...
epoch 133100  training loss: 3.466132879257202

 44%|███████████████████████████████████                                            | 133140/300000 [54:24<42:04, 66.11it/s]
epoch 133200  training loss: 3.3981399536132812

 44%|███████████████████████████████████                                            | 133273/300000 [54:26<41:59, 66.18it/s]
epoch 133300  training loss: 3.398543119430542

 45%|███████████████████████████████████▎                                           | 133861/300000 [54:35<42:24, 65.28it/s]
epoch 133400  training loss: 3.485261917114258
epoch 133400  clean testing loss: 10.714385032653809
epoch 133500  training loss: 3.330172538757324
epoch 133500  clean testing loss: 10.75289535522461
epoch 133600  training loss: 3.357792854309082
epoch 133600  clean testing loss: 10.753480911254883
epoch 133700  training loss: 3.4695541858673096
epoch 133700  clean testing loss: 10.735503196716309
epoch 133800  training loss: 3.463179111480713

 45%|███████████████████████████████████▎                                           | 133994/300000 [54:37<41:49, 66.16it/s]
epoch 133900  training loss: 3.3022537231445312

 45%|███████████████████████████████████▎                                           | 134127/300000 [54:39<41:41, 66.32it/s]
epoch 134000  training loss: 3.321713447570801
epoch 134000  clean testing loss: 10.739873886108398
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop1 ...
epoch 134100  training loss: 3.3222928047180176

 45%|███████████████████████████████████▎                                           | 134260/300000 [54:41<41:42, 66.23it/s]
epoch 134200  training loss: 3.4086577892303467

 45%|███████████████████████████████████▍                                           | 134393/300000 [54:43<41:51, 65.93it/s]
epoch 134300  training loss: 3.3332512378692627

 45%|███████████████████████████████████▍                                           | 134526/300000 [54:45<41:47, 65.99it/s]
epoch 134400  training loss: 3.3700013160705566
epoch 134400  clean testing loss: 10.7741060256958
epoch 134500  training loss: 3.381108283996582

 45%|███████████████████████████████████▍                                           | 134652/300000 [54:47<41:57, 65.68it/s]
epoch 134600  training loss: 3.328929901123047

 45%|███████████████████████████████████▍                                           | 134785/300000 [54:49<42:08, 65.35it/s]
epoch 134700  training loss: 3.5475664138793945

 45%|███████████████████████████████████▌                                           | 134918/300000 [54:51<41:12, 66.77it/s]
epoch 134800  training loss: 3.3399507999420166
epoch 134800  clean testing loss: 10.770502090454102
epoch 134900  training loss: 3.445603609085083

 45%|███████████████████████████████████▌                                           | 135051/300000 [54:53<41:53, 65.61it/s]
epoch 135000  training loss: 3.4821972846984863
epoch 135000  clean testing loss: 10.757759094238281

 45%|███████████████████████████████████▌                                           | 135184/300000 [54:55<41:43, 65.83it/s]
epoch 135100  training loss: 3.3758013248443604

 45%|███████████████████████████████████▋                                           | 135317/300000 [54:57<41:31, 66.09it/s]
epoch 135200  training loss: 3.5817887783050537

 45%|███████████████████████████████████▋                                           | 135450/300000 [54:59<41:51, 65.52it/s]
epoch 135300  training loss: 3.3155694007873535
epoch 135300  clean testing loss: 10.745390892028809
epoch 135400  training loss: 3.39719557762146

 45%|███████████████████████████████████▋                                           | 135576/300000 [55:01<42:11, 64.95it/s]
epoch 135500  training loss: 3.4618077278137207

 45%|███████████████████████████████████▋                                           | 135709/300000 [55:03<41:42, 65.66it/s]
epoch 135600  training loss: 3.3615241050720215

 45%|███████████████████████████████████▊                                           | 135842/300000 [55:05<41:49, 65.43it/s]
epoch 135700  training loss: 3.3521342277526855
epoch 135700  clean testing loss: 10.75515079498291
epoch 135800  training loss: 3.324172258377075

 45%|███████████████████████████████████▊                                           | 135975/300000 [55:07<41:16, 66.22it/s]
epoch 135900  training loss: 3.311157464981079

 45%|███████████████████████████████████▊                                           | 135996/300000 [55:07<41:36, 65.69it/s]
epoch 136000  training loss: 3.4585373401641846
epoch 136000  clean testing loss: 10.745030403137207

 45%|███████████████████████████████████▊                                           | 136113/300000 [55:31<51:16, 53.27it/s]
epoch 136100  training loss: 3.3574376106262207

 45%|███████████████████████████████████▉                                           | 136265/300000 [55:33<36:06, 75.59it/s]
epoch 136200  training loss: 3.413524627685547

 45%|███████████████████████████████████▉                                           | 136417/300000 [55:35<35:53, 75.96it/s]
epoch 136300  training loss: 3.4123728275299072
epoch 136300  clean testing loss: 10.72614574432373
epoch 136400  training loss: 3.2778022289276123
epoch 136400  clean testing loss: 10.728232383728027
epoch 136500  training loss: 3.5150363445281982
epoch 136500  clean testing loss: 10.738105773925781
epoch 136600  training loss: 3.5452117919921875
epoch 136600  clean testing loss: 10.724287986755371
epoch 136700  training loss: 3.456587553024292
epoch 136700  clean testing loss: 10.696555137634277
epoch 136800  training loss: 3.361250638961792
epoch 136800  clean testing loss: 10.7129487991333
epoch 136900  training loss: 3.3120577335357666
epoch 136900  clean testing loss: 10.715248107910156
epoch 137000  training loss: 3.352391481399536
epoch 137000  clean testing loss: 10.718369483947754
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop1 ...
epoch 137100  training loss: 3.435468912124634

 46%|████████████████████████████████████                                           | 137137/300000 [55:44<35:54, 75.60it/s]
epoch 137200  training loss: 3.3909828662872314

 46%|████████████████████████████████████▏                                          | 137289/300000 [55:46<35:58, 75.37it/s]
epoch 137300  training loss: 3.3860249519348145
epoch 137300  clean testing loss: 10.711636543273926
epoch 137400  training loss: 3.485517740249634

 46%|████████████████████████████████████▏                                          | 137433/300000 [55:48<35:48, 75.66it/s]
epoch 137500  training loss: 3.456958770751953
epoch 137500  clean testing loss: 10.716943740844727
epoch 137600  training loss: 3.4468190670013428

 46%|████████████████████████████████████▏                                          | 137585/300000 [55:50<35:44, 75.73it/s]
epoch 137700  training loss: 3.449967384338379

 46%|████████████████████████████████████▎                                          | 137737/300000 [55:52<35:42, 75.72it/s]
epoch 137800  training loss: 3.4632275104522705
epoch 137800  clean testing loss: 10.693740844726562
epoch 137900  training loss: 3.483825445175171


 46%|████████████████████████████████████▎                                          | 138113/300000 [56:00<37:44, 71.50it/s]
epoch 138000  training loss: 3.441037654876709
epoch 138000  clean testing loss: 10.707226753234863
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop1 ...
epoch 138100  training loss: 3.511643886566162
epoch 138100  clean testing loss: 10.692403793334961
epoch 138200  training loss: 3.462449312210083

 46%|████████████████████████████████████▍                                          | 138265/300000 [56:02<35:14, 76.50it/s]
epoch 138300  training loss: 3.3805582523345947
epoch 138300  clean testing loss: 10.688236236572266
epoch 138400  training loss: 3.3677022457122803


 46%|████████████████████████████████████▍                                          | 138569/300000 [56:06<35:20, 76.14it/s]
epoch 138500  training loss: 3.4995830059051514

 46%|████████████████████████████████████▌                                          | 138721/300000 [56:08<35:22, 76.00it/s]
epoch 138600  training loss: 3.4162354469299316
epoch 138600  clean testing loss: 10.687384605407715
epoch 138700  training loss: 3.2527480125427246

 46%|████████████████████████████████████▌                                          | 138873/300000 [56:10<35:19, 76.02it/s]
epoch 138800  training loss: 3.3171255588531494

 46%|████████████████████████████████████▌                                          | 139025/300000 [56:12<35:34, 75.43it/s]
epoch 138900  training loss: 3.4033596515655518
epoch 138900  clean testing loss: 10.691122055053711
epoch 139000  training loss: 3.2685344219207764
epoch 139000  clean testing loss: 10.70698356628418

 46%|████████████████████████████████████▋                                          | 139177/300000 [56:14<35:01, 76.52it/s]
epoch 139100  training loss: 3.4250011444091797

 46%|████████████████████████████████████▋                                          | 139329/300000 [56:16<35:31, 75.38it/s]
epoch 139200  training loss: 3.364135503768921
epoch 139200  clean testing loss: 10.698578834533691
epoch 139300  training loss: 3.3799502849578857

 46%|████████████████████████████████████▋                                          | 139481/300000 [56:18<35:18, 75.75it/s]
epoch 139400  training loss: 3.36811900138855

 47%|████████████████████████████████████▊                                          | 139633/300000 [56:20<35:10, 76.00it/s]
epoch 139500  training loss: 3.4229209423065186
epoch 139500  clean testing loss: 10.68593978881836
epoch 139600  training loss: 3.4517290592193604

 47%|████████████████████████████████████▊                                          | 139785/300000 [56:22<35:20, 75.55it/s]
epoch 139700  training loss: 3.41648268699646

 47%|████████████████████████████████████▊                                          | 139937/300000 [56:24<35:15, 75.66it/s]
epoch 139800  training loss: 3.5521562099456787
epoch 139800  clean testing loss: 10.69245719909668
epoch 139900  training loss: 3.6043243408203125

 47%|████████████████████████████████████▉                                          | 140111/300000 [56:38<43:27, 61.32it/s]
epoch 140000  training loss: 3.527862071990967
epoch 140000  clean testing loss: 10.7004976272583
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop1 ...
epoch 140100  training loss: 3.456261396408081

 47%|████████████████████████████████████▉                                          | 140255/300000 [56:40<35:27, 75.08it/s]
epoch 140200  training loss: 3.3018760681152344

 47%|████████████████████████████████████▉                                          | 140407/300000 [56:42<35:18, 75.33it/s]
epoch 140300  training loss: 3.3387911319732666
epoch 140300  clean testing loss: 10.678082466125488
epoch 140400  training loss: 3.3505806922912598

 47%|█████████████████████████████████████                                          | 140559/300000 [56:44<34:52, 76.18it/s]
epoch 140500  training loss: 3.247058153152466

 47%|█████████████████████████████████████                                          | 140711/300000 [56:46<35:03, 75.72it/s]
epoch 140600  training loss: 3.3940517902374268
epoch 140600  clean testing loss: 10.677607536315918
epoch 140700  training loss: 3.4092583656311035
epoch 140700  clean testing loss: 10.69046688079834
epoch 140800  training loss: 3.333300828933716
epoch 140800  clean testing loss: 10.692597389221191
epoch 140900  training loss: 3.4339823722839355
epoch 140900  clean testing loss: 10.698698997497559
epoch 141000  training loss: 3.418546676635742
epoch 141000  clean testing loss: 10.6988525390625
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop1 ...
epoch 141100  training loss: 3.3216991424560547

 47%|█████████████████████████████████████▏                                         | 141128/300000 [56:59<38:35, 68.62it/s]
epoch 141200  training loss: 3.4739997386932373

 47%|█████████████████████████████████████▏                                         | 141272/300000 [57:01<36:15, 72.96it/s]
epoch 141300  training loss: 3.377195358276367
epoch 141300  clean testing loss: 10.688383102416992
epoch 141400  training loss: 3.3312268257141113

 47%|█████████████████████████████████████▏                                         | 141424/300000 [57:03<35:06, 75.27it/s]
epoch 141500  training loss: 3.3487980365753174

 47%|█████████████████████████████████████▎                                         | 141576/300000 [57:05<36:03, 73.23it/s]
epoch 141600  training loss: 3.356375217437744
epoch 141600  clean testing loss: 10.690357208251953
epoch 141700  training loss: 3.3276290893554688

 47%|█████████████████████████████████████▎                                         | 141720/300000 [57:07<35:09, 75.03it/s]
epoch 141800  training loss: 3.4822614192962646

 47%|█████████████████████████████████████▎                                         | 141872/300000 [57:09<34:45, 75.81it/s]
epoch 141900  training loss: 3.5183072090148926
epoch 141900  clean testing loss: 10.697157859802246
epoch 142000  training loss: 3.44856858253479
epoch 142000  clean testing loss: 10.691311836242676

 47%|█████████████████████████████████████▍                                         | 142024/300000 [57:11<35:06, 74.99it/s]
epoch 142100  training loss: 3.276456594467163

 47%|█████████████████████████████████████▍                                         | 142176/300000 [57:13<34:43, 75.76it/s]
epoch 142200  training loss: 3.3878214359283447
epoch 142200  clean testing loss: 10.692157745361328
epoch 142300  training loss: 3.4103140830993652

 47%|█████████████████████████████████████▍                                         | 142328/300000 [57:15<34:38, 75.87it/s]
epoch 142400  training loss: 3.3460609912872314

 47%|█████████████████████████████████████▌                                         | 142480/300000 [57:17<34:41, 75.69it/s]
epoch 142500  training loss: 3.370760917663574
epoch 142500  clean testing loss: 10.688798904418945
epoch 142600  training loss: 3.445234775543213

 48%|█████████████████████████████████████▌                                         | 142632/300000 [57:19<34:44, 75.49it/s]
epoch 142700  training loss: 3.4539384841918945

 48%|█████████████████████████████████████▌                                         | 142784/300000 [57:21<34:48, 75.29it/s]
epoch 142800  training loss: 3.410033941268921
epoch 142800  clean testing loss: 10.672714233398438
epoch 142900  training loss: 3.308690309524536

 48%|█████████████████████████████████████▋                                         | 142936/300000 [57:23<34:31, 75.83it/s]
epoch 143000  training loss: 3.4406418800354004
epoch 143000  clean testing loss: 10.674336433410645

 48%|█████████████████████████████████████▋                                         | 143088/300000 [57:26<34:43, 75.30it/s]
epoch 143100  training loss: 3.2755014896392822
epoch 143100  clean testing loss: 10.674354553222656
epoch 143200  training loss: 3.2979748249053955

 48%|█████████████████████████████████████▋                                         | 143240/300000 [57:28<34:46, 75.15it/s]
epoch 143300  training loss: 3.331566095352173

 48%|█████████████████████████████████████▊                                         | 143392/300000 [57:30<34:57, 74.66it/s]
epoch 143400  training loss: 3.3771049976348877
epoch 143400  clean testing loss: 10.675191879272461
epoch 143500  training loss: 3.4966678619384766

 48%|█████████████████████████████████████▊                                         | 143536/300000 [57:31<34:39, 75.25it/s]
epoch 143600  training loss: 3.4782087802886963
epoch 143600  clean testing loss: 10.68850326538086
Validation loss variation < 1e-6, trained to interpolation, stop

 48%|████████████████████████████████████▊                                        | 143600/300000 [57:32<1:02:40, 41.59it/s]