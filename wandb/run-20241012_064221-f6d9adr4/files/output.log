
  1%|          | 1009/100000 [00:01<03:03, 540.26it/s]
epoch 0  training loss: 0.5715824961662292
epoch 0  clean testing loss: 1.3635395765304565
Saving the clean test loss file into log/new_plot/output_dim2_width1024_layers2_relu_size50_noise1.00e-01_invop0_lr0.005 ...
epoch 100  training loss: 0.17270129919052124
epoch 100  clean testing loss: 0.09807629138231277
epoch 200  training loss: 0.12364795058965683
epoch 200  clean testing loss: 0.11538653820753098
epoch 300  training loss: 0.10278251767158508
epoch 300  clean testing loss: 0.15105684101581573
epoch 400  training loss: 0.08844099193811417
epoch 400  clean testing loss: 0.21532543003559113
epoch 500  training loss: 0.06614129990339279
epoch 500  clean testing loss: 0.3383547067642212
epoch 600  training loss: 0.05827959254384041
epoch 600  clean testing loss: 0.5641936659812927
epoch 700  training loss: 0.05335421487689018
epoch 700  clean testing loss: 0.705125093460083
epoch 800  training loss: 0.04402530565857887

  2%|▏         | 2146/100000 [00:04<02:57, 551.68it/s]
epoch 900  training loss: 0.04068009927868843
epoch 900  clean testing loss: 1.0524163246154785
epoch 1000  training loss: 0.042058832943439484
epoch 1000  clean testing loss: 1.7907471656799316
Saving the clean test loss file into log/new_plot/output_dim2_width1024_layers2_relu_size50_noise1.00e-01_invop0_lr0.005 ...
epoch 1100  training loss: 0.03424457088112831
epoch 1100  clean testing loss: 2.66487979888916
epoch 1200  training loss: 0.032659225165843964
epoch 1200  clean testing loss: 3.3816723823547363
epoch 1300  training loss: 0.025353508070111275
epoch 1300  clean testing loss: 3.7317636013031006
epoch 1400  training loss: 0.02357279695570469
epoch 1400  clean testing loss: 3.436901092529297
epoch 1500  training loss: 0.02350008860230446
epoch 1500  clean testing loss: 3.353853702545166
epoch 1600  training loss: 0.021935930475592613
epoch 1600  clean testing loss: 3.760904550552368
epoch 1700  training loss: 0.01958976313471794
epoch 1700  clean testing loss: 3.545351505279541
epoch 1800  training loss: 0.0187221672385931
epoch 1800  clean testing loss: 3.96044659614563
epoch 1900  training loss: 0.02029573917388916

  3%|▎         | 3215/100000 [00:06<02:57, 545.62it/s]
epoch 2000  training loss: 0.01797463931143284
epoch 2000  clean testing loss: 3.3140947818756104
Saving the clean test loss file into log/new_plot/output_dim2_width1024_layers2_relu_size50_noise1.00e-01_invop0_lr0.005 ...
epoch 2100  training loss: 0.019159110262989998
epoch 2100  clean testing loss: 3.0777275562286377
epoch 2200  training loss: 0.019214097410440445
epoch 2200  clean testing loss: 3.6600019931793213
epoch 2300  training loss: 0.02051643282175064
epoch 2300  clean testing loss: 2.9422295093536377
epoch 2400  training loss: 0.018747538328170776
epoch 2400  clean testing loss: 3.1211535930633545
epoch 2500  training loss: 0.01976119913160801
epoch 2500  clean testing loss: 2.6764984130859375
epoch 2600  training loss: 0.018878011032938957
epoch 2600  clean testing loss: 2.873295307159424
epoch 2700  training loss: 0.017171956598758698
epoch 2700  clean testing loss: 3.1666510105133057
epoch 2800  training loss: 0.01650361344218254
epoch 2800  clean testing loss: 3.093738317489624
epoch 2900  training loss: 0.015858538448810577
epoch 2900  clean testing loss: 3.194309949874878
epoch 3000  training loss: 0.017208833247423172
epoch 3000  clean testing loss: 2.912437677383423

  4%|▍         | 4295/100000 [00:07<02:51, 556.55it/s]
epoch 3100  training loss: 0.01594594120979309
epoch 3100  clean testing loss: 3.1130852699279785
epoch 3200  training loss: 0.015131611377000809
epoch 3200  clean testing loss: 3.2136502265930176
epoch 3300  training loss: 0.01669517159461975
epoch 3300  clean testing loss: 2.9950380325317383
epoch 3400  training loss: 0.01626172475516796
epoch 3400  clean testing loss: 2.834484815597534
epoch 3500  training loss: 0.014923212118446827
epoch 3500  clean testing loss: 3.1082866191864014
epoch 3600  training loss: 0.014909552410244942
epoch 3600  clean testing loss: 3.046496629714966
epoch 3700  training loss: 0.01860550418496132
epoch 3700  clean testing loss: 2.8178582191467285
epoch 3800  training loss: 0.06704245507717133
epoch 3800  clean testing loss: 1.601173996925354
epoch 3900  training loss: 0.020074838772416115
epoch 3900  clean testing loss: 3.14971923828125
epoch 4000  training loss: 0.019855115562677383
epoch 4000  clean testing loss: 3.1409082412719727
Saving the clean test loss file into log/new_plot/output_dim2_width1024_layers2_relu_size50_noise1.00e-01_invop0_lr0.005 ...
epoch 4100  training loss: 0.016058681532740593

  5%|▌         | 5427/100000 [00:10<02:50, 553.67it/s]
epoch 4200  training loss: 0.01602749712765217
epoch 4200  clean testing loss: 2.8552534580230713
epoch 4300  training loss: 0.014735746197402477
epoch 4300  clean testing loss: 2.9594290256500244
epoch 4400  training loss: 0.015155592001974583
epoch 4400  clean testing loss: 3.0072131156921387
epoch 4500  training loss: 0.031349122524261475
epoch 4500  clean testing loss: 1.3991680145263672
epoch 4600  training loss: 0.02005145698785782
epoch 4600  clean testing loss: 2.7145726680755615
epoch 4700  training loss: 0.016775798052549362
epoch 4700  clean testing loss: 3.2977054119110107
epoch 4800  training loss: 0.01478558499366045
epoch 4800  clean testing loss: 3.082284450531006
epoch 4900  training loss: 0.017074067145586014
epoch 4900  clean testing loss: 2.806610584259033
epoch 5000  training loss: 0.014824467711150646
epoch 5000  clean testing loss: 2.946270704269409
Saving the clean test loss file into log/new_plot/output_dim2_width1024_layers2_relu_size50_noise1.00e-01_invop0_lr0.005 ...
epoch 5100  training loss: 0.015490326099097729
epoch 5100  clean testing loss: 2.718308687210083
epoch 5200  training loss: 0.01697351597249508

  7%|▋         | 6503/100000 [00:11<02:49, 551.96it/s]
epoch 5300  training loss: 0.01510457880795002
epoch 5300  clean testing loss: 3.0191476345062256
epoch 5400  training loss: 0.015100197866559029
epoch 5400  clean testing loss: 2.7435271739959717
epoch 5500  training loss: 0.01502180565148592
epoch 5500  clean testing loss: 3.031693935394287
epoch 5600  training loss: 0.01470878440886736
epoch 5600  clean testing loss: 2.843381881713867
epoch 5700  training loss: 0.01458179485052824
epoch 5700  clean testing loss: 2.896653890609741
epoch 5800  training loss: 0.015992475673556328
epoch 5800  clean testing loss: 3.189451217651367
epoch 5900  training loss: 0.03948522359132767
epoch 5900  clean testing loss: 1.4893373250961304
epoch 6000  training loss: 0.014654570259153843
epoch 6000  clean testing loss: 2.813824415206909
Saving the clean test loss file into log/new_plot/output_dim2_width1024_layers2_relu_size50_noise1.00e-01_invop0_lr0.005 ...
epoch 6100  training loss: 0.014799275435507298
epoch 6100  clean testing loss: 2.8397068977355957
epoch 6200  training loss: 0.014767282642424107
epoch 6200  clean testing loss: 2.8506646156311035
epoch 6300  training loss: 0.014910702593624592

  8%|▊         | 7633/100000 [00:13<02:46, 553.23it/s]
epoch 6400  training loss: 0.015176293440163136
epoch 6400  clean testing loss: 2.7994935512542725
epoch 6500  training loss: 0.014628889970481396
epoch 6500  clean testing loss: 2.7854857444763184
epoch 6600  training loss: 0.014583716168999672
epoch 6600  clean testing loss: 2.8372631072998047
epoch 6700  training loss: 0.015089591033756733
epoch 6700  clean testing loss: 2.712948799133301
epoch 6800  training loss: 0.01465135533362627
epoch 6800  clean testing loss: 2.8188443183898926
epoch 6900  training loss: 0.014524664729833603
epoch 6900  clean testing loss: 2.8405346870422363
epoch 7000  training loss: 0.05617507919669151
epoch 7000  clean testing loss: 1.129204511642456
Saving the clean test loss file into log/new_plot/output_dim2_width1024_layers2_relu_size50_noise1.00e-01_invop0_lr0.005 ...
epoch 7100  training loss: 0.01664041168987751
epoch 7100  clean testing loss: 3.554363965988159
epoch 7200  training loss: 0.016109636053442955
epoch 7200  clean testing loss: 3.7254295349121094
epoch 7300  training loss: 0.01563359424471855
epoch 7300  clean testing loss: 3.469438314437866
epoch 7400  training loss: 0.015054158866405487

  8%|▊         | 8481/100000 [00:15<02:43, 559.61it/s]
epoch 7500  training loss: 0.015644999220967293
epoch 7500  clean testing loss: 3.461812734603882
epoch 7600  training loss: 0.014927374199032784
epoch 7600  clean testing loss: 3.355982542037964
epoch 7700  training loss: 0.017646601423621178
epoch 7700  clean testing loss: 3.111530303955078
epoch 7800  training loss: 0.014570587314665318
epoch 7800  clean testing loss: 3.284996271133423
epoch 7900  training loss: 0.014728727750480175
epoch 7900  clean testing loss: 3.249375581741333
epoch 8000  training loss: 0.015284445136785507
epoch 8000  clean testing loss: 3.0320637226104736
Saving the clean test loss file into log/new_plot/output_dim2_width1024_layers2_relu_size50_noise1.00e-01_invop0_lr0.005 ...
epoch 8100  training loss: 0.014543252065777779
epoch 8100  clean testing loss: 3.2319369316101074
epoch 8200  training loss: 0.014557687565684319
epoch 8200  clean testing loss: 3.268188714981079
epoch 8300  training loss: 0.01598242111504078
epoch 8300  clean testing loss: 3.4927642345428467
epoch 8400  training loss: 0.014995443634688854
epoch 8400  clean testing loss: 3.0880227088928223
epoch 8500  training loss: 0.015048985369503498

 10%|▉         | 9553/100000 [00:17<02:42, 556.26it/s]
epoch 8600  training loss: 0.014771527610719204
epoch 8600  clean testing loss: 3.190403461456299
epoch 8700  training loss: 0.014717260375618935
epoch 8700  clean testing loss: 3.127641439437866
epoch 8800  training loss: 0.01454351656138897
epoch 8800  clean testing loss: 3.220273971557617
epoch 8900  training loss: 0.014233370311558247
epoch 8900  clean testing loss: 3.13486647605896
epoch 9000  training loss: 0.016305049881339073
epoch 9000  clean testing loss: 2.974376678466797
Saving the clean test loss file into log/new_plot/output_dim2_width1024_layers2_relu_size50_noise1.00e-01_invop0_lr0.005 ...
epoch 9100  training loss: 0.014510384760797024
epoch 9100  clean testing loss: 3.185586452484131
epoch 9200  training loss: 0.01425309106707573
epoch 9200  clean testing loss: 3.2242913246154785
epoch 9300  training loss: 0.014212717302143574
epoch 9300  clean testing loss: 3.0768284797668457
epoch 9400  training loss: 0.014243240468204021
epoch 9400  clean testing loss: 3.115370512008667
epoch 9500  training loss: 0.014447038061916828
epoch 9500  clean testing loss: 3.0056049823760986
epoch 9600  training loss: 0.016332047060132027

 11%|█         | 10686/100000 [00:19<02:39, 561.02it/s]
epoch 9700  training loss: 0.014788155443966389
epoch 9700  clean testing loss: 2.90256404876709
epoch 9800  training loss: 0.014406478963792324
epoch 9800  clean testing loss: 2.852590322494507
epoch 9900  training loss: 0.014503064565360546
epoch 9900  clean testing loss: 3.0163891315460205
epoch 10000  training loss: 0.014455237425863743
epoch 10000  clean testing loss: 2.9458601474761963
Saving the clean test loss file into log/new_plot/output_dim2_width1024_layers2_relu_size50_noise1.00e-01_invop0_lr0.005 ...
epoch 10100  training loss: 0.014100641012191772
epoch 10100  clean testing loss: 3.045130491256714
epoch 10200  training loss: 0.015547950752079487
epoch 10200  clean testing loss: 3.168489933013916
epoch 10300  training loss: 0.015282364562153816
epoch 10300  clean testing loss: 2.598283290863037
epoch 10400  training loss: 0.01416630856692791
epoch 10400  clean testing loss: 2.9938127994537354
epoch 10500  training loss: 0.014424885623157024
epoch 10500  clean testing loss: 3.0661888122558594
epoch 10600  training loss: 0.01446510385721922
epoch 10600  clean testing loss: 2.890489339828491
epoch 10700  training loss: 0.014835294336080551

 12%|█▏        | 11815/100000 [00:21<02:38, 557.33it/s]
epoch 10800  training loss: 0.014272439293563366
epoch 10800  clean testing loss: 3.0125975608825684
epoch 10900  training loss: 0.014151508919894695
epoch 10900  clean testing loss: 2.9533042907714844
epoch 11000  training loss: 0.015031466260552406
epoch 11000  clean testing loss: 2.8301961421966553
Saving the clean test loss file into log/new_plot/output_dim2_width1024_layers2_relu_size50_noise1.00e-01_invop0_lr0.005 ...
epoch 11100  training loss: 0.014070939272642136
epoch 11100  clean testing loss: 2.8439667224884033
epoch 11200  training loss: 0.015008298680186272
epoch 11200  clean testing loss: 2.874021291732788
epoch 11300  training loss: 0.014148742891848087
epoch 11300  clean testing loss: 3.030015707015991
epoch 11400  training loss: 0.014342105016112328
epoch 11400  clean testing loss: 3.0403168201446533
epoch 11500  training loss: 0.014804910868406296
epoch 11500  clean testing loss: 2.965200424194336
epoch 11600  training loss: 0.014222938567399979
epoch 11600  clean testing loss: 2.7828786373138428
epoch 11700  training loss: 0.014287065714597702
epoch 11700  clean testing loss: 2.8700499534606934
epoch 11800  training loss: 0.014552154578268528

 13%|█▎        | 12891/100000 [00:23<02:34, 562.71it/s]
epoch 11900  training loss: 0.015347468666732311
epoch 11900  clean testing loss: 2.971630811691284
epoch 12000  training loss: 0.015927158296108246
epoch 12000  clean testing loss: 2.68686580657959
Saving the clean test loss file into log/new_plot/output_dim2_width1024_layers2_relu_size50_noise1.00e-01_invop0_lr0.005 ...
epoch 12100  training loss: 0.014076260849833488
epoch 12100  clean testing loss: 3.01027774810791
epoch 12200  training loss: 0.014046718366444111
epoch 12200  clean testing loss: 3.0510199069976807
epoch 12300  training loss: 0.014258228242397308
epoch 12300  clean testing loss: 2.980861186981201
epoch 12400  training loss: 0.014182815328240395
epoch 12400  clean testing loss: 2.9968230724334717
epoch 12500  training loss: 0.014058359898626804
epoch 12500  clean testing loss: 3.0871083736419678
epoch 12600  training loss: 0.01447719894349575
epoch 12600  clean testing loss: 2.76454496383667
epoch 12700  training loss: 0.014377612620592117
epoch 12700  clean testing loss: 3.0044474601745605
epoch 12800  training loss: 0.01436552219092846
epoch 12800  clean testing loss: 2.8078067302703857
epoch 12900  training loss: 0.014056455343961716

 14%|█▍        | 13956/100000 [00:25<02:34, 557.15it/s]
epoch 13000  training loss: 0.014091181568801403
epoch 13000  clean testing loss: 2.862532377243042
Saving the clean test loss file into log/new_plot/output_dim2_width1024_layers2_relu_size50_noise1.00e-01_invop0_lr0.005 ...
epoch 13100  training loss: 0.014044014737010002
epoch 13100  clean testing loss: 2.887319803237915
epoch 13200  training loss: 0.019154276698827744
epoch 13200  clean testing loss: 3.1706271171569824
epoch 13300  training loss: 0.014282751828432083
epoch 13300  clean testing loss: 2.7545385360717773
epoch 13400  training loss: 0.014157530851662159
epoch 13400  clean testing loss: 2.895174503326416
epoch 13500  training loss: 0.014114673249423504
epoch 13500  clean testing loss: 2.895050525665283
epoch 13600  training loss: 0.014267184771597385
epoch 13600  clean testing loss: 2.7827861309051514
epoch 13700  training loss: 0.015066144056618214
epoch 13700  clean testing loss: 2.671351909637451
epoch 13800  training loss: 0.014041457325220108
epoch 13800  clean testing loss: 2.767104387283325
epoch 13900  training loss: 0.014241127297282219
epoch 13900  clean testing loss: 2.635333299636841
epoch 14000  training loss: 0.015116047114133835
epoch 14000  clean testing loss: 2.5924010276794434

 15%|█▌        | 15086/100000 [00:27<02:34, 551.24it/s]
epoch 14100  training loss: 0.013981287367641926
epoch 14100  clean testing loss: 2.8679802417755127
epoch 14200  training loss: 0.016209358349442482
epoch 14200  clean testing loss: 2.6156625747680664
epoch 14300  training loss: 0.014013790525496006
epoch 14300  clean testing loss: 2.82106614112854
epoch 14400  training loss: 0.014377406798303127
epoch 14400  clean testing loss: 2.773390054702759
epoch 14500  training loss: 0.014192625880241394
epoch 14500  clean testing loss: 2.707700252532959
epoch 14600  training loss: 0.014076816849410534
epoch 14600  clean testing loss: 2.753591537475586
epoch 14700  training loss: 0.014086442068219185
epoch 14700  clean testing loss: 2.854418992996216
epoch 14800  training loss: 0.01410578191280365
epoch 14800  clean testing loss: 2.677544593811035
epoch 14900  training loss: 0.014070327393710613
epoch 14900  clean testing loss: 2.796746253967285
epoch 15000  training loss: 0.014207548461854458
epoch 15000  clean testing loss: 2.6517555713653564
Saving the clean test loss file into log/new_plot/output_dim2_width1024_layers2_relu_size50_noise1.00e-01_invop0_lr0.005 ...
epoch 15100  training loss: 0.013957425951957703

 16%|█▌        | 16211/100000 [00:29<02:32, 549.48it/s]
epoch 15200  training loss: 0.014039539732038975
epoch 15200  clean testing loss: 2.7535400390625
epoch 15300  training loss: 0.014025270938873291
epoch 15300  clean testing loss: 2.769827127456665
epoch 15400  training loss: 0.014199110679328442
epoch 15400  clean testing loss: 2.8065054416656494
epoch 15500  training loss: 0.014017920941114426
epoch 15500  clean testing loss: 2.7239434719085693
epoch 15600  training loss: 0.01399517897516489
epoch 15600  clean testing loss: 2.781053304672241
epoch 15700  training loss: 0.014052793383598328
epoch 15700  clean testing loss: 2.720576047897339
epoch 15800  training loss: 0.01401968952268362
epoch 15800  clean testing loss: 2.7270009517669678
epoch 15900  training loss: 0.014078838750720024
epoch 15900  clean testing loss: 2.6368439197540283
epoch 16000  training loss: 0.014047350734472275
epoch 16000  clean testing loss: 2.776427745819092
Saving the clean test loss file into log/new_plot/output_dim2_width1024_layers2_relu_size50_noise1.00e-01_invop0_lr0.005 ...
epoch 16100  training loss: 0.01407322846353054
epoch 16100  clean testing loss: 2.6474549770355225
epoch 16200  training loss: 0.014006458222866058

 17%|█▋        | 17286/100000 [00:31<02:27, 559.33it/s]
epoch 16300  training loss: 0.01408777292817831
epoch 16300  clean testing loss: 2.697131633758545
epoch 16400  training loss: 0.014113158918917179
epoch 16400  clean testing loss: 2.678988218307495
epoch 16500  training loss: 0.014623463153839111
epoch 16500  clean testing loss: 2.761157751083374
epoch 16600  training loss: 0.014024683274328709
epoch 16600  clean testing loss: 2.6718547344207764
epoch 16700  training loss: 0.014023520983755589
epoch 16700  clean testing loss: 2.68265438079834
epoch 16800  training loss: 0.0141385393217206
epoch 16800  clean testing loss: 2.5738561153411865
epoch 16900  training loss: 0.015932278707623482
epoch 16900  clean testing loss: 2.654305934906006
epoch 17000  training loss: 0.015165623277425766
epoch 17000  clean testing loss: 2.755831241607666
Saving the clean test loss file into log/new_plot/output_dim2_width1024_layers2_relu_size50_noise1.00e-01_invop0_lr0.005 ...
epoch 17100  training loss: 0.013986830599606037
epoch 17100  clean testing loss: 2.611536741256714
epoch 17200  training loss: 0.015020872466266155
epoch 17200  clean testing loss: 2.668877601623535
epoch 17300  training loss: 0.014035176485776901

 18%|█▊        | 18414/100000 [00:33<02:26, 555.33it/s]
epoch 17400  training loss: 0.01445628609508276
epoch 17400  clean testing loss: 2.5241963863372803
epoch 17500  training loss: 0.01401841826736927
epoch 17500  clean testing loss: 2.651273250579834
epoch 17600  training loss: 0.01502272579818964
epoch 17600  clean testing loss: 2.5133800506591797
epoch 17700  training loss: 0.014017748646438122
epoch 17700  clean testing loss: 2.7152395248413086
epoch 17800  training loss: 0.014067189767956734
epoch 17800  clean testing loss: 2.7036192417144775
epoch 17900  training loss: 0.014210519380867481
epoch 17900  clean testing loss: 2.5577266216278076
epoch 18000  training loss: 0.0139566445723176
epoch 18000  clean testing loss: 2.608306407928467
Saving the clean test loss file into log/new_plot/output_dim2_width1024_layers2_relu_size50_noise1.00e-01_invop0_lr0.005 ...
epoch 18100  training loss: 0.01398555003106594
epoch 18100  clean testing loss: 2.6390297412872314
epoch 18200  training loss: 0.01395382359623909
epoch 18200  clean testing loss: 2.686234712600708
epoch 18300  training loss: 0.0140204643830657
epoch 18300  clean testing loss: 2.6303069591522217
epoch 18400  training loss: 0.013918574899435043

 19%|█▉        | 19491/100000 [00:35<02:23, 560.11it/s]
epoch 18500  training loss: 0.013945125974714756
epoch 18500  clean testing loss: 2.6786117553710938
epoch 18600  training loss: 0.013960571028292179
epoch 18600  clean testing loss: 2.6467010974884033
epoch 18700  training loss: 0.014050215482711792
epoch 18700  clean testing loss: 2.630063533782959
epoch 18800  training loss: 0.014058159664273262
epoch 18800  clean testing loss: 2.644537925720215
epoch 18900  training loss: 0.014064903371036053
epoch 18900  clean testing loss: 2.571335792541504
epoch 19000  training loss: 0.013926285319030285
epoch 19000  clean testing loss: 2.6727867126464844
Saving the clean test loss file into log/new_plot/output_dim2_width1024_layers2_relu_size50_noise1.00e-01_invop0_lr0.005 ...
epoch 19100  training loss: 0.013931751251220703
epoch 19100  clean testing loss: 2.6883459091186523
epoch 19200  training loss: 0.01409069262444973
epoch 19200  clean testing loss: 2.643911600112915
epoch 19300  training loss: 0.013910378329455853
epoch 19300  clean testing loss: 2.678685188293457
epoch 19400  training loss: 0.013935928232967854
epoch 19400  clean testing loss: 2.645242929458618
epoch 19500  training loss: 0.014024872332811356

 21%|██        | 20620/100000 [00:37<02:22, 556.80it/s]
epoch 19600  training loss: 0.013923314400017262
epoch 19600  clean testing loss: 2.7020223140716553
epoch 19700  training loss: 0.013962659984827042
epoch 19700  clean testing loss: 2.5560662746429443
epoch 19800  training loss: 0.014352475292980671
epoch 19800  clean testing loss: 2.7197682857513428
epoch 19900  training loss: 0.014160217717289925
epoch 19900  clean testing loss: 2.732461452484131
epoch 20000  training loss: 0.013896026648581028
epoch 20000  clean testing loss: 2.699195146560669
Saving the clean test loss file into log/new_plot/output_dim2_width1024_layers2_relu_size50_noise1.00e-01_invop0_lr0.005 ...
epoch 20100  training loss: 0.015423759818077087
epoch 20100  clean testing loss: 2.306061029434204
epoch 20200  training loss: 0.013962109573185444
epoch 20200  clean testing loss: 2.6483843326568604
epoch 20300  training loss: 0.013878234662115574
epoch 20300  clean testing loss: 2.6862246990203857
epoch 20400  training loss: 0.013922201469540596
epoch 20400  clean testing loss: 2.701627492904663
epoch 20500  training loss: 0.014026262797415257
epoch 20500  clean testing loss: 2.7164833545684814
epoch 20600  training loss: 0.013887903653085232
epoch 20600  clean testing loss: 2.62447452545166
epoch 20700  training loss: 0.013925265520811081

 22%|██▏       | 21753/100000 [00:39<02:20, 557.45it/s]
epoch 20800  training loss: 0.013928580097854137
epoch 20800  clean testing loss: 2.650556802749634
epoch 20900  training loss: 0.014492513611912727
epoch 20900  clean testing loss: 2.630981683731079
epoch 21000  training loss: 0.0139010613784194
epoch 21000  clean testing loss: 2.6550133228302
Saving the clean test loss file into log/new_plot/output_dim2_width1024_layers2_relu_size50_noise1.00e-01_invop0_lr0.005 ...
epoch 21100  training loss: 0.013873079791665077
epoch 21100  clean testing loss: 2.6585826873779297
epoch 21200  training loss: 0.013889940455555916
epoch 21200  clean testing loss: 2.6807446479797363
epoch 21300  training loss: 0.013913015834987164
epoch 21300  clean testing loss: 2.678927183151245
epoch 21400  training loss: 0.013894355855882168
epoch 21400  clean testing loss: 2.6775341033935547
epoch 21500  training loss: 0.01394468080252409
epoch 21500  clean testing loss: 2.6502091884613037
epoch 21600  training loss: 0.013900616206228733
epoch 21600  clean testing loss: 2.716402769088745
epoch 21700  training loss: 0.013940928503870964
epoch 21700  clean testing loss: 2.5839297771453857
epoch 21800  training loss: 0.013958459720015526

 23%|██▎       | 22823/100000 [00:41<02:19, 554.37it/s]
epoch 21900  training loss: 0.014024438336491585
epoch 21900  clean testing loss: 2.6707262992858887
epoch 22000  training loss: 0.013873438350856304
epoch 22000  clean testing loss: 2.7457995414733887
Saving the clean test loss file into log/new_plot/output_dim2_width1024_layers2_relu_size50_noise1.00e-01_invop0_lr0.005 ...
epoch 22100  training loss: 0.013952277600765228
epoch 22100  clean testing loss: 2.762176513671875
epoch 22200  training loss: 0.013832442462444305
epoch 22200  clean testing loss: 2.708374500274658
epoch 22300  training loss: 0.01421529334038496
epoch 22300  clean testing loss: 2.69411039352417
epoch 22400  training loss: 0.013890673406422138
epoch 22400  clean testing loss: 2.6563708782196045
epoch 22500  training loss: 0.013924404047429562
epoch 22500  clean testing loss: 2.7271623611450195
epoch 22600  training loss: 0.013910391367971897
epoch 22600  clean testing loss: 2.714747667312622
epoch 22700  training loss: 0.013875256292521954
epoch 22700  clean testing loss: 2.720587730407715
epoch 22800  training loss: 0.013957459479570389
epoch 22800  clean testing loss: 2.788954734802246
epoch 22900  training loss: 0.013920588418841362

 24%|██▍       | 23952/100000 [00:43<02:16, 555.21it/s]
epoch 23000  training loss: 0.013858484104275703
epoch 23000  clean testing loss: 2.7278099060058594
Saving the clean test loss file into log/new_plot/output_dim2_width1024_layers2_relu_size50_noise1.00e-01_invop0_lr0.005 ...
epoch 23100  training loss: 0.013847006484866142
epoch 23100  clean testing loss: 2.72563099861145
epoch 23200  training loss: 0.013877972029149532
epoch 23200  clean testing loss: 2.7080540657043457
epoch 23300  training loss: 0.014030948281288147
epoch 23300  clean testing loss: 2.6411526203155518
epoch 23400  training loss: 0.013901039958000183
epoch 23400  clean testing loss: 2.71437668800354
epoch 23500  training loss: 0.013948173262178898
epoch 23500  clean testing loss: 2.714995861053467
epoch 23600  training loss: 0.013864483684301376
epoch 23600  clean testing loss: 2.744755268096924
epoch 23700  training loss: 0.014090616255998611
epoch 23700  clean testing loss: 2.5959277153015137
epoch 23800  training loss: 0.014135369099676609
epoch 23800  clean testing loss: 2.765329599380493
epoch 23900  training loss: 0.013841716572642326
epoch 23900  clean testing loss: 2.7416493892669678
epoch 24000  training loss: 0.014324569143354893
epoch 24000  clean testing loss: 2.742417812347412

 25%|██▌       | 25025/100000 [00:45<02:17, 547.16it/s]
epoch 24100  training loss: 0.013825305737555027
epoch 24100  clean testing loss: 2.754953145980835
epoch 24200  training loss: 0.01382703147828579
epoch 24200  clean testing loss: 2.759061813354492
epoch 24300  training loss: 0.013835053890943527
epoch 24300  clean testing loss: 2.7610888481140137
epoch 24400  training loss: 0.013909990899264812
epoch 24400  clean testing loss: 2.7329113483428955
epoch 24500  training loss: 0.013840138912200928
epoch 24500  clean testing loss: 2.739604949951172
epoch 24600  training loss: 0.013878654688596725
epoch 24600  clean testing loss: 2.7381982803344727
epoch 24700  training loss: 0.013952286913990974
epoch 24700  clean testing loss: 2.6896541118621826
epoch 24800  training loss: 0.013869681395590305
epoch 24800  clean testing loss: 2.76526141166687
epoch 24900  training loss: 0.013902361504733562
epoch 24900  clean testing loss: 2.772817850112915
epoch 25000  training loss: 0.013900207355618477
epoch 25000  clean testing loss: 2.749878406524658
Saving the clean test loss file into log/new_plot/output_dim2_width1024_layers2_relu_size50_noise1.00e-01_invop0_lr0.005 ...
epoch 25100  training loss: 0.013874711468815804

 26%|██▌       | 26158/100000 [00:47<02:12, 555.53it/s]
epoch 25200  training loss: 0.01403032522648573
epoch 25200  clean testing loss: 2.8165948390960693
epoch 25300  training loss: 0.013858331367373466
epoch 25300  clean testing loss: 2.7626869678497314
epoch 25400  training loss: 0.013849583454430103
epoch 25400  clean testing loss: 2.7585527896881104
epoch 25500  training loss: 0.01384897530078888
epoch 25500  clean testing loss: 2.739706039428711
epoch 25600  training loss: 0.013837342150509357
epoch 25600  clean testing loss: 2.7965831756591797
epoch 25700  training loss: 0.01392770279198885
epoch 25700  clean testing loss: 2.7972607612609863
epoch 25800  training loss: 0.013884980231523514
epoch 25800  clean testing loss: 2.823486804962158
epoch 25900  training loss: 0.014026411809027195
epoch 25900  clean testing loss: 2.757889986038208
epoch 26000  training loss: 0.013814813457429409
epoch 26000  clean testing loss: 2.8076422214508057
Saving the clean test loss file into log/new_plot/output_dim2_width1024_layers2_relu_size50_noise1.00e-01_invop0_lr0.005 ...
epoch 26100  training loss: 0.013865957036614418
epoch 26100  clean testing loss: 2.8405652046203613
epoch 26200  training loss: 0.013819077983498573

 27%|██▋       | 27289/100000 [00:49<02:10, 558.81it/s]
epoch 26300  training loss: 0.014298214577138424
epoch 26300  clean testing loss: 2.9078590869903564
epoch 26400  training loss: 0.013804657384753227
epoch 26400  clean testing loss: 2.793186664581299
epoch 26500  training loss: 0.01386119332164526
epoch 26500  clean testing loss: 2.804760456085205
epoch 26600  training loss: 0.013842107728123665
epoch 26600  clean testing loss: 2.7755415439605713
epoch 26700  training loss: 0.013827972114086151
epoch 26700  clean testing loss: 2.782621145248413
epoch 26800  training loss: 0.01383127924054861
epoch 26800  clean testing loss: 2.81786847114563
epoch 26900  training loss: 0.014099664054811
epoch 26900  clean testing loss: 2.7595794200897217
epoch 27000  training loss: 0.013795750215649605
epoch 27000  clean testing loss: 2.846000909805298
Saving the clean test loss file into log/new_plot/output_dim2_width1024_layers2_relu_size50_noise1.00e-01_invop0_lr0.005 ...
epoch 27100  training loss: 0.013794388622045517
epoch 27100  clean testing loss: 2.818215847015381
epoch 27200  training loss: 0.01379508338868618
epoch 27200  clean testing loss: 2.8334667682647705
epoch 27300  training loss: 0.013786792755126953

 28%|██▊       | 28366/100000 [00:51<02:08, 559.49it/s]
epoch 27400  training loss: 0.013817200437188148
epoch 27400  clean testing loss: 2.8407821655273438
epoch 27500  training loss: 0.013795394450426102
epoch 27500  clean testing loss: 2.8418033123016357
epoch 27600  training loss: 0.013779405504465103
epoch 27600  clean testing loss: 2.8384249210357666
epoch 27700  training loss: 0.013795671984553337
epoch 27700  clean testing loss: 2.8355119228363037
epoch 27800  training loss: 0.01382214855402708
epoch 27800  clean testing loss: 2.7990849018096924
epoch 27900  training loss: 0.013777312822639942
epoch 27900  clean testing loss: 2.8450541496276855
epoch 28000  training loss: 0.013790616765618324
epoch 28000  clean testing loss: 2.867330312728882
Saving the clean test loss file into log/new_plot/output_dim2_width1024_layers2_relu_size50_noise1.00e-01_invop0_lr0.005 ...
epoch 28100  training loss: 0.013825005851686
epoch 28100  clean testing loss: 2.9028561115264893
epoch 28200  training loss: 0.013853846117854118
epoch 28200  clean testing loss: 2.8740832805633545
epoch 28300  training loss: 0.013797723688185215
epoch 28300  clean testing loss: 2.8788182735443115
epoch 28400  training loss: 0.013795346021652222

 29%|██▉       | 29495/100000 [00:53<02:05, 560.18it/s]
epoch 28500  training loss: 0.013862892985343933
epoch 28500  clean testing loss: 2.922515392303467
epoch 28600  training loss: 0.013777386397123337
epoch 28600  clean testing loss: 2.878037691116333
epoch 28700  training loss: 0.013780350796878338
epoch 28700  clean testing loss: 2.8927361965179443
epoch 28800  training loss: 0.013802547007799149
epoch 28800  clean testing loss: 2.9167091846466064
epoch 28900  training loss: 0.013786034658551216
epoch 28900  clean testing loss: 2.936131715774536
epoch 29000  training loss: 0.013829329051077366
epoch 29000  clean testing loss: 2.8590118885040283
Saving the clean test loss file into log/new_plot/output_dim2_width1024_layers2_relu_size50_noise1.00e-01_invop0_lr0.005 ...
epoch 29100  training loss: 0.013776611536741257
epoch 29100  clean testing loss: 2.8913140296936035
epoch 29200  training loss: 0.013830402866005898
epoch 29200  clean testing loss: 2.888942241668701
epoch 29300  training loss: 0.013772442936897278
epoch 29300  clean testing loss: 2.8771941661834717
epoch 29400  training loss: 0.013757984153926373
epoch 29400  clean testing loss: 2.9047319889068604
epoch 29500  training loss: 0.01379356812685728

 31%|███       | 30560/100000 [00:55<02:04, 557.60it/s]
epoch 29600  training loss: 0.0138481967151165
epoch 29600  clean testing loss: 2.8933558464050293
epoch 29700  training loss: 0.013810500502586365
epoch 29700  clean testing loss: 2.879408597946167
epoch 29800  training loss: 0.013788428157567978
epoch 29800  clean testing loss: 2.894819974899292
epoch 29900  training loss: 0.014042424969375134
epoch 29900  clean testing loss: 2.963397264480591
epoch 30000  training loss: 0.0139069939032197
epoch 30000  clean testing loss: 2.881995677947998
Saving the clean test loss file into log/new_plot/output_dim2_width1024_layers2_relu_size50_noise1.00e-01_invop0_lr0.005 ...
epoch 30100  training loss: 0.013759746216237545
epoch 30100  clean testing loss: 2.924849271774292
epoch 30200  training loss: 0.013785148970782757
epoch 30200  clean testing loss: 2.8991851806640625
epoch 30300  training loss: 0.01376633532345295
epoch 30300  clean testing loss: 2.9566915035247803
epoch 30400  training loss: 0.013768353499472141
epoch 30400  clean testing loss: 2.962996244430542
epoch 30500  training loss: 0.013759656809270382
epoch 30500  clean testing loss: 2.961765766143799
epoch 30600  training loss: 0.0137687548995018

 32%|███▏      | 31689/100000 [00:57<02:03, 553.54it/s]
epoch 30700  training loss: 0.01380652841180563
epoch 30700  clean testing loss: 2.8991353511810303
epoch 30800  training loss: 0.01377282477915287
epoch 30800  clean testing loss: 2.944512367248535
epoch 30900  training loss: 0.013743617571890354
epoch 30900  clean testing loss: 2.969257354736328
epoch 31000  training loss: 0.013761179521679878
epoch 31000  clean testing loss: 2.9660308361053467
Saving the clean test loss file into log/new_plot/output_dim2_width1024_layers2_relu_size50_noise1.00e-01_invop0_lr0.005 ...
epoch 31100  training loss: 0.013753579929471016
epoch 31100  clean testing loss: 2.9831314086914062
epoch 31200  training loss: 0.01374298520386219
epoch 31200  clean testing loss: 2.9837841987609863
epoch 31300  training loss: 0.01374159287661314
epoch 31300  clean testing loss: 2.9730987548828125
epoch 31400  training loss: 0.013736356049776077
epoch 31400  clean testing loss: 2.9729528427124023
epoch 31500  training loss: 0.013755034655332565
epoch 31500  clean testing loss: 2.9854588508605957
epoch 31600  training loss: 0.013756871223449707
epoch 31600  clean testing loss: 2.9751884937286377
epoch 31700  training loss: 0.01372691709548235
 32%|███▏      | 32026/100000 [00:58<02:06, 535.86it/s]wandb: 429 encountered (Filestream rate limit exceeded, retrying in 2.1 seconds.), retrying request
 33%|███▎      | 32759/100000 [00:59<02:00, 559.04it/s]
epoch 31800  training loss: 0.01376499142497778
epoch 31800  clean testing loss: 3.009155750274658
epoch 31900  training loss: 0.013745765201747417
epoch 31900  clean testing loss: 2.9785492420196533
epoch 32000  training loss: 0.013749768026173115
epoch 32000  clean testing loss: 2.9986085891723633
Saving the clean test loss file into log/new_plot/output_dim2_width1024_layers2_relu_size50_noise1.00e-01_invop0_lr0.005 ...
epoch 32100  training loss: 0.013771955855190754
epoch 32100  clean testing loss: 3.020766258239746
epoch 32200  training loss: 0.013760575093328953
epoch 32200  clean testing loss: 2.996541738510132
epoch 32300  training loss: 0.013742457143962383
epoch 32300  clean testing loss: 3.0254838466644287
epoch 32400  training loss: 0.013739749789237976
epoch 32400  clean testing loss: 3.012272357940674
epoch 32500  training loss: 0.013749768026173115
epoch 32500  clean testing loss: 3.026134967803955
epoch 32600  training loss: 0.013746960088610649
epoch 32600  clean testing loss: 2.988154888153076
epoch 32700  training loss: 0.013756519183516502
epoch 32700  clean testing loss: 3.0297210216522217
epoch 32800  training loss: 0.0137345464900136

 34%|███▍      | 33892/100000 [01:01<01:57, 560.33it/s]
epoch 32900  training loss: 0.013751381076872349
epoch 32900  clean testing loss: 2.998013734817505
epoch 33000  training loss: 0.013738109730184078
epoch 33000  clean testing loss: 3.0560646057128906
Saving the clean test loss file into log/new_plot/output_dim2_width1024_layers2_relu_size50_noise1.00e-01_invop0_lr0.005 ...
epoch 33100  training loss: 0.013720559887588024
epoch 33100  clean testing loss: 3.0371291637420654
epoch 33200  training loss: 0.01372821256518364
epoch 33200  clean testing loss: 3.0274498462677
epoch 33300  training loss: 0.013738651759922504
epoch 33300  clean testing loss: 3.0437347888946533
epoch 33400  training loss: 0.013731948100030422
epoch 33400  clean testing loss: 3.0608465671539307
epoch 33500  training loss: 0.013722740113735199
epoch 33500  clean testing loss: 3.057572603225708
epoch 33600  training loss: 0.013741444796323776
epoch 33600  clean testing loss: 3.0442399978637695
epoch 33700  training loss: 0.013794132508337498
epoch 33700  clean testing loss: 3.043933629989624
epoch 33800  training loss: 0.013741806149482727
epoch 33800  clean testing loss: 3.0560142993927
epoch 33900  training loss: 0.013739165849983692

 35%|███▍      | 34967/100000 [01:03<01:56, 559.10it/s]
epoch 34000  training loss: 0.01372221764177084
epoch 34000  clean testing loss: 3.0607752799987793
Saving the clean test loss file into log/new_plot/output_dim2_width1024_layers2_relu_size50_noise1.00e-01_invop0_lr0.005 ...
epoch 34100  training loss: 0.013712228275835514
epoch 34100  clean testing loss: 3.086662530899048
epoch 34200  training loss: 0.013809828087687492
epoch 34200  clean testing loss: 3.1050877571105957
epoch 34300  training loss: 0.013720674440264702
epoch 34300  clean testing loss: 3.09028959274292
epoch 34400  training loss: 0.01373611856251955
epoch 34400  clean testing loss: 3.120614528656006
epoch 34500  training loss: 0.013735287822782993
epoch 34500  clean testing loss: 3.0798351764678955
epoch 34600  training loss: 0.013736416585743427
epoch 34600  clean testing loss: 3.114647388458252
epoch 34700  training loss: 0.013710841536521912
epoch 34700  clean testing loss: 3.1072404384613037
epoch 34800  training loss: 0.01375065091997385
epoch 34800  clean testing loss: 3.0825092792510986
epoch 34900  training loss: 0.013728520832955837
epoch 34900  clean testing loss: 3.123279333114624
epoch 35000  training loss: 0.013710648752748966
epoch 35000  clean testing loss: 3.119227170944214

 36%|███▌      | 36100/100000 [01:05<01:55, 555.41it/s]
epoch 35100  training loss: 0.013710524886846542
epoch 35100  clean testing loss: 3.1112236976623535
epoch 35200  training loss: 0.013736645691096783
epoch 35200  clean testing loss: 3.107616424560547
epoch 35300  training loss: 0.013715705834329128
epoch 35300  clean testing loss: 3.106663227081299
epoch 35400  training loss: 0.013716497458517551
epoch 35400  clean testing loss: 3.1154489517211914
epoch 35500  training loss: 0.01372593268752098
epoch 35500  clean testing loss: 3.1143743991851807
epoch 35600  training loss: 0.013704024255275726
epoch 35600  clean testing loss: 3.1555769443511963
epoch 35700  training loss: 0.013793452642858028
epoch 35700  clean testing loss: 3.173166036605835
epoch 35800  training loss: 0.013736847788095474
epoch 35800  clean testing loss: 3.15704083442688
epoch 35900  training loss: 0.013724382035434246
epoch 35900  clean testing loss: 3.1551594734191895
epoch 36000  training loss: 0.013704088516533375
epoch 36000  clean testing loss: 3.1539549827575684
Saving the clean test loss file into log/new_plot/output_dim2_width1024_layers2_relu_size50_noise1.00e-01_invop0_lr0.005 ...
epoch 36100  training loss: 0.013689439743757248

 37%|███▋      | 37228/100000 [01:07<01:53, 553.29it/s]
epoch 36200  training loss: 0.013704652898013592
epoch 36200  clean testing loss: 3.1837854385375977
epoch 36300  training loss: 0.013710672967135906
epoch 36300  clean testing loss: 3.1828112602233887
epoch 36400  training loss: 0.013689879328012466
epoch 36400  clean testing loss: 3.1784074306488037
epoch 36500  training loss: 0.013703844510018826
epoch 36500  clean testing loss: 3.2071263790130615
epoch 36600  training loss: 0.013691809959709644
epoch 36600  clean testing loss: 3.197127342224121
epoch 36700  training loss: 0.013702615164220333
epoch 36700  clean testing loss: 3.1907026767730713
epoch 36800  training loss: 0.013694129884243011
epoch 36800  clean testing loss: 3.1898889541625977
epoch 36900  training loss: 0.013692973181605339
epoch 36900  clean testing loss: 3.203583002090454
epoch 37000  training loss: 0.01370655745267868
epoch 37000  clean testing loss: 3.1900711059570312
Saving the clean test loss file into log/new_plot/output_dim2_width1024_layers2_relu_size50_noise1.00e-01_invop0_lr0.005 ...
epoch 37100  training loss: 0.013693840242922306
epoch 37100  clean testing loss: 3.2051002979278564
epoch 37200  training loss: 0.013699732720851898

 38%|███▊      | 38301/100000 [01:09<01:51, 553.55it/s]
epoch 37300  training loss: 0.013701770454645157
epoch 37300  clean testing loss: 3.2147881984710693
epoch 37400  training loss: 0.013698132708668709
epoch 37400  clean testing loss: 3.237147092819214
epoch 37500  training loss: 0.013702892698347569
epoch 37500  clean testing loss: 3.2286217212677
epoch 37600  training loss: 0.013712555170059204
epoch 37600  clean testing loss: 3.213137626647949
epoch 37700  training loss: 0.01370038278400898
epoch 37700  clean testing loss: 3.251011371612549
epoch 37800  training loss: 0.01372857391834259
epoch 37800  clean testing loss: 3.2076237201690674
epoch 37900  training loss: 0.013693762943148613
epoch 37900  clean testing loss: 3.235868453979492
epoch 38000  training loss: 0.013677917420864105
epoch 38000  clean testing loss: 3.2457573413848877
Saving the clean test loss file into log/new_plot/output_dim2_width1024_layers2_relu_size50_noise1.00e-01_invop0_lr0.005 ...
epoch 38100  training loss: 0.013703313656151295
epoch 38100  clean testing loss: 3.2681548595428467
epoch 38200  training loss: 0.013696106150746346
epoch 38200  clean testing loss: 3.250307559967041
epoch 38300  training loss: 0.013690385967493057

 39%|███▉      | 39430/100000 [01:11<01:48, 556.29it/s]
epoch 38400  training loss: 0.013704372569918633
epoch 38400  clean testing loss: 3.2359066009521484
epoch 38500  training loss: 0.013683540746569633
epoch 38500  clean testing loss: 3.248230457305908
epoch 38600  training loss: 0.013706996105611324
epoch 38600  clean testing loss: 3.2417616844177246
epoch 38700  training loss: 0.013672505505383015
epoch 38700  clean testing loss: 3.2742912769317627
epoch 38800  training loss: 0.01369447447359562
epoch 38800  clean testing loss: 3.278994083404541
epoch 38900  training loss: 0.01370303612202406
epoch 38900  clean testing loss: 3.311779499053955
epoch 39000  training loss: 0.013683440163731575
epoch 39000  clean testing loss: 3.289583444595337
Saving the clean test loss file into log/new_plot/output_dim2_width1024_layers2_relu_size50_noise1.00e-01_invop0_lr0.005 ...
epoch 39100  training loss: 0.013671353459358215
epoch 39100  clean testing loss: 3.286203145980835
epoch 39200  training loss: 0.013670394197106361
epoch 39200  clean testing loss: 3.2825329303741455
epoch 39300  training loss: 0.013674907386302948
epoch 39300  clean testing loss: 3.294266700744629
epoch 39400  training loss: 0.013681911863386631
epoch 39400  clean testing loss: 3.306828737258911
epoch 39500  training loss: 0.013670732267200947

 41%|████      | 40503/100000 [01:13<01:47, 552.53it/s]
epoch 39600  training loss: 0.013666551560163498
epoch 39600  clean testing loss: 3.3185927867889404
epoch 39700  training loss: 0.01368268858641386
epoch 39700  clean testing loss: 3.3244247436523438
epoch 39800  training loss: 0.013667487539350986
epoch 39800  clean testing loss: 3.2970519065856934
epoch 39900  training loss: 0.013662902638316154
epoch 39900  clean testing loss: 3.3178393840789795
epoch 40000  training loss: 0.013676881790161133
epoch 40000  clean testing loss: 3.307788848876953
Saving the clean test loss file into log/new_plot/output_dim2_width1024_layers2_relu_size50_noise1.00e-01_invop0_lr0.005 ...
epoch 40100  training loss: 0.013684921897947788
epoch 40100  clean testing loss: 3.300469398498535
epoch 40200  training loss: 0.013661831617355347
epoch 40200  clean testing loss: 3.329799175262451
epoch 40300  training loss: 0.01367422565817833
epoch 40300  clean testing loss: 3.346985101699829
epoch 40400  training loss: 0.013664161786437035
epoch 40400  clean testing loss: 3.336946964263916
epoch 40500  training loss: 0.013672919012606144
epoch 40500  clean testing loss: 3.327000617980957
epoch 40600  training loss: 0.013664067722856998

 42%|████▏     | 41637/100000 [01:15<01:44, 558.16it/s]
epoch 40700  training loss: 0.013660728931427002
epoch 40700  clean testing loss: 3.353424549102783
epoch 40800  training loss: 0.013657859526574612
epoch 40800  clean testing loss: 3.351091146469116
epoch 40900  training loss: 0.013664960861206055
epoch 40900  clean testing loss: 3.336686849594116
epoch 41000  training loss: 0.013667730614542961
epoch 41000  clean testing loss: 3.346694231033325
Saving the clean test loss file into log/new_plot/output_dim2_width1024_layers2_relu_size50_noise1.00e-01_invop0_lr0.005 ...
epoch 41100  training loss: 0.01365755032747984
epoch 41100  clean testing loss: 3.3727757930755615
epoch 41200  training loss: 0.013666081242263317
epoch 41200  clean testing loss: 3.3474626541137695
epoch 41300  training loss: 0.013659619726240635
epoch 41300  clean testing loss: 3.3479082584381104
epoch 41400  training loss: 0.013683545403182507
epoch 41400  clean testing loss: 3.3637545108795166
epoch 41500  training loss: 0.013655237853527069
epoch 41500  clean testing loss: 3.3739492893218994
epoch 41600  training loss: 0.013697492890059948
epoch 41600  clean testing loss: 3.3518505096435547
epoch 41700  training loss: 0.013644427061080933

 43%|████▎     | 42772/100000 [01:17<01:42, 560.89it/s]
epoch 41800  training loss: 0.01366488914936781
epoch 41800  clean testing loss: 3.3820250034332275
epoch 41900  training loss: 0.013656776398420334
epoch 41900  clean testing loss: 3.3656442165374756
epoch 42000  training loss: 0.013650655746459961
epoch 42000  clean testing loss: 3.3950984477996826
Saving the clean test loss file into log/new_plot/output_dim2_width1024_layers2_relu_size50_noise1.00e-01_invop0_lr0.005 ...
epoch 42100  training loss: 0.013644706457853317
epoch 42100  clean testing loss: 3.379007577896118
epoch 42200  training loss: 0.013646695762872696
epoch 42200  clean testing loss: 3.3707733154296875
epoch 42300  training loss: 0.013643087819218636
epoch 42300  clean testing loss: 3.392287015914917
epoch 42400  training loss: 0.013646355830132961
epoch 42400  clean testing loss: 3.3832316398620605
epoch 42500  training loss: 0.013655015267431736
epoch 42500  clean testing loss: 3.4111413955688477
epoch 42600  training loss: 0.013643579557538033
epoch 42600  clean testing loss: 3.379488706588745
epoch 42700  training loss: 0.013647071085870266
epoch 42700  clean testing loss: 3.388392210006714
epoch 42800  training loss: 0.013646646402776241

 44%|████▍     | 43847/100000 [01:19<01:40, 556.30it/s]
epoch 42900  training loss: 0.01365567184984684
epoch 42900  clean testing loss: 3.402330160140991
epoch 43000  training loss: 0.013643942773342133
epoch 43000  clean testing loss: 3.407543182373047
Saving the clean test loss file into log/new_plot/output_dim2_width1024_layers2_relu_size50_noise1.00e-01_invop0_lr0.005 ...
epoch 43100  training loss: 0.01363586075603962
epoch 43100  clean testing loss: 3.417163610458374
epoch 43200  training loss: 0.013638527132570744
epoch 43200  clean testing loss: 3.41512393951416
epoch 43300  training loss: 0.01363819558173418
epoch 43300  clean testing loss: 3.3972058296203613
epoch 43400  training loss: 0.013634090311825275
epoch 43400  clean testing loss: 3.3944926261901855
epoch 43500  training loss: 0.013637573458254337
epoch 43500  clean testing loss: 3.4077258110046387
epoch 43600  training loss: 0.013634788803756237
epoch 43600  clean testing loss: 3.4082820415496826
epoch 43700  training loss: 0.013650891371071339
epoch 43700  clean testing loss: 3.4363632202148438
epoch 43800  training loss: 0.013637539930641651
epoch 43800  clean testing loss: 3.4127278327941895
epoch 43900  training loss: 0.013632605783641338

 45%|████▍     | 44980/100000 [01:21<01:38, 560.30it/s]
epoch 44000  training loss: 0.01362965814769268
epoch 44000  clean testing loss: 3.4349257946014404
Saving the clean test loss file into log/new_plot/output_dim2_width1024_layers2_relu_size50_noise1.00e-01_invop0_lr0.005 ...
epoch 44100  training loss: 0.013648705556988716
epoch 44100  clean testing loss: 3.430479049682617
epoch 44200  training loss: 0.013627874664962292
epoch 44200  clean testing loss: 3.429206371307373
epoch 44300  training loss: 0.013634557835757732
epoch 44300  clean testing loss: 3.4324495792388916
epoch 44400  training loss: 0.013647438026964664
epoch 44400  clean testing loss: 3.438149929046631
epoch 44500  training loss: 0.013646185398101807
epoch 44500  clean testing loss: 3.417534351348877
epoch 44600  training loss: 0.01362419594079256
epoch 44600  clean testing loss: 3.441384792327881
epoch 44700  training loss: 0.013654761016368866
epoch 44700  clean testing loss: 3.429187774658203
epoch 44800  training loss: 0.013628009706735611
epoch 44800  clean testing loss: 3.43742036819458
epoch 44900  training loss: 0.013631699606776237
epoch 44900  clean testing loss: 3.4409708976745605
epoch 45000  training loss: 0.013649048283696175
epoch 45000  clean testing loss: 3.432579278945923

 46%|████▌     | 46052/100000 [01:23<01:38, 546.49it/s]
epoch 45100  training loss: 0.013626658357679844
epoch 45100  clean testing loss: 3.437954902648926
epoch 45200  training loss: 0.013619770295917988
epoch 45200  clean testing loss: 3.451169729232788
epoch 45300  training loss: 0.013627876527607441
epoch 45300  clean testing loss: 3.451533794403076
epoch 45400  training loss: 0.013626616448163986
epoch 45400  clean testing loss: 3.4522032737731934
epoch 45500  training loss: 0.013627514243125916
epoch 45500  clean testing loss: 3.456782817840576
epoch 45600  training loss: 0.01361777726560831
epoch 45600  clean testing loss: 3.44100022315979
epoch 45700  training loss: 0.013627956621348858
epoch 45700  clean testing loss: 3.4463398456573486
epoch 45800  training loss: 0.013628029264509678
epoch 45800  clean testing loss: 3.4465279579162598
epoch 45900  training loss: 0.013618121854960918
epoch 45900  clean testing loss: 3.466474771499634
epoch 46000  training loss: 0.013617674820125103
epoch 46000  clean testing loss: 3.4612276554107666
Saving the clean test loss file into log/new_plot/output_dim2_width1024_layers2_relu_size50_noise1.00e-01_invop0_lr0.005 ...
epoch 46100  training loss: 0.013621428050100803

 47%|████▋     | 47117/100000 [01:25<01:36, 549.36it/s]
epoch 46200  training loss: 0.013618207536637783
epoch 46200  clean testing loss: 3.453376293182373
epoch 46300  training loss: 0.013626367785036564
epoch 46300  clean testing loss: 3.4451406002044678
epoch 46400  training loss: 0.013623075559735298
epoch 46400  clean testing loss: 3.472166061401367
epoch 46500  training loss: 0.013618159107863903
epoch 46500  clean testing loss: 3.4589452743530273
epoch 46600  training loss: 0.013611845672130585
epoch 46600  clean testing loss: 3.4660191535949707
epoch 46700  training loss: 0.013619189150631428
epoch 46700  clean testing loss: 3.47356915473938
epoch 46800  training loss: 0.013617299497127533
epoch 46800  clean testing loss: 3.4640612602233887
epoch 46900  training loss: 0.013622169382870197
epoch 46900  clean testing loss: 3.4659855365753174
epoch 47000  training loss: 0.013612736947834492
epoch 47000  clean testing loss: 3.4672558307647705
Saving the clean test loss file into log/new_plot/output_dim2_width1024_layers2_relu_size50_noise1.00e-01_invop0_lr0.005 ...
epoch 47100  training loss: 0.013619344681501389
epoch 47100  clean testing loss: 3.4793455600738525
epoch 47200  training loss: 0.013618017546832561

 48%|████▊     | 48250/100000 [01:27<01:33, 554.44it/s]
epoch 47300  training loss: 0.013621125370264053
epoch 47300  clean testing loss: 3.4680376052856445
epoch 47400  training loss: 0.01361347921192646
epoch 47400  clean testing loss: 3.47721791267395
epoch 47500  training loss: 0.013620237819850445
epoch 47500  clean testing loss: 3.460266590118408
epoch 47600  training loss: 0.013610074296593666
epoch 47600  clean testing loss: 3.4739596843719482
epoch 47700  training loss: 0.013614460825920105
epoch 47700  clean testing loss: 3.469470739364624
epoch 47800  training loss: 0.013604271225631237
epoch 47800  clean testing loss: 3.492037534713745
epoch 47900  training loss: 0.013624865561723709
epoch 47900  clean testing loss: 3.4683544635772705
epoch 48000  training loss: 0.01361091248691082
epoch 48000  clean testing loss: 3.490370512008667
Saving the clean test loss file into log/new_plot/output_dim2_width1024_layers2_relu_size50_noise1.00e-01_invop0_lr0.005 ...
epoch 48100  training loss: 0.01360505074262619
epoch 48100  clean testing loss: 3.4809160232543945
epoch 48200  training loss: 0.013607488945126534
epoch 48200  clean testing loss: 3.4806675910949707
epoch 48300  training loss: 0.013602976687252522

 49%|████▉     | 49381/100000 [01:29<01:30, 559.41it/s]
epoch 48400  training loss: 0.01360456645488739
epoch 48400  clean testing loss: 3.4835100173950195
epoch 48500  training loss: 0.01361159048974514
epoch 48500  clean testing loss: 3.481816291809082
epoch 48600  training loss: 0.013604586943984032
epoch 48600  clean testing loss: 3.495171070098877
epoch 48700  training loss: 0.013605059124529362
epoch 48700  clean testing loss: 3.492950916290283
epoch 48800  training loss: 0.013601266779005527
epoch 48800  clean testing loss: 3.4860754013061523
epoch 48900  training loss: 0.013596747070550919
epoch 48900  clean testing loss: 3.501091957092285
epoch 49000  training loss: 0.013600311242043972
epoch 49000  clean testing loss: 3.4988584518432617
Saving the clean test loss file into log/new_plot/output_dim2_width1024_layers2_relu_size50_noise1.00e-01_invop0_lr0.005 ...
epoch 49100  training loss: 0.013598714955151081
epoch 49100  clean testing loss: 3.488192081451416
epoch 49200  training loss: 0.013597586192190647
epoch 49200  clean testing loss: 3.50009822845459
epoch 49300  training loss: 0.013597638346254826
epoch 49300  clean testing loss: 3.48763370513916
epoch 49400  training loss: 0.013606131076812744

 50%|█████     | 50458/100000 [01:31<01:28, 559.76it/s]
epoch 49500  training loss: 0.01360340602695942
epoch 49500  clean testing loss: 3.478847026824951
epoch 49600  training loss: 0.013595647178590298
epoch 49600  clean testing loss: 3.5005626678466797
epoch 49700  training loss: 0.0135987289249897
epoch 49700  clean testing loss: 3.4909310340881348
epoch 49800  training loss: 0.013601548038423061
epoch 49800  clean testing loss: 3.5107057094573975
epoch 49900  training loss: 0.01359200943261385
epoch 49900  clean testing loss: 3.507418155670166
epoch 50000  training loss: 0.01359384972602129
epoch 50000  clean testing loss: 3.5104458332061768
Saving the clean test loss file into log/new_plot/output_dim2_width1024_layers2_relu_size50_noise1.00e-01_invop0_lr0.005 ...
epoch 50100  training loss: 0.013592015951871872
epoch 50100  clean testing loss: 3.509589672088623
epoch 50200  training loss: 0.013597450219094753
epoch 50200  clean testing loss: 3.514254331588745
epoch 50300  training loss: 0.013590405695140362
epoch 50300  clean testing loss: 3.5035908222198486
epoch 50400  training loss: 0.013592504896223545
epoch 50400  clean testing loss: 3.510228157043457
epoch 50500  training loss: 0.013593746349215508

 52%|█████▏    | 51585/100000 [01:33<01:26, 558.93it/s]
epoch 50600  training loss: 0.013593575917184353
epoch 50600  clean testing loss: 3.508160352706909
epoch 50700  training loss: 0.013599609024822712
epoch 50700  clean testing loss: 3.516685724258423
epoch 50800  training loss: 0.013589737005531788
epoch 50800  clean testing loss: 3.502498149871826
epoch 50900  training loss: 0.013592720031738281
epoch 50900  clean testing loss: 3.5261354446411133
epoch 51000  training loss: 0.013598890975117683
epoch 51000  clean testing loss: 3.508289098739624
Saving the clean test loss file into log/new_plot/output_dim2_width1024_layers2_relu_size50_noise1.00e-01_invop0_lr0.005 ...
epoch 51100  training loss: 0.013585126027464867
epoch 51100  clean testing loss: 3.511969566345215
epoch 51200  training loss: 0.01358484011143446
epoch 51200  clean testing loss: 3.5135369300842285
epoch 51300  training loss: 0.013584112748503685
epoch 51300  clean testing loss: 3.515674114227295
epoch 51400  training loss: 0.013589425012469292
epoch 51400  clean testing loss: 3.510791540145874
epoch 51500  training loss: 0.01358683593571186
epoch 51500  clean testing loss: 3.51454758644104
epoch 51600  training loss: 0.013590047135949135

 53%|█████▎    | 52709/100000 [01:35<01:25, 553.25it/s]
epoch 51700  training loss: 0.013591544702649117
epoch 51700  clean testing loss: 3.523993968963623
epoch 51800  training loss: 0.013587357476353645
epoch 51800  clean testing loss: 3.522648572921753
epoch 51900  training loss: 0.013581712730228901
epoch 51900  clean testing loss: 3.5191686153411865
epoch 52000  training loss: 0.013584939762949944
epoch 52000  clean testing loss: 3.5228633880615234
Saving the clean test loss file into log/new_plot/output_dim2_width1024_layers2_relu_size50_noise1.00e-01_invop0_lr0.005 ...
epoch 52100  training loss: 0.013586999848484993
epoch 52100  clean testing loss: 3.5203022956848145
epoch 52200  training loss: 0.013581588864326477
epoch 52200  clean testing loss: 3.5238921642303467
epoch 52300  training loss: 0.013583204708993435
epoch 52300  clean testing loss: 3.5260937213897705
epoch 52400  training loss: 0.013584984466433525
epoch 52400  clean testing loss: 3.533284902572632
epoch 52500  training loss: 0.01358065102249384
epoch 52500  clean testing loss: 3.5349373817443848
epoch 52600  training loss: 0.013586131855845451
epoch 52600  clean testing loss: 3.5277392864227295
epoch 52700  training loss: 0.013584738597273827

 54%|█████▍    | 53784/100000 [01:37<01:22, 559.79it/s]
epoch 52800  training loss: 0.013579439371824265
epoch 52800  clean testing loss: 3.5301666259765625
epoch 52900  training loss: 0.013583462685346603
epoch 52900  clean testing loss: 3.5346806049346924
epoch 53000  training loss: 0.013581723906099796
epoch 53000  clean testing loss: 3.5386950969696045
Saving the clean test loss file into log/new_plot/output_dim2_width1024_layers2_relu_size50_noise1.00e-01_invop0_lr0.005 ...
epoch 53100  training loss: 0.013582061976194382
epoch 53100  clean testing loss: 3.5267860889434814
epoch 53200  training loss: 0.013581173494458199
epoch 53200  clean testing loss: 3.534034013748169
epoch 53300  training loss: 0.013578266836702824
epoch 53300  clean testing loss: 3.5253655910491943
epoch 53400  training loss: 0.013584217987954617
epoch 53400  clean testing loss: 3.5334300994873047
epoch 53500  training loss: 0.01357741467654705
epoch 53500  clean testing loss: 3.5358035564422607
epoch 53600  training loss: 0.013578827492892742
epoch 53600  clean testing loss: 3.5262444019317627
epoch 53700  training loss: 0.013588477857410908
epoch 53700  clean testing loss: 3.537618398666382
epoch 53800  training loss: 0.013578519225120544

 55%|█████▍    | 54914/100000 [01:39<01:21, 555.24it/s]
epoch 53900  training loss: 0.01358028594404459
epoch 53900  clean testing loss: 3.5455150604248047
epoch 54000  training loss: 0.013580561615526676
epoch 54000  clean testing loss: 3.5377047061920166
Saving the clean test loss file into log/new_plot/output_dim2_width1024_layers2_relu_size50_noise1.00e-01_invop0_lr0.005 ...
epoch 54100  training loss: 0.013575751334428787
epoch 54100  clean testing loss: 3.539517641067505
epoch 54200  training loss: 0.013578671962022781
epoch 54200  clean testing loss: 3.535223960876465
epoch 54300  training loss: 0.013576237484812737
epoch 54300  clean testing loss: 3.5377073287963867
epoch 54400  training loss: 0.013575330376625061
epoch 54400  clean testing loss: 3.5384161472320557
epoch 54500  training loss: 0.013577768579125404
epoch 54500  clean testing loss: 3.543884754180908
epoch 54600  training loss: 0.013574137352406979
epoch 54600  clean testing loss: 3.5399911403656006
epoch 54700  training loss: 0.013576104305684566
epoch 54700  clean testing loss: 3.545060873031616
epoch 54800  training loss: 0.013578206300735474
epoch 54800  clean testing loss: 3.546323537826538
epoch 54900  training loss: 0.013574349693953991
epoch 54900  clean testing loss: 3.544520854949951
epoch 55000  training loss: 0.013571616262197495

 56%|█████▌    | 55988/100000 [01:41<01:18, 561.24it/s]
Saving the clean test loss file into log/new_plot/output_dim2_width1024_layers2_relu_size50_noise1.00e-01_invop0_lr0.005 ...
epoch 55100  training loss: 0.013575630262494087
epoch 55100  clean testing loss: 3.5423583984375
epoch 55200  training loss: 0.013572398573160172
epoch 55200  clean testing loss: 3.5466184616088867
epoch 55300  training loss: 0.013575883582234383
epoch 55300  clean testing loss: 3.5389163494110107
epoch 55400  training loss: 0.013577277772128582
epoch 55400  clean testing loss: 3.5452871322631836
epoch 55500  training loss: 0.013575985096395016
epoch 55500  clean testing loss: 3.548231363296509
epoch 55600  training loss: 0.013572903349995613
epoch 55600  clean testing loss: 3.5478463172912598
epoch 55700  training loss: 0.01357204094529152
epoch 55700  clean testing loss: 3.5473015308380127
epoch 55800  training loss: 0.01357231568545103
epoch 55800  clean testing loss: 3.545823574066162
epoch 55900  training loss: 0.013569049537181854
epoch 55900  clean testing loss: 3.5532658100128174
epoch 56000  training loss: 0.0135737843811512
epoch 56000  clean testing loss: 3.543210983276367
Saving the clean test loss file into log/new_plot/output_dim2_width1024_layers2_relu_size50_noise1.00e-01_invop0_lr0.005 ...
epoch 56100  training loss: 0.013572375290095806

 57%|█████▋    | 57120/100000 [01:43<01:18, 549.43it/s]
epoch 56200  training loss: 0.013572911731898785
epoch 56200  clean testing loss: 3.548426628112793
epoch 56300  training loss: 0.013571718707680702
epoch 56300  clean testing loss: 3.5499143600463867
epoch 56400  training loss: 0.013570894487202168
epoch 56400  clean testing loss: 3.5520853996276855
epoch 56500  training loss: 0.01357252337038517
epoch 56500  clean testing loss: 3.5514256954193115
epoch 56600  training loss: 0.013570667244493961
epoch 56600  clean testing loss: 3.5505733489990234
epoch 56700  training loss: 0.013570287264883518
epoch 56700  clean testing loss: 3.552685260772705
epoch 56800  training loss: 0.013571476563811302
epoch 56800  clean testing loss: 3.5515756607055664
epoch 56900  training loss: 0.013568317517638206
epoch 56900  clean testing loss: 3.557126045227051
epoch 57000  training loss: 0.013570505194365978
epoch 57000  clean testing loss: 3.5462629795074463
Saving the clean test loss file into log/new_plot/output_dim2_width1024_layers2_relu_size50_noise1.00e-01_invop0_lr0.005 ...
epoch 57100  training loss: 0.013566462323069572
epoch 57100  clean testing loss: 3.5557591915130615
epoch 57200  training loss: 0.013567439280450344

 58%|█████▊    | 58249/100000 [01:45<01:15, 552.51it/s]
epoch 57300  training loss: 0.013568533584475517
epoch 57300  clean testing loss: 3.5510144233703613
epoch 57400  training loss: 0.013566682115197182
epoch 57400  clean testing loss: 3.5522356033325195
epoch 57500  training loss: 0.013565907254815102
epoch 57500  clean testing loss: 3.5553741455078125
epoch 57600  training loss: 0.013568048365414143
epoch 57600  clean testing loss: 3.553826332092285
epoch 57700  training loss: 0.013565490953624249
epoch 57700  clean testing loss: 3.551790714263916
epoch 57800  training loss: 0.013565417379140854
epoch 57800  clean testing loss: 3.5512194633483887
epoch 57900  training loss: 0.013563408516347408
epoch 57900  clean testing loss: 3.559032440185547
epoch 58000  training loss: 0.013566240668296814
epoch 58000  clean testing loss: 3.5562942028045654
Saving the clean test loss file into log/new_plot/output_dim2_width1024_layers2_relu_size50_noise1.00e-01_invop0_lr0.005 ...
epoch 58100  training loss: 0.013565213419497013
epoch 58100  clean testing loss: 3.5577354431152344
epoch 58200  training loss: 0.01356472261250019
epoch 58200  clean testing loss: 3.5594873428344727
epoch 58300  training loss: 0.013566498644649982

 59%|█████▉    | 59320/100000 [01:47<01:13, 553.25it/s]
epoch 58400  training loss: 0.013562383130192757
epoch 58400  clean testing loss: 3.5527615547180176
epoch 58500  training loss: 0.01356381643563509
epoch 58500  clean testing loss: 3.5571465492248535
epoch 58600  training loss: 0.013562291860580444
epoch 58600  clean testing loss: 3.5620296001434326
epoch 58700  training loss: 0.013564178720116615
epoch 58700  clean testing loss: 3.555518627166748
epoch 58800  training loss: 0.013562836684286594
epoch 58800  clean testing loss: 3.5564305782318115
epoch 58900  training loss: 0.013562788255512714
epoch 58900  clean testing loss: 3.5562188625335693
epoch 59000  training loss: 0.013562768697738647
epoch 59000  clean testing loss: 3.5546488761901855
Saving the clean test loss file into log/new_plot/output_dim2_width1024_layers2_relu_size50_noise1.00e-01_invop0_lr0.005 ...
epoch 59100  training loss: 0.01356539223343134
epoch 59100  clean testing loss: 3.559422492980957
epoch 59200  training loss: 0.013561096042394638
epoch 59200  clean testing loss: 3.5633842945098877
epoch 59300  training loss: 0.013562719337642193
epoch 59300  clean testing loss: 3.5556044578552246
epoch 59400  training loss: 0.013560342602431774

 60%|██████    | 60448/100000 [01:49<01:11, 554.47it/s]
epoch 59500  training loss: 0.013559592887759209
epoch 59500  clean testing loss: 3.5571463108062744
epoch 59600  training loss: 0.01355829369276762
epoch 59600  clean testing loss: 3.56265926361084
epoch 59700  training loss: 0.01356059405952692
epoch 59700  clean testing loss: 3.5600743293762207
epoch 59800  training loss: 0.013561398722231388
epoch 59800  clean testing loss: 3.557508707046509
epoch 59900  training loss: 0.013559656217694283
epoch 59900  clean testing loss: 3.5588717460632324
epoch 60000  training loss: 0.013562615029513836
epoch 60000  clean testing loss: 3.558502197265625
Saving the clean test loss file into log/new_plot/output_dim2_width1024_layers2_relu_size50_noise1.00e-01_invop0_lr0.005 ...
epoch 60100  training loss: 0.013557369820773602
epoch 60100  clean testing loss: 3.560868263244629
epoch 60200  training loss: 0.013557463884353638
epoch 60200  clean testing loss: 3.560300827026367
epoch 60300  training loss: 0.01355751696974039
epoch 60300  clean testing loss: 3.5597755908966064
epoch 60400  training loss: 0.013556835241615772
epoch 60400  clean testing loss: 3.56717586517334
epoch 60500  training loss: 0.013556067831814289

 62%|██████▏   | 61578/100000 [01:51<01:08, 559.40it/s]
epoch 60600  training loss: 0.013557324185967445
epoch 60600  clean testing loss: 3.5673861503601074
epoch 60700  training loss: 0.013557177037000656
epoch 60700  clean testing loss: 3.5610666275024414
epoch 60800  training loss: 0.013558167964220047
epoch 60800  clean testing loss: 3.561077117919922
epoch 60900  training loss: 0.013559645973145962
epoch 60900  clean testing loss: 3.5561580657958984
epoch 61000  training loss: 0.013556319288909435
epoch 61000  clean testing loss: 3.5675923824310303
Saving the clean test loss file into log/new_plot/output_dim2_width1024_layers2_relu_size50_noise1.00e-01_invop0_lr0.005 ...
epoch 61100  training loss: 0.013554910197854042
epoch 61100  clean testing loss: 3.5625486373901367
epoch 61200  training loss: 0.013556904159486294
epoch 61200  clean testing loss: 3.5573267936706543
epoch 61300  training loss: 0.013554634526371956
epoch 61300  clean testing loss: 3.562553644180298
epoch 61400  training loss: 0.01355387456715107
epoch 61400  clean testing loss: 3.5620803833007812
epoch 61500  training loss: 0.013555021956562996
epoch 61500  clean testing loss: 3.561521530151367
epoch 61600  training loss: 0.013555186800658703

 63%|██████▎   | 62653/100000 [01:53<01:07, 557.20it/s]
epoch 61700  training loss: 0.013553664088249207
epoch 61700  clean testing loss: 3.563741445541382
epoch 61800  training loss: 0.01355712953954935
epoch 61800  clean testing loss: 3.5616447925567627
epoch 61900  training loss: 0.013553370721638203
epoch 61900  clean testing loss: 3.5650508403778076
epoch 62000  training loss: 0.013552829623222351
epoch 62000  clean testing loss: 3.568119764328003
Saving the clean test loss file into log/new_plot/output_dim2_width1024_layers2_relu_size50_noise1.00e-01_invop0_lr0.005 ...
epoch 62100  training loss: 0.013555075973272324
epoch 62100  clean testing loss: 3.564453601837158
epoch 62200  training loss: 0.013554498553276062
epoch 62200  clean testing loss: 3.567228317260742
epoch 62300  training loss: 0.013553721830248833
epoch 62300  clean testing loss: 3.5673468112945557
epoch 62400  training loss: 0.013551720418035984
epoch 62400  clean testing loss: 3.570321559906006
epoch 62500  training loss: 0.01355317048728466
epoch 62500  clean testing loss: 3.5685107707977295
epoch 62600  training loss: 0.013550509698688984
epoch 62600  clean testing loss: 3.5647404193878174
epoch 62700  training loss: 0.013549706898629665

 64%|██████▎   | 63723/100000 [01:55<01:05, 556.79it/s]
epoch 62800  training loss: 0.013554361648857594
epoch 62800  clean testing loss: 3.568711280822754
epoch 62900  training loss: 0.013550682924687862
epoch 62900  clean testing loss: 3.5675532817840576
epoch 63000  training loss: 0.013551189564168453
epoch 63000  clean testing loss: 3.5709104537963867
Saving the clean test loss file into log/new_plot/output_dim2_width1024_layers2_relu_size50_noise1.00e-01_invop0_lr0.005 ...
epoch 63100  training loss: 0.013550824485719204
epoch 63100  clean testing loss: 3.568082809448242
epoch 63200  training loss: 0.013548353686928749
epoch 63200  clean testing loss: 3.566375732421875
epoch 63300  training loss: 0.013548793271183968
epoch 63300  clean testing loss: 3.568842649459839
epoch 63400  training loss: 0.013547003269195557
epoch 63400  clean testing loss: 3.570955991744995
epoch 63500  training loss: 0.01354724820703268
epoch 63500  clean testing loss: 3.568415641784668
epoch 63600  training loss: 0.013545827940106392
epoch 63600  clean testing loss: 3.5677006244659424
epoch 63700  training loss: 0.0135454460978508
epoch 63700  clean testing loss: 3.5710232257843018
epoch 63800  training loss: 0.013546764850616455

 65%|██████▍   | 64858/100000 [01:57<01:02, 561.13it/s]
epoch 63900  training loss: 0.013543774373829365
epoch 63900  clean testing loss: 3.5689830780029297
epoch 64000  training loss: 0.013544603250920773
epoch 64000  clean testing loss: 3.569584369659424
Saving the clean test loss file into log/new_plot/output_dim2_width1024_layers2_relu_size50_noise1.00e-01_invop0_lr0.005 ...
epoch 64100  training loss: 0.013541895896196365
epoch 64100  clean testing loss: 3.5637075901031494
epoch 64200  training loss: 0.013541406020522118
epoch 64200  clean testing loss: 3.5682499408721924
epoch 64300  training loss: 0.01354178972542286
epoch 64300  clean testing loss: 3.566986083984375
epoch 64400  training loss: 0.013540105894207954
epoch 64400  clean testing loss: 3.5685131549835205
epoch 64500  training loss: 0.013539861887693405
epoch 64500  clean testing loss: 3.5708677768707275
epoch 64600  training loss: 0.013540329411625862
epoch 64600  clean testing loss: 3.571808099746704
epoch 64700  training loss: 0.01354061346501112
epoch 64700  clean testing loss: 3.5673067569732666
epoch 64800  training loss: 0.013540457002818584
epoch 64800  clean testing loss: 3.5700502395629883
epoch 64900  training loss: 0.013537402264773846

 66%|██████▌   | 65986/100000 [01:59<01:01, 557.10it/s]
epoch 65000  training loss: 0.013537862338125706
epoch 65000  clean testing loss: 3.569164276123047
Saving the clean test loss file into log/new_plot/output_dim2_width1024_layers2_relu_size50_noise1.00e-01_invop0_lr0.005 ...
epoch 65100  training loss: 0.01353686023503542
epoch 65100  clean testing loss: 3.567803382873535
epoch 65200  training loss: 0.013536125421524048
epoch 65200  clean testing loss: 3.5713140964508057
epoch 65300  training loss: 0.013535525649785995
epoch 65300  clean testing loss: 3.5693562030792236
epoch 65400  training loss: 0.013535257428884506
epoch 65400  clean testing loss: 3.570762872695923
epoch 65500  training loss: 0.013534512370824814
epoch 65500  clean testing loss: 3.5681848526000977
epoch 65600  training loss: 0.013533520512282848
epoch 65600  clean testing loss: 3.570540189743042
epoch 65700  training loss: 0.013532805256545544
epoch 65700  clean testing loss: 3.568058967590332
epoch 65800  training loss: 0.013531537726521492
epoch 65800  clean testing loss: 3.5713298320770264
epoch 65900  training loss: 0.013531272299587727
epoch 65900  clean testing loss: 3.569995641708374
epoch 66000  training loss: 0.013530507683753967
epoch 66000  clean testing loss: 3.5684449672698975

 67%|██████▋   | 67055/100000 [02:01<01:00, 546.00it/s]
epoch 66100  training loss: 0.01352940034121275
epoch 66100  clean testing loss: 3.568561315536499
epoch 66200  training loss: 0.01353052631020546
epoch 66200  clean testing loss: 3.5664074420928955
epoch 66300  training loss: 0.01352866180241108
epoch 66300  clean testing loss: 3.567715883255005
epoch 66400  training loss: 0.013530885800719261
epoch 66400  clean testing loss: 3.567131280899048
epoch 66500  training loss: 0.013529948890209198
epoch 66500  clean testing loss: 3.5684654712677
epoch 66600  training loss: 0.013528808951377869
epoch 66600  clean testing loss: 3.567295789718628
epoch 66700  training loss: 0.013527735136449337
epoch 66700  clean testing loss: 3.5655572414398193
epoch 66800  training loss: 0.013528000563383102
epoch 66800  clean testing loss: 3.5680019855499268
epoch 66900  training loss: 0.01352756004780531
epoch 66900  clean testing loss: 3.568934917449951
epoch 67000  training loss: 0.013526927679777145
epoch 67000  clean testing loss: 3.5668280124664307
Saving the clean test loss file into log/new_plot/output_dim2_width1024_layers2_relu_size50_noise1.00e-01_invop0_lr0.005 ...
epoch 67100  training loss: 0.01352598425000906

 68%|██████▊   | 68182/100000 [02:03<00:57, 553.46it/s]
epoch 67200  training loss: 0.0135262506082654
epoch 67200  clean testing loss: 3.569763660430908
epoch 67300  training loss: 0.013525690883398056
epoch 67300  clean testing loss: 3.567364454269409
epoch 67400  training loss: 0.013525513000786304
epoch 67400  clean testing loss: 3.5667333602905273
epoch 67500  training loss: 0.013523823581635952
epoch 67500  clean testing loss: 3.5677525997161865
epoch 67600  training loss: 0.013524394482374191
epoch 67600  clean testing loss: 3.5667781829833984
epoch 67700  training loss: 0.0135249774903059
epoch 67700  clean testing loss: 3.5700721740722656
epoch 67800  training loss: 0.013523206114768982
epoch 67800  clean testing loss: 3.567316770553589
epoch 67900  training loss: 0.013523194938898087
epoch 67900  clean testing loss: 3.5682778358459473
epoch 68000  training loss: 0.013523651286959648
epoch 68000  clean testing loss: 3.5673210620880127
Saving the clean test loss file into log/new_plot/output_dim2_width1024_layers2_relu_size50_noise1.00e-01_invop0_lr0.005 ...
epoch 68100  training loss: 0.013522626832127571
epoch 68100  clean testing loss: 3.5683512687683105
epoch 68200  training loss: 0.013520576991140842

 69%|██████▉   | 69256/100000 [02:05<00:55, 555.11it/s]
epoch 68300  training loss: 0.013521386310458183
epoch 68300  clean testing loss: 3.57112979888916
epoch 68400  training loss: 0.013521701097488403
epoch 68400  clean testing loss: 3.5652916431427
epoch 68500  training loss: 0.013519799336791039
epoch 68500  clean testing loss: 3.5678813457489014
epoch 68600  training loss: 0.013519082218408585
epoch 68600  clean testing loss: 3.56319260597229
epoch 68700  training loss: 0.013519007712602615
epoch 68700  clean testing loss: 3.5621514320373535
epoch 68800  training loss: 0.013518640771508217
epoch 68800  clean testing loss: 3.5668728351593018
epoch 68900  training loss: 0.01351871807128191
epoch 68900  clean testing loss: 3.5652172565460205
epoch 69000  training loss: 0.013518073596060276
epoch 69000  clean testing loss: 3.5695369243621826
Saving the clean test loss file into log/new_plot/output_dim2_width1024_layers2_relu_size50_noise1.00e-01_invop0_lr0.005 ...
epoch 69100  training loss: 0.013517200946807861
epoch 69100  clean testing loss: 3.5651605129241943
epoch 69200  training loss: 0.013516261242330074
epoch 69200  clean testing loss: 3.5690433979034424
epoch 69300  training loss: 0.013515639118850231

 70%|███████   | 70387/100000 [02:07<00:52, 559.48it/s]
epoch 69400  training loss: 0.013515272177755833
epoch 69400  clean testing loss: 3.5641746520996094
epoch 69500  training loss: 0.013515586964786053
epoch 69500  clean testing loss: 3.5666890144348145
epoch 69600  training loss: 0.013513810001313686
epoch 69600  clean testing loss: 3.5690932273864746
epoch 69700  training loss: 0.013512220233678818
epoch 69700  clean testing loss: 3.5707075595855713
epoch 69800  training loss: 0.013512915931642056
epoch 69800  clean testing loss: 3.569486618041992
epoch 69900  training loss: 0.01351176667958498
epoch 69900  clean testing loss: 3.570833683013916
epoch 70000  training loss: 0.013511702418327332
epoch 70000  clean testing loss: 3.5705714225769043
Saving the clean test loss file into log/new_plot/output_dim2_width1024_layers2_relu_size50_noise1.00e-01_invop0_lr0.005 ...
epoch 70100  training loss: 0.013511753641068935
epoch 70100  clean testing loss: 3.570451498031616
epoch 70200  training loss: 0.013510487042367458
epoch 70200  clean testing loss: 3.571758270263672
epoch 70300  training loss: 0.013510189019143581
epoch 70300  clean testing loss: 3.5693979263305664
epoch 70400  training loss: 0.013510024175047874

 72%|███████▏  | 71518/100000 [02:09<00:51, 556.15it/s]
epoch 70500  training loss: 0.01350837666541338
epoch 70500  clean testing loss: 3.5685925483703613
epoch 70600  training loss: 0.01350850984454155
epoch 70600  clean testing loss: 3.569600820541382
epoch 70700  training loss: 0.013507685624063015
epoch 70700  clean testing loss: 3.5699996948242188
epoch 70800  training loss: 0.013506759889423847
epoch 70800  clean testing loss: 3.5703344345092773
epoch 70900  training loss: 0.01350560411810875
epoch 70900  clean testing loss: 3.5697174072265625
epoch 71000  training loss: 0.01350611262023449
epoch 71000  clean testing loss: 3.5698819160461426
Saving the clean test loss file into log/new_plot/output_dim2_width1024_layers2_relu_size50_noise1.00e-01_invop0_lr0.005 ...
epoch 71100  training loss: 0.01350570097565651
epoch 71100  clean testing loss: 3.571676015853882
epoch 71200  training loss: 0.013505769893527031
epoch 71200  clean testing loss: 3.570845603942871
epoch 71300  training loss: 0.013504616916179657
epoch 71300  clean testing loss: 3.57012677192688
epoch 71400  training loss: 0.013503233902156353
epoch 71400  clean testing loss: 3.5759687423706055
epoch 71500  training loss: 0.013503508642315865
epoch 71500  clean testing loss: 3.5722885131835938
epoch 71600  training loss: 0.013502521440386772

 73%|███████▎  | 72582/100000 [02:11<00:49, 555.84it/s]
epoch 71700  training loss: 0.013502024114131927
epoch 71700  clean testing loss: 3.5713143348693848
epoch 71800  training loss: 0.013502880930900574
epoch 71800  clean testing loss: 3.5701699256896973
epoch 71900  training loss: 0.013501935638487339
epoch 71900  clean testing loss: 3.5736796855926514
epoch 72000  training loss: 0.013502086512744427
epoch 72000  clean testing loss: 3.5753448009490967
Saving the clean test loss file into log/new_plot/output_dim2_width1024_layers2_relu_size50_noise1.00e-01_invop0_lr0.005 ...
epoch 72100  training loss: 0.013500341214239597
epoch 72100  clean testing loss: 3.5748889446258545
epoch 72200  training loss: 0.013500874862074852
epoch 72200  clean testing loss: 3.570610523223877
epoch 72300  training loss: 0.01349985133856535
epoch 72300  clean testing loss: 3.5731756687164307
epoch 72400  training loss: 0.013499711640179157
epoch 72400  clean testing loss: 3.5746495723724365
epoch 72500  training loss: 0.013499791733920574
epoch 72500  clean testing loss: 3.573042869567871
epoch 72600  training loss: 0.013499809429049492

 74%|███████▎  | 73704/100000 [02:13<00:47, 550.09it/s]
epoch 72700  training loss: 0.013499190099537373
epoch 72700  clean testing loss: 3.5753674507141113
epoch 72800  training loss: 0.013499303720891476
epoch 72800  clean testing loss: 3.5732181072235107
epoch 72900  training loss: 0.013497943989932537
epoch 72900  clean testing loss: 3.5731468200683594
epoch 73000  training loss: 0.013497536070644855
epoch 73000  clean testing loss: 3.574450969696045
Saving the clean test loss file into log/new_plot/output_dim2_width1024_layers2_relu_size50_noise1.00e-01_invop0_lr0.005 ...
epoch 73100  training loss: 0.013497660867869854
epoch 73100  clean testing loss: 3.575373411178589
epoch 73200  training loss: 0.013497247360646725
epoch 73200  clean testing loss: 3.5776476860046387
epoch 73300  training loss: 0.013496058993041515
epoch 73300  clean testing loss: 3.5773518085479736
epoch 73400  training loss: 0.013496382161974907
epoch 73400  clean testing loss: 3.5763027667999268
epoch 73500  training loss: 0.01349600963294506
epoch 73500  clean testing loss: 3.5756237506866455
epoch 73600  training loss: 0.01349578332155943
epoch 73600  clean testing loss: 3.57625675201416
epoch 73700  training loss: 0.013495602644979954
epoch 73700  clean testing loss: 3.5769238471984863
epoch 73800  training loss: 0.013495062477886677

 75%|███████▍  | 74829/100000 [02:15<00:45, 554.07it/s]
epoch 73900  training loss: 0.013495475985109806
epoch 73900  clean testing loss: 3.574775218963623
epoch 74000  training loss: 0.013495251536369324
epoch 74000  clean testing loss: 3.5762579441070557
Saving the clean test loss file into log/new_plot/output_dim2_width1024_layers2_relu_size50_noise1.00e-01_invop0_lr0.005 ...
epoch 74100  training loss: 0.013494743965566158
epoch 74100  clean testing loss: 3.5780858993530273
epoch 74200  training loss: 0.01349572278559208
epoch 74200  clean testing loss: 3.5770699977874756
epoch 74300  training loss: 0.013494032435119152
epoch 74300  clean testing loss: 3.5778088569641113
epoch 74400  training loss: 0.013493114151060581
epoch 74400  clean testing loss: 3.577437162399292
epoch 74500  training loss: 0.013494415208697319
epoch 74500  clean testing loss: 3.580448627471924
epoch 74600  training loss: 0.013493561185896397
epoch 74600  clean testing loss: 3.5791313648223877
epoch 74700  training loss: 0.013493991456925869
epoch 74700  clean testing loss: 3.5816752910614014
epoch 74800  training loss: 0.013493389822542667
epoch 74800  clean testing loss: 3.5768449306488037
epoch 74900  training loss: 0.01349232904613018

 76%|███████▌  | 75901/100000 [02:17<00:43, 556.24it/s]
epoch 75000  training loss: 0.013491909019649029
epoch 75000  clean testing loss: 3.579850435256958
Saving the clean test loss file into log/new_plot/output_dim2_width1024_layers2_relu_size50_noise1.00e-01_invop0_lr0.005 ...
epoch 75100  training loss: 0.01349179819226265
epoch 75100  clean testing loss: 3.57882022857666
epoch 75200  training loss: 0.013491792604327202
epoch 75200  clean testing loss: 3.578670024871826
epoch 75300  training loss: 0.013491355814039707
epoch 75300  clean testing loss: 3.57891583442688
epoch 75400  training loss: 0.013491864316165447
epoch 75400  clean testing loss: 3.578990936279297
epoch 75500  training loss: 0.013491600751876831
epoch 75500  clean testing loss: 3.579738140106201
epoch 75600  training loss: 0.013490937650203705
epoch 75600  clean testing loss: 3.5794219970703125
epoch 75700  training loss: 0.013491440564393997
epoch 75700  clean testing loss: 3.5804948806762695
epoch 75800  training loss: 0.013490574434399605
epoch 75800  clean testing loss: 3.582003116607666
epoch 75900  training loss: 0.013490359298884869
epoch 75900  clean testing loss: 3.581205368041992
epoch 76000  training loss: 0.013491158373653889
epoch 76000  clean testing loss: 3.5802409648895264

 77%|███████▋  | 77033/100000 [02:19<00:41, 548.03it/s]
epoch 76100  training loss: 0.013490896672010422
epoch 76100  clean testing loss: 3.5820469856262207
epoch 76200  training loss: 0.013489778153598309
epoch 76200  clean testing loss: 3.581732988357544
epoch 76300  training loss: 0.013489942066371441
epoch 76300  clean testing loss: 3.582643985748291
epoch 76400  training loss: 0.013489529490470886
epoch 76400  clean testing loss: 3.5814385414123535
epoch 76500  training loss: 0.013490341603755951
epoch 76500  clean testing loss: 3.5804481506347656
epoch 76600  training loss: 0.01348961889743805
epoch 76600  clean testing loss: 3.5816709995269775
epoch 76700  training loss: 0.013489384204149246
epoch 76700  clean testing loss: 3.5815703868865967
epoch 76800  training loss: 0.013488859869539738
epoch 76800  clean testing loss: 3.58294415473938
epoch 76900  training loss: 0.01348865032196045
epoch 76900  clean testing loss: 3.582540512084961
epoch 77000  training loss: 0.013489133678376675
epoch 77000  clean testing loss: 3.582451105117798
Saving the clean test loss file into log/new_plot/output_dim2_width1024_layers2_relu_size50_noise1.00e-01_invop0_lr0.005 ...
epoch 77100  training loss: 0.013488118536770344

 78%|███████▊  | 78165/100000 [02:21<00:39, 552.88it/s]
epoch 77200  training loss: 0.013489141128957272
epoch 77200  clean testing loss: 3.5816094875335693
epoch 77300  training loss: 0.013487747870385647
epoch 77300  clean testing loss: 3.582864284515381
epoch 77400  training loss: 0.013487515971064568
epoch 77400  clean testing loss: 3.583514928817749
epoch 77500  training loss: 0.013488322496414185
epoch 77500  clean testing loss: 3.5830447673797607
epoch 77600  training loss: 0.01348815206438303
epoch 77600  clean testing loss: 3.5835869312286377
epoch 77700  training loss: 0.013487226329743862
epoch 77700  clean testing loss: 3.584857702255249
epoch 77800  training loss: 0.013486986048519611
epoch 77800  clean testing loss: 3.5835087299346924
epoch 77900  training loss: 0.013487443327903748
epoch 77900  clean testing loss: 3.584379196166992
epoch 78000  training loss: 0.013486944139003754
epoch 78000  clean testing loss: 3.5842766761779785
Saving the clean test loss file into log/new_plot/output_dim2_width1024_layers2_relu_size50_noise1.00e-01_invop0_lr0.005 ...
epoch 78100  training loss: 0.013486314564943314
epoch 78100  clean testing loss: 3.583082914352417
epoch 78200  training loss: 0.01348594855517149

 79%|███████▉  | 79241/100000 [02:23<00:37, 550.88it/s]
epoch 78300  training loss: 0.013486271724104881
epoch 78300  clean testing loss: 3.584242582321167
epoch 78400  training loss: 0.013485632836818695
epoch 78400  clean testing loss: 3.5844991207122803
epoch 78500  training loss: 0.013485504314303398
epoch 78500  clean testing loss: 3.58416748046875
epoch 78600  training loss: 0.013485705479979515
epoch 78600  clean testing loss: 3.584087371826172
epoch 78700  training loss: 0.013485133647918701
epoch 78700  clean testing loss: 3.5832629203796387
epoch 78800  training loss: 0.013485419563949108
epoch 78800  clean testing loss: 3.5841100215911865
epoch 78900  training loss: 0.013485114090144634
epoch 78900  clean testing loss: 3.58476185798645
epoch 79000  training loss: 0.013485042378306389
epoch 79000  clean testing loss: 3.5851845741271973
Saving the clean test loss file into log/new_plot/output_dim2_width1024_layers2_relu_size50_noise1.00e-01_invop0_lr0.005 ...
epoch 79100  training loss: 0.01348497811704874
epoch 79100  clean testing loss: 3.5850722789764404
epoch 79200  training loss: 0.013485061004757881
epoch 79200  clean testing loss: 3.5852596759796143
epoch 79300  training loss: 0.01348420325666666

 80%|████████  | 80298/100000 [02:25<00:36, 545.15it/s]
epoch 79400  training loss: 0.013484256342053413
epoch 79400  clean testing loss: 3.582430839538574
epoch 79500  training loss: 0.013484467752277851
epoch 79500  clean testing loss: 3.5867838859558105
epoch 79600  training loss: 0.013483818620443344
epoch 79600  clean testing loss: 3.5851519107818604
epoch 79700  training loss: 0.01348386611789465
epoch 79700  clean testing loss: 3.585508346557617
epoch 79800  training loss: 0.013483972288668156
epoch 79800  clean testing loss: 3.585012197494507
epoch 79900  training loss: 0.013483140617609024
epoch 79900  clean testing loss: 3.586282968521118
epoch 80000  training loss: 0.013483159244060516
epoch 80000  clean testing loss: 3.585277557373047
Saving the clean test loss file into log/new_plot/output_dim2_width1024_layers2_relu_size50_noise1.00e-01_invop0_lr0.005 ...
epoch 80100  training loss: 0.013483094982802868
epoch 80100  clean testing loss: 3.58536434173584
epoch 80200  training loss: 0.013483388349413872
epoch 80200  clean testing loss: 3.585392713546753
epoch 80300  training loss: 0.013483021408319473
epoch 80300  clean testing loss: 3.584721565246582
epoch 80400  training loss: 0.013482452370226383

 81%|████████▏ | 81425/100000 [02:27<00:33, 552.25it/s]
epoch 80500  training loss: 0.0134828956797719
epoch 80500  clean testing loss: 3.5864830017089844
epoch 80600  training loss: 0.013482408598065376
epoch 80600  clean testing loss: 3.585252285003662
epoch 80700  training loss: 0.01348277647048235
epoch 80700  clean testing loss: 3.586958646774292
epoch 80800  training loss: 0.01348221767693758
epoch 80800  clean testing loss: 3.5851473808288574
epoch 80900  training loss: 0.013482004404067993
epoch 80900  clean testing loss: 3.5862159729003906
epoch 81000  training loss: 0.013481417670845985
epoch 81000  clean testing loss: 3.5859215259552
Saving the clean test loss file into log/new_plot/output_dim2_width1024_layers2_relu_size50_noise1.00e-01_invop0_lr0.005 ...
epoch 81100  training loss: 0.013481822796165943
epoch 81100  clean testing loss: 3.587322235107422
epoch 81200  training loss: 0.013481429778039455
epoch 81200  clean testing loss: 3.5860562324523926
epoch 81300  training loss: 0.013481044210493565
epoch 81300  clean testing loss: 3.5860202312469482
epoch 81400  training loss: 0.013481094501912594
epoch 81400  clean testing loss: 3.5863842964172363
epoch 81500  training loss: 0.013481080532073975

 83%|████████▎ | 82551/100000 [02:29<00:31, 552.07it/s]
epoch 81600  training loss: 0.01348056085407734
epoch 81600  clean testing loss: 3.5855119228363037
epoch 81700  training loss: 0.013481144793331623
epoch 81700  clean testing loss: 3.58573055267334
epoch 81800  training loss: 0.013480703346431255
epoch 81800  clean testing loss: 3.58634877204895
epoch 81900  training loss: 0.013480467721819878
epoch 81900  clean testing loss: 3.586611747741699
epoch 82000  training loss: 0.013480203226208687
epoch 82000  clean testing loss: 3.5873095989227295
Saving the clean test loss file into log/new_plot/output_dim2_width1024_layers2_relu_size50_noise1.00e-01_invop0_lr0.005 ...
epoch 82100  training loss: 0.013479778543114662
epoch 82100  clean testing loss: 3.5864577293395996
epoch 82200  training loss: 0.013480097986757755
epoch 82200  clean testing loss: 3.5868921279907227
epoch 82300  training loss: 0.013479919172823429
epoch 82300  clean testing loss: 3.587003707885742
epoch 82400  training loss: 0.013479757122695446
epoch 82400  clean testing loss: 3.586819887161255
epoch 82500  training loss: 0.013480335474014282
epoch 82500  clean testing loss: 3.5865163803100586
epoch 82600  training loss: 0.013479488901793957

 84%|████████▎ | 83624/100000 [02:31<00:29, 554.94it/s]
epoch 82700  training loss: 0.013479182496666908
epoch 82700  clean testing loss: 3.5860202312469482
epoch 82800  training loss: 0.013479499146342278
epoch 82800  clean testing loss: 3.5869197845458984
epoch 82900  training loss: 0.013478860259056091
epoch 82900  clean testing loss: 3.5879385471343994
epoch 83000  training loss: 0.013479609973728657
epoch 83000  clean testing loss: 3.586509943008423
Saving the clean test loss file into log/new_plot/output_dim2_width1024_layers2_relu_size50_noise1.00e-01_invop0_lr0.005 ...
epoch 83100  training loss: 0.013478787615895271
epoch 83100  clean testing loss: 3.5864627361297607
epoch 83200  training loss: 0.013478966429829597
epoch 83200  clean testing loss: 3.5865516662597656
epoch 83300  training loss: 0.013478458859026432
epoch 83300  clean testing loss: 3.5868358612060547
epoch 83400  training loss: 0.013478580862283707
epoch 83400  clean testing loss: 3.588944911956787
epoch 83500  training loss: 0.013478327542543411
epoch 83500  clean testing loss: 3.587939739227295
epoch 83600  training loss: 0.01347815990447998
epoch 83600  clean testing loss: 3.5882790088653564
epoch 83700  training loss: 0.013477932661771774

 85%|████████▍ | 84755/100000 [02:33<00:27, 554.81it/s]
epoch 83800  training loss: 0.013477589935064316
epoch 83800  clean testing loss: 3.585259437561035
epoch 83900  training loss: 0.01347754243761301
epoch 83900  clean testing loss: 3.587207555770874
epoch 84000  training loss: 0.0134776895865798
epoch 84000  clean testing loss: 3.5875236988067627
Saving the clean test loss file into log/new_plot/output_dim2_width1024_layers2_relu_size50_noise1.00e-01_invop0_lr0.005 ...
epoch 84100  training loss: 0.013476934283971786
epoch 84100  clean testing loss: 3.5872983932495117
epoch 84200  training loss: 0.013476941734552383
epoch 84200  clean testing loss: 3.5869390964508057
epoch 84300  training loss: 0.013476700522005558
epoch 84300  clean testing loss: 3.5883395671844482
epoch 84400  training loss: 0.013476517051458359
epoch 84400  clean testing loss: 3.587801218032837
epoch 84500  training loss: 0.013476889580488205
epoch 84500  clean testing loss: 3.587209463119507
epoch 84600  training loss: 0.013476746156811714
epoch 84600  clean testing loss: 3.5883560180664062
epoch 84700  training loss: 0.013476449996232986
epoch 84700  clean testing loss: 3.5870842933654785
epoch 84800  training loss: 0.013476552441716194

 86%|████████▌ | 85886/100000 [02:35<00:25, 559.10it/s]
epoch 84900  training loss: 0.013476375490427017
epoch 84900  clean testing loss: 3.588196277618408
epoch 85000  training loss: 0.013475850224494934
epoch 85000  clean testing loss: 3.588047504425049
Saving the clean test loss file into log/new_plot/output_dim2_width1024_layers2_relu_size50_noise1.00e-01_invop0_lr0.005 ...
epoch 85100  training loss: 0.013475836254656315
epoch 85100  clean testing loss: 3.587181568145752
epoch 85200  training loss: 0.013475795276463032
epoch 85200  clean testing loss: 3.5882790088653564
epoch 85300  training loss: 0.013475507497787476
epoch 85300  clean testing loss: 3.589090585708618
epoch 85400  training loss: 0.013475439511239529
epoch 85400  clean testing loss: 3.588167428970337
epoch 85500  training loss: 0.013475315645337105
epoch 85500  clean testing loss: 3.5883216857910156
epoch 85600  training loss: 0.013475101441144943
epoch 85600  clean testing loss: 3.58837628364563
epoch 85700  training loss: 0.013474921695888042
epoch 85700  clean testing loss: 3.5877788066864014
epoch 85800  training loss: 0.013475204817950726
epoch 85800  clean testing loss: 3.588818311691284
epoch 85900  training loss: 0.013474749401211739

 87%|████████▋ | 86956/100000 [02:37<00:23, 554.62it/s]
epoch 86000  training loss: 0.013474822044372559
epoch 86000  clean testing loss: 3.588857889175415
Saving the clean test loss file into log/new_plot/output_dim2_width1024_layers2_relu_size50_noise1.00e-01_invop0_lr0.005 ...
epoch 86100  training loss: 0.013474218547344208
epoch 86100  clean testing loss: 3.5882785320281982
epoch 86200  training loss: 0.013474277220666409
epoch 86200  clean testing loss: 3.5874433517456055
epoch 86300  training loss: 0.01347440667450428
epoch 86300  clean testing loss: 3.587517738342285
epoch 86400  training loss: 0.013474018312990665
epoch 86400  clean testing loss: 3.5875344276428223
epoch 86500  training loss: 0.013474522158503532
epoch 86500  clean testing loss: 3.5879900455474854
epoch 86600  training loss: 0.013474068604409695
epoch 86600  clean testing loss: 3.588611602783203
epoch 86700  training loss: 0.013473570346832275
epoch 86700  clean testing loss: 3.5879712104797363
epoch 86800  training loss: 0.01347386185079813
epoch 86800  clean testing loss: 3.587956428527832
epoch 86900  training loss: 0.013473442755639553
epoch 86900  clean testing loss: 3.588291883468628
epoch 87000  training loss: 0.01347304042428732
epoch 87000  clean testing loss: 3.587597608566284

 88%|████████▊ | 88084/100000 [02:39<00:21, 550.03it/s]
epoch 87100  training loss: 0.013472802005708218
epoch 87100  clean testing loss: 3.588381767272949
epoch 87200  training loss: 0.01347261294722557
epoch 87200  clean testing loss: 3.5884790420532227
epoch 87300  training loss: 0.013472288846969604
epoch 87300  clean testing loss: 3.588876962661743
epoch 87400  training loss: 0.013472454622387886
epoch 87400  clean testing loss: 3.588653564453125
epoch 87500  training loss: 0.013472429476678371
epoch 87500  clean testing loss: 3.587871551513672
epoch 87600  training loss: 0.013471967540681362
epoch 87600  clean testing loss: 3.588114023208618
epoch 87700  training loss: 0.013472048565745354
epoch 87700  clean testing loss: 3.588884115219116
epoch 87800  training loss: 0.01347176544368267
epoch 87800  clean testing loss: 3.587923288345337
epoch 87900  training loss: 0.013471757993102074
epoch 87900  clean testing loss: 3.5885026454925537
epoch 88000  training loss: 0.013471643440425396
epoch 88000  clean testing loss: 3.5888121128082275
Saving the clean test loss file into log/new_plot/output_dim2_width1024_layers2_relu_size50_noise1.00e-01_invop0_lr0.005 ...
epoch 88100  training loss: 0.013471256010234356

 89%|████████▉ | 89157/100000 [02:41<00:19, 549.41it/s]
epoch 88200  training loss: 0.013471357524394989
epoch 88200  clean testing loss: 3.5886905193328857
epoch 88300  training loss: 0.013471230864524841
epoch 88300  clean testing loss: 3.5891475677490234
epoch 88400  training loss: 0.013471052050590515
epoch 88400  clean testing loss: 3.5891737937927246
epoch 88500  training loss: 0.013471030630171299
epoch 88500  clean testing loss: 3.58907151222229
epoch 88600  training loss: 0.013470782898366451
epoch 88600  clean testing loss: 3.589664936065674
epoch 88700  training loss: 0.013470646925270557
epoch 88700  clean testing loss: 3.588796377182007
epoch 88800  training loss: 0.01347056869417429
epoch 88800  clean testing loss: 3.588977336883545
epoch 88900  training loss: 0.013470173813402653
epoch 88900  clean testing loss: 3.5881099700927734
epoch 89000  training loss: 0.013470285572111607
epoch 89000  clean testing loss: 3.5896599292755127
Saving the clean test loss file into log/new_plot/output_dim2_width1024_layers2_relu_size50_noise1.00e-01_invop0_lr0.005 ...
epoch 89100  training loss: 0.01347022969275713
epoch 89100  clean testing loss: 3.5897881984710693
epoch 89200  training loss: 0.013469960540533066

 90%|█████████ | 90284/100000 [02:43<00:17, 552.22it/s]
epoch 89300  training loss: 0.013469965197145939
epoch 89300  clean testing loss: 3.588740110397339
epoch 89400  training loss: 0.013469451107084751
epoch 89400  clean testing loss: 3.589409112930298
epoch 89500  training loss: 0.013469581492245197
epoch 89500  clean testing loss: 3.5885913372039795
epoch 89600  training loss: 0.013469517230987549
epoch 89600  clean testing loss: 3.5888781547546387
epoch 89700  training loss: 0.013469173572957516
epoch 89700  clean testing loss: 3.5887203216552734
epoch 89800  training loss: 0.013469415716826916
epoch 89800  clean testing loss: 3.5895140171051025
epoch 89900  training loss: 0.013469216413795948
epoch 89900  clean testing loss: 3.589231491088867
epoch 90000  training loss: 0.013469122350215912
epoch 90000  clean testing loss: 3.5895566940307617
Saving the clean test loss file into log/new_plot/output_dim2_width1024_layers2_relu_size50_noise1.00e-01_invop0_lr0.005 ...
epoch 90100  training loss: 0.013468969613313675
epoch 90100  clean testing loss: 3.589937448501587
epoch 90200  training loss: 0.013468436896800995
epoch 90200  clean testing loss: 3.589172840118408
epoch 90300  training loss: 0.013468588702380657
epoch 90300  clean testing loss: 3.589427947998047
epoch 90400  training loss: 0.01346839964389801

 91%|█████████▏| 91418/100000 [02:45<00:15, 551.28it/s]
epoch 90500  training loss: 0.013468258082866669
epoch 90500  clean testing loss: 3.5892181396484375
epoch 90600  training loss: 0.013468150980770588
epoch 90600  clean testing loss: 3.589735746383667
epoch 90700  training loss: 0.013468083925545216
epoch 90700  clean testing loss: 3.589167356491089
epoch 90800  training loss: 0.013468111865222454
epoch 90800  clean testing loss: 3.5891594886779785
epoch 90900  training loss: 0.01346778403967619
epoch 90900  clean testing loss: 3.5888378620147705
epoch 91000  training loss: 0.013467911630868912
epoch 91000  clean testing loss: 3.5893239974975586
Saving the clean test loss file into log/new_plot/output_dim2_width1024_layers2_relu_size50_noise1.00e-01_invop0_lr0.005 ...
epoch 91100  training loss: 0.013467738404870033
epoch 91100  clean testing loss: 3.5892622470855713
epoch 91200  training loss: 0.013467883691191673
epoch 91200  clean testing loss: 3.589930534362793
epoch 91300  training loss: 0.01346740685403347
epoch 91300  clean testing loss: 3.5894081592559814
epoch 91400  training loss: 0.013467399403452873
epoch 91400  clean testing loss: 3.589085817337036
epoch 91500  training loss: 0.013467593118548393

 92%|█████████▏| 92494/100000 [02:47<00:13, 559.02it/s]
epoch 91600  training loss: 0.01346716471016407
epoch 91600  clean testing loss: 3.588771343231201
epoch 91700  training loss: 0.013467151671648026
epoch 91700  clean testing loss: 3.589174509048462
epoch 91800  training loss: 0.013467095792293549
epoch 91800  clean testing loss: 3.589353561401367
epoch 91900  training loss: 0.013467083685100079
epoch 91900  clean testing loss: 3.5891027450561523
epoch 92000  training loss: 0.013467015698552132
epoch 92000  clean testing loss: 3.588782787322998
Saving the clean test loss file into log/new_plot/output_dim2_width1024_layers2_relu_size50_noise1.00e-01_invop0_lr0.005 ...
epoch 92100  training loss: 0.01346684992313385
epoch 92100  clean testing loss: 3.5897674560546875
epoch 92200  training loss: 0.013466699048876762
epoch 92200  clean testing loss: 3.5890135765075684
epoch 92300  training loss: 0.013466824777424335
epoch 92300  clean testing loss: 3.5894153118133545
epoch 92400  training loss: 0.013466487638652325
epoch 92400  clean testing loss: 3.5892457962036133
epoch 92500  training loss: 0.013466529548168182
epoch 92500  clean testing loss: 3.5888798236846924
epoch 92600  training loss: 0.013466326519846916

 94%|█████████▎| 93625/100000 [02:49<00:11, 551.78it/s]
epoch 92700  training loss: 0.01346650905907154
epoch 92700  clean testing loss: 3.588870048522949
epoch 92800  training loss: 0.01346618589013815
epoch 92800  clean testing loss: 3.5895369052886963
epoch 92900  training loss: 0.01346619613468647
epoch 92900  clean testing loss: 3.58931827545166
epoch 93000  training loss: 0.013465691357851028
epoch 93000  clean testing loss: 3.5889885425567627
Saving the clean test loss file into log/new_plot/output_dim2_width1024_layers2_relu_size50_noise1.00e-01_invop0_lr0.005 ...
epoch 93100  training loss: 0.013465601950883865
epoch 93100  clean testing loss: 3.588928699493408
epoch 93200  training loss: 0.013465811498463154
epoch 93200  clean testing loss: 3.5887293815612793
epoch 93300  training loss: 0.013465606607496738
epoch 93300  clean testing loss: 3.588679790496826
epoch 93400  training loss: 0.01346548181027174
epoch 93400  clean testing loss: 3.589266061782837
epoch 93500  training loss: 0.01346549578011036
epoch 93500  clean testing loss: 3.589198589324951
epoch 93600  training loss: 0.013465270400047302
epoch 93600  clean testing loss: 3.5891761779785156
epoch 93700  training loss: 0.01346530020236969

 95%|█████████▍| 94752/100000 [02:51<00:09, 552.64it/s]
epoch 93800  training loss: 0.013465072959661484
epoch 93800  clean testing loss: 3.5886356830596924
epoch 93900  training loss: 0.013465166091918945
epoch 93900  clean testing loss: 3.5889015197753906
epoch 94000  training loss: 0.013465269468724728
epoch 94000  clean testing loss: 3.5892584323883057
Saving the clean test loss file into log/new_plot/output_dim2_width1024_layers2_relu_size50_noise1.00e-01_invop0_lr0.005 ...
epoch 94100  training loss: 0.013464778661727905
epoch 94100  clean testing loss: 3.589150905609131
epoch 94200  training loss: 0.013464835472404957
epoch 94200  clean testing loss: 3.5894930362701416
epoch 94300  training loss: 0.013464903458952904
epoch 94300  clean testing loss: 3.5893542766571045
epoch 94400  training loss: 0.013464805670082569
epoch 94400  clean testing loss: 3.5894832611083984
epoch 94500  training loss: 0.013464625924825668
epoch 94500  clean testing loss: 3.588921070098877
epoch 94600  training loss: 0.013464672490954399
epoch 94600  clean testing loss: 3.5894765853881836
epoch 94700  training loss: 0.013464459218084812
epoch 94700  clean testing loss: 3.589388370513916
epoch 94800  training loss: 0.013464374467730522

 96%|█████████▌| 95825/100000 [02:53<00:07, 549.82it/s]
epoch 94900  training loss: 0.013464301824569702
epoch 94900  clean testing loss: 3.589836359024048
epoch 95000  training loss: 0.013464326970279217
epoch 95000  clean testing loss: 3.5893821716308594
Saving the clean test loss file into log/new_plot/output_dim2_width1024_layers2_relu_size50_noise1.00e-01_invop0_lr0.005 ...
epoch 95100  training loss: 0.013464163988828659
epoch 95100  clean testing loss: 3.5893871784210205
epoch 95200  training loss: 0.01346413791179657
epoch 95200  clean testing loss: 3.589183807373047
epoch 95300  training loss: 0.013464021496474743
epoch 95300  clean testing loss: 3.58971905708313
epoch 95400  training loss: 0.013463984243571758
epoch 95400  clean testing loss: 3.5892443656921387
epoch 95500  training loss: 0.013463937677443027
epoch 95500  clean testing loss: 3.5895280838012695
epoch 95600  training loss: 0.013463890179991722
epoch 95600  clean testing loss: 3.5897927284240723
epoch 95700  training loss: 0.013463689014315605
epoch 95700  clean testing loss: 3.590087890625
epoch 95800  training loss: 0.013463693670928478
epoch 95800  clean testing loss: 3.5903167724609375
epoch 95900  training loss: 0.013463486917316914

 97%|█████████▋| 96887/100000 [02:55<00:05, 542.51it/s]
epoch 96000  training loss: 0.013463655486702919
epoch 96000  clean testing loss: 3.5893800258636475
Saving the clean test loss file into log/new_plot/output_dim2_width1024_layers2_relu_size50_noise1.00e-01_invop0_lr0.005 ...
epoch 96100  training loss: 0.013463541865348816
epoch 96100  clean testing loss: 3.589327096939087
epoch 96200  training loss: 0.013463239185512066
epoch 96200  clean testing loss: 3.589484691619873
epoch 96300  training loss: 0.013463208451867104
epoch 96300  clean testing loss: 3.58940052986145
epoch 96400  training loss: 0.013463214039802551
epoch 96400  clean testing loss: 3.589508533477783
epoch 96500  training loss: 0.013463103212416172
epoch 96500  clean testing loss: 3.5895445346832275
epoch 96600  training loss: 0.013462932780385017
epoch 96600  clean testing loss: 3.5891900062561035
epoch 96700  training loss: 0.013462928123772144
epoch 96700  clean testing loss: 3.589137554168701
epoch 96800  training loss: 0.013462912291288376
epoch 96800  clean testing loss: 3.589385986328125
epoch 96900  training loss: 0.013462632894515991
epoch 96900  clean testing loss: 3.589498281478882
epoch 97000  training loss: 0.01346278004348278
epoch 97000  clean testing loss: 3.5892815589904785

 98%|█████████▊| 98017/100000 [02:57<00:03, 541.60it/s]
epoch 97100  training loss: 0.013462569564580917
epoch 97100  clean testing loss: 3.5887954235076904
epoch 97200  training loss: 0.013462805189192295
epoch 97200  clean testing loss: 3.5892772674560547
epoch 97300  training loss: 0.013462560251355171
epoch 97300  clean testing loss: 3.588862180709839
epoch 97400  training loss: 0.01346259843558073
epoch 97400  clean testing loss: 3.589801549911499
epoch 97500  training loss: 0.013462779112160206
epoch 97500  clean testing loss: 3.5889620780944824
epoch 97600  training loss: 0.013462354429066181
epoch 97600  clean testing loss: 3.589087963104248
epoch 97700  training loss: 0.013462373055517673
epoch 97700  clean testing loss: 3.588613271713257
epoch 97800  training loss: 0.013462362810969353
epoch 97800  clean testing loss: 3.589292287826538
epoch 97900  training loss: 0.013462132774293423
epoch 97900  clean testing loss: 3.588945150375366
epoch 98000  training loss: 0.01346184965223074
epoch 98000  clean testing loss: 3.588934898376465
Saving the clean test loss file into log/new_plot/output_dim2_width1024_layers2_relu_size50_noise1.00e-01_invop0_lr0.005 ...
epoch 98100  training loss: 0.013462015427649021

 99%|█████████▉| 99092/100000 [02:59<00:01, 550.89it/s]
epoch 98200  training loss: 0.013461969792842865
epoch 98200  clean testing loss: 3.588731050491333
epoch 98300  training loss: 0.013461817987263203
epoch 98300  clean testing loss: 3.5891473293304443
epoch 98400  training loss: 0.013461730442941189
epoch 98400  clean testing loss: 3.588918924331665
epoch 98500  training loss: 0.013461926952004433
epoch 98500  clean testing loss: 3.5895848274230957
epoch 98600  training loss: 0.013461757451295853
epoch 98600  clean testing loss: 3.5887510776519775
epoch 98700  training loss: 0.013461549766361713
epoch 98700  clean testing loss: 3.5888078212738037
epoch 98800  training loss: 0.013461596332490444
epoch 98800  clean testing loss: 3.588914632797241
epoch 98900  training loss: 0.013461467809975147
epoch 98900  clean testing loss: 3.5888218879699707
epoch 99000  training loss: 0.013461489230394363
epoch 99000  clean testing loss: 3.588887929916382
Saving the clean test loss file into log/new_plot/output_dim2_width1024_layers2_relu_size50_noise1.00e-01_invop0_lr0.005 ...
epoch 99100  training loss: 0.013461245223879814
epoch 99100  clean testing loss: 3.58864426612854
epoch 99200  training loss: 0.013461119495332241

100%|██████████| 100000/100000 [03:01<00:00, 551.10it/s]
epoch 99300  training loss: 0.013461165130138397
epoch 99300  clean testing loss: 3.5890355110168457
epoch 99400  training loss: 0.013461148366332054
epoch 99400  clean testing loss: 3.589005947113037
epoch 99500  training loss: 0.013461018912494183
epoch 99500  clean testing loss: 3.5890135765075684
epoch 99600  training loss: 0.013461064547300339
epoch 99600  clean testing loss: 3.588984489440918
epoch 99700  training loss: 0.01346101239323616
epoch 99700  clean testing loss: 3.588841199874878
epoch 99800  training loss: 0.013460793532431126
epoch 99800  clean testing loss: 3.5884275436401367
epoch 99900  training loss: 0.013460874557495117
epoch 99900  clean testing loss: 3.5890932083129883
Saving the clean test loss file into log/new_plot/output_dim2_width1024_layers2_relu_size50_noise1.00e-01_invop0_lr0.005 ...