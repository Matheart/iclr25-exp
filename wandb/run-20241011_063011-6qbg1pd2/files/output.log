
  0%|                                                                     | 104/100000 [00:01<20:48, 79.99it/s]
epoch 0  training loss: 49.04990768432617
epoch 0  clean testing loss: nan
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers2_relu15_size500_noise1.00e+00_invop1_lr5e-05 ...
epoch 100  training loss: nan

  0%|▏                                                                    | 266/100000 [00:03<20:14, 82.14it/s]
epoch 200  training loss: nan

  0%|▎                                                                    | 428/100000 [00:05<20:20, 81.56it/s]
epoch 300  training loss: nan
epoch 300  clean testing loss: nan
epoch 400  training loss: nan

  1%|▍                                                                    | 590/100000 [00:07<20:17, 81.65it/s]
epoch 500  training loss: nan
epoch 500  clean testing loss: nan
epoch 600  training loss: nan

  1%|▌                                                                    | 761/100000 [00:09<20:04, 82.40it/s]
epoch 700  training loss: nan

  1%|▋                                                                    | 923/100000 [00:11<20:07, 82.06it/s]
epoch 800  training loss: nan
epoch 800  clean testing loss: nan
epoch 900  training loss: nan

  1%|▋                                                                   | 1085/100000 [00:13<20:15, 81.38it/s]
epoch 1000  training loss: nan
epoch 1000  clean testing loss: nan
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers2_relu15_size500_noise1.00e+00_invop1_lr5e-05 ...
epoch 1100  training loss: nan

  1%|▊                                                                   | 1247/100000 [00:15<20:11, 81.54it/s]
epoch 1200  training loss: nan
epoch 1200  clean testing loss: nan
epoch 1300  training loss: nan

  1%|▉                                                                   | 1409/100000 [00:17<20:03, 81.93it/s]
epoch 1400  training loss: nan

  2%|█                                                                   | 1580/100000 [00:19<19:49, 82.75it/s]
epoch 1500  training loss: nan
epoch 1500  clean testing loss: nan
epoch 1600  training loss: nan

  2%|█▏                                                                  | 1742/100000 [00:21<19:55, 82.22it/s]
epoch 1700  training loss: nan

  2%|█▎                                                                  | 1904/100000 [00:23<20:05, 81.40it/s]
epoch 1800  training loss: nan
epoch 1800  clean testing loss: nan
epoch 1900  training loss: nan

  2%|█▍                                                                  | 2075/100000 [00:25<19:54, 81.99it/s]
epoch 2000  training loss: nan
epoch 2000  clean testing loss: nan
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers2_relu15_size500_noise1.00e+00_invop1_lr5e-05 ...
epoch 2100  training loss: nan

  2%|█▌                                                                  | 2237/100000 [00:27<19:43, 82.58it/s]
epoch 2200  training loss: nan

  2%|█▋                                                                  | 2399/100000 [00:29<19:40, 82.71it/s]
epoch 2300  training loss: nan
epoch 2300  clean testing loss: nan
epoch 2400  training loss: nan

  3%|█▋                                                                  | 2570/100000 [00:31<19:40, 82.56it/s]
epoch 2500  training loss: nan
epoch 2500  clean testing loss: nan
epoch 2600  training loss: nan

  3%|█▊                                                                  | 2732/100000 [00:33<19:39, 82.45it/s]
epoch 2700  training loss: nan

  3%|█▉                                                                  | 2894/100000 [00:35<19:37, 82.44it/s]
epoch 2800  training loss: nan
epoch 2800  clean testing loss: nan
epoch 2900  training loss: nan

  3%|██                                                                  | 3065/100000 [00:37<19:39, 82.17it/s]
epoch 3000  training loss: nan
epoch 3000  clean testing loss: nan
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers2_relu15_size500_noise1.00e+00_invop1_lr5e-05 ...
epoch 3100  training loss: nan

  3%|██▏                                                                 | 3227/100000 [00:39<19:41, 81.88it/s]
epoch 3200  training loss: nan

  3%|██▎                                                                 | 3389/100000 [00:41<19:28, 82.70it/s]
epoch 3300  training loss: nan
epoch 3300  clean testing loss: nan
epoch 3400  training loss: nan

  4%|██▍                                                                 | 3551/100000 [00:43<19:39, 81.74it/s]
epoch 3500  training loss: nan
epoch 3500  clean testing loss: nan
epoch 3600  training loss: nan
  4%|██▍                                                                 | 3578/100000 [00:43<19:30, 82.39it/s][34m[1mwandb[39m[22m: 429 encountered (Filestream rate limit exceeded, retrying in 2.4 seconds.), retrying request
  4%|██▌                                                                 | 3722/100000 [00:45<19:38, 81.73it/s]
epoch 3700  training loss: nan

  4%|██▋                                                                 | 3884/100000 [00:47<19:29, 82.21it/s]
epoch 3800  training loss: nan
epoch 3800  clean testing loss: nan
epoch 3900  training loss: nan

  4%|██▊                                                                 | 4046/100000 [00:49<19:28, 82.12it/s]
epoch 4000  training loss: nan
epoch 4000  clean testing loss: nan

  4%|██▊                                                                 | 4208/100000 [00:51<19:29, 81.91it/s]
epoch 4100  training loss: nan
epoch 4100  clean testing loss: nan
epoch 4200  training loss: nan

  4%|██▉                                                                 | 4379/100000 [00:53<19:28, 81.82it/s]
epoch 4300  training loss: nan
epoch 4300  clean testing loss: nan
epoch 4400  training loss: nan

  5%|███                                                                 | 4541/100000 [00:55<19:28, 81.67it/s]
epoch 4500  training loss: nan

  5%|███▏                                                                | 4703/100000 [00:57<19:29, 81.50it/s]
epoch 4600  training loss: nan
epoch 4600  clean testing loss: nan
epoch 4700  training loss: nan

  5%|███▎                                                                | 4865/100000 [00:59<19:26, 81.56it/s]
epoch 4800  training loss: nan
epoch 4800  clean testing loss: nan
epoch 4900  training loss: nan

  5%|███▍                                                                | 5036/100000 [01:01<19:26, 81.42it/s]
epoch 5000  training loss: nan
epoch 5000  clean testing loss: nan
  5%|███▍                                                                | 5071/100000 [01:02<19:22, 81.64it/s]
Traceback (most recent call last):
  File "/home/howon/iclr25-exp/nn_exp.py", line 282, in <module>
    losses = closure()
  File "/home/howon/iclr25-exp/nn_exp.py", line 271, in closure
    losses = compute_loss(train_x, train_y, inv_op_power)
  File "/home/howon/iclr25-exp/nn_exp.py", line 249, in compute_loss
    uxx_tem = torch.autograd.grad(ux_tem,train_x,grad_outputs=v,create_graph=True)[0]
  File "/home/howon/.conda/envs/cs552/lib/python3.10/site-packages/torch/autograd/__init__.py", line 412, in grad
    result = _engine_run_backward(
  File "/home/howon/.conda/envs/cs552/lib/python3.10/site-packages/torch/autograd/graph.py", line 744, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt