
  0%|                                                                      | 90/100000 [00:01<20:04, 82.94it/s]
epoch 0  training loss: 48.73348617553711
epoch 0  clean testing loss: 47.228878021240234
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_sin_size500_noise1.00e+00_invop1_lr5e-05 ...
epoch 100  training loss: 1.9516456127166748

  0%|▏                                                                    | 261/100000 [00:03<19:35, 84.85it/s]
epoch 200  training loss: 1.4658610820770264
epoch 200  clean testing loss: 0.4059803783893585
epoch 300  training loss: 1.3077964782714844

  0%|▎                                                                    | 432/100000 [00:05<19:28, 85.25it/s]
epoch 400  training loss: 1.2125147581100464
epoch 400  clean testing loss: 0.23582518100738525
epoch 500  training loss: 1.1351757049560547

  1%|▍                                                                    | 595/100000 [00:07<22:33, 73.47it/s]
epoch 600  training loss: 1.0591723918914795

  1%|▌                                                                    | 739/100000 [00:09<22:56, 72.13it/s]
epoch 700  training loss: 1.00082266330719
epoch 700  clean testing loss: 0.11658980697393417
epoch 800  training loss: 0.9647921919822693

  1%|▌                                                                    | 883/100000 [00:11<22:34, 73.18it/s]
epoch 900  training loss: 0.9402443766593933

  1%|▋                                                                   | 1027/100000 [00:13<22:39, 72.81it/s]
epoch 1000  training loss: 0.9207814335823059
epoch 1000  clean testing loss: 0.1205231249332428

  1%|▊                                                                   | 1179/100000 [00:15<22:35, 72.91it/s]
epoch 1100  training loss: 0.9013639688491821
epoch 1100  clean testing loss: 0.13486875593662262
epoch 1200  training loss: 0.8878991603851318

  1%|▉                                                                   | 1323/100000 [00:17<22:42, 72.43it/s]
epoch 1300  training loss: 0.8804612159729004

  1%|▉                                                                   | 1467/100000 [00:19<22:25, 73.22it/s]
epoch 1400  training loss: 0.8748496174812317
epoch 1400  clean testing loss: 0.17742718756198883
epoch 1500  training loss: 0.870162844657898

  2%|█                                                                   | 1611/100000 [00:21<22:25, 73.13it/s]
epoch 1600  training loss: 0.865749716758728

  2%|█▏                                                                  | 1763/100000 [00:23<22:17, 73.47it/s]
epoch 1700  training loss: 0.8613674640655518
epoch 1700  clean testing loss: 0.21175596117973328
epoch 1800  training loss: 0.8568453192710876

  2%|█▎                                                                  | 1907/100000 [00:25<22:28, 72.72it/s]
epoch 1900  training loss: 0.8513374328613281

  2%|█▍                                                                  | 2051/100000 [00:27<22:26, 72.73it/s]
epoch 2000  training loss: 0.8429034948348999
epoch 2000  clean testing loss: 0.24302206933498383
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_sin_size500_noise1.00e+00_invop1_lr5e-05 ...
epoch 2100  training loss: 0.8325261473655701

  2%|█▍                                                                  | 2195/100000 [00:29<22:37, 72.07it/s]
epoch 2200  training loss: 0.8235141038894653

  2%|█▌                                                                  | 2347/100000 [00:31<22:26, 72.53it/s]
epoch 2300  training loss: 0.8147668838500977
epoch 2300  clean testing loss: 0.25888723134994507
epoch 2400  training loss: 0.8051155209541321

  2%|█▋                                                                  | 2491/100000 [00:33<22:17, 72.89it/s]
epoch 2500  training loss: 0.793782114982605

  3%|█▊                                                                  | 2635/100000 [00:35<22:25, 72.36it/s]
epoch 2600  training loss: 0.7816339731216431

  3%|█▉                                                                  | 2779/100000 [00:37<22:08, 73.15it/s]
epoch 2700  training loss: 0.7703503966331482
epoch 2700  clean testing loss: 0.32910215854644775
epoch 2800  training loss: 0.7555956840515137

  3%|█▉                                                                  | 2931/100000 [00:39<22:19, 72.44it/s]
epoch 2900  training loss: 0.7353731989860535

  3%|██                                                                  | 3075/100000 [00:41<22:20, 72.31it/s]
epoch 3000  training loss: 0.7157928943634033
epoch 3000  clean testing loss: 0.4121656119823456
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_sin_size500_noise1.00e+00_invop1_lr5e-05 ...
epoch 3100  training loss: 0.7016554474830627
  3%|██▏                                                                 | 3203/100000 [00:43<22:11, 72.70it/s][34m[1mwandb[39m[22m: 429 encountered (Filestream rate limit exceeded, retrying in 2.2 seconds.), retrying request
  3%|██▏                                                                 | 3219/100000 [00:43<21:56, 73.49it/s]
epoch 3200  training loss: 0.6896531581878662

  3%|██▎                                                                 | 3363/100000 [00:45<22:10, 72.62it/s]
epoch 3300  training loss: 0.6766544580459595
epoch 3300  clean testing loss: 0.4896790385246277
epoch 3400  training loss: 0.6647190451622009

  4%|██▍                                                                 | 3515/100000 [00:47<22:14, 72.30it/s]
epoch 3500  training loss: 0.6524864435195923

  4%|██▍                                                                 | 3659/100000 [00:49<21:57, 73.12it/s]
epoch 3600  training loss: 0.6383606791496277
epoch 3600  clean testing loss: 0.5882807970046997
epoch 3700  training loss: 0.6222020387649536

  4%|██▌                                                                 | 3803/100000 [00:51<22:13, 72.11it/s]
epoch 3800  training loss: 0.6010242700576782

  4%|██▋                                                                 | 3947/100000 [00:53<21:56, 72.98it/s]
epoch 3900  training loss: 0.5767718553543091
epoch 3900  clean testing loss: 0.7120574116706848
epoch 4000  training loss: 0.5526956915855408
epoch 4000  clean testing loss: 0.794446587562561

  4%|██▊                                                                 | 4099/100000 [00:55<22:01, 72.57it/s]
epoch 4100  training loss: 0.5310160517692566

  4%|██▉                                                                 | 4243/100000 [00:57<21:41, 73.60it/s]
epoch 4200  training loss: 0.5160642862319946

  4%|██▉                                                                 | 4387/100000 [00:59<21:52, 72.88it/s]
epoch 4300  training loss: 0.5140448808670044
epoch 4300  clean testing loss: 1.0180702209472656
epoch 4400  training loss: 0.4892483055591583

  5%|███                                                                 | 4531/100000 [01:01<22:02, 72.16it/s]
epoch 4500  training loss: 0.4693000316619873

  5%|███▏                                                                | 4683/100000 [01:03<21:58, 72.28it/s]
epoch 4600  training loss: 0.45426517724990845
epoch 4600  clean testing loss: 1.1640307903289795
epoch 4700  training loss: 0.4391007721424103

  5%|███▎                                                                | 4827/100000 [01:05<21:26, 74.00it/s]
epoch 4800  training loss: 0.4274594187736511

  5%|███▍                                                                | 4971/100000 [01:07<22:07, 71.60it/s]
epoch 4900  training loss: 0.4170013666152954
epoch 4900  clean testing loss: 1.417587399482727
epoch 5000  training loss: 0.4072946310043335
epoch 5000  clean testing loss: 1.4989796876907349

  5%|███▍                                                                | 5115/100000 [01:09<21:47, 72.56it/s]
epoch 5100  training loss: 0.39812490344047546

  5%|███▌                                                                | 5259/100000 [01:11<21:49, 72.34it/s]
epoch 5200  training loss: 0.38925930857658386
epoch 5200  clean testing loss: 1.654944896697998
epoch 5300  training loss: 0.38031867146492004

  5%|███▋                                                                | 5411/100000 [01:13<21:39, 72.79it/s]
epoch 5400  training loss: 0.37161940336227417

  6%|███▊                                                                | 5555/100000 [01:15<21:36, 72.85it/s]
epoch 5500  training loss: 0.36289656162261963
epoch 5500  clean testing loss: 1.864655613899231
epoch 5600  training loss: 0.3580910861492157

  6%|███▉                                                                | 5699/100000 [01:17<21:40, 72.52it/s]
epoch 5700  training loss: 0.34834957122802734

  6%|███▉                                                                | 5843/100000 [01:19<21:34, 72.73it/s]
epoch 5800  training loss: 0.33718636631965637

  6%|████                                                                | 5987/100000 [01:21<21:45, 72.02it/s]
epoch 5900  training loss: 0.32773247361183167
epoch 5900  clean testing loss: 2.1470437049865723
epoch 6000  training loss: 0.3200596272945404
epoch 6000  clean testing loss: 2.2506396770477295

  6%|████▏                                                               | 6139/100000 [01:23<21:29, 72.81it/s]
epoch 6100  training loss: 0.3109530806541443

  6%|████▎                                                               | 6283/100000 [01:25<21:16, 73.41it/s]
epoch 6200  training loss: 0.30269214510917664
epoch 6200  clean testing loss: 2.490889549255371
epoch 6300  training loss: 0.29796770215034485

  6%|████▎                                                               | 6427/100000 [01:27<21:22, 72.96it/s]
epoch 6400  training loss: 0.28629767894744873

  7%|████▍                                                               | 6571/100000 [01:29<21:23, 72.80it/s]
epoch 6500  training loss: 0.2793683111667633
epoch 6500  clean testing loss: 2.944894313812256
epoch 6600  training loss: 0.27345651388168335

  7%|████▌                                                               | 6723/100000 [01:31<21:13, 73.24it/s]
epoch 6700  training loss: 0.26838183403015137

  7%|████▋                                                               | 6867/100000 [01:33<21:06, 73.51it/s]
epoch 6800  training loss: 0.26379841566085815
epoch 6800  clean testing loss: 3.281700611114502
epoch 6900  training loss: 0.2596134841442108

  7%|████▊                                                               | 7011/100000 [01:35<21:32, 71.95it/s]
epoch 7000  training loss: 0.25711992383003235
epoch 7000  clean testing loss: 3.481576681137085

  7%|████▊                                                               | 7163/100000 [01:37<21:01, 73.59it/s]
epoch 7100  training loss: 0.2528567910194397
epoch 7100  clean testing loss: 3.5537047386169434
epoch 7200  training loss: 0.24793751537799835

  7%|████▉                                                               | 7307/100000 [01:39<21:21, 72.33it/s]
epoch 7300  training loss: 0.24424725770950317

  7%|█████                                                               | 7451/100000 [01:41<21:16, 72.49it/s]
epoch 7400  training loss: 0.2417875975370407
epoch 7400  clean testing loss: 3.8441789150238037
epoch 7500  training loss: 0.23730309307575226

  8%|█████▏                                                              | 7595/100000 [01:43<21:06, 72.99it/s]
epoch 7600  training loss: 0.23469677567481995

  8%|█████▎                                                              | 7747/100000 [01:45<21:14, 72.39it/s]
epoch 7700  training loss: 0.230600506067276

  8%|█████▎                                                              | 7891/100000 [01:47<21:06, 72.73it/s]
epoch 7800  training loss: 0.22733506560325623
epoch 7800  clean testing loss: 4.3737030029296875
epoch 7900  training loss: 0.22606198489665985

  8%|█████▍                                                              | 8035/100000 [01:49<20:58, 73.07it/s]
epoch 8000  training loss: 0.2201552391052246
epoch 8000  clean testing loss: 4.684903621673584

  8%|█████▌                                                              | 8179/100000 [01:51<21:00, 72.84it/s]
epoch 8100  training loss: 0.2187662124633789
epoch 8100  clean testing loss: 4.861923694610596
epoch 8200  training loss: 0.21331852674484253

  8%|█████▋                                                              | 8331/100000 [01:53<20:49, 73.36it/s]
epoch 8300  training loss: 0.21125644445419312

  8%|█████▊                                                              | 8475/100000 [01:55<21:03, 72.46it/s]
epoch 8400  training loss: 0.20769190788269043
epoch 8400  clean testing loss: 5.444220066070557
epoch 8500  training loss: 0.20287849009037018

  9%|█████▊                                                              | 8619/100000 [01:57<21:01, 72.42it/s]
epoch 8600  training loss: 0.19856517016887665

  9%|█████▉                                                              | 8763/100000 [01:59<20:54, 72.72it/s]
epoch 8700  training loss: 0.1962348073720932
epoch 8700  clean testing loss: 6.14835786819458
epoch 8800  training loss: 0.19131086766719818

  9%|██████                                                              | 8915/100000 [02:01<21:07, 71.88it/s]
epoch 8900  training loss: 0.18866541981697083

  9%|██████▏                                                             | 9059/100000 [02:03<20:34, 73.67it/s]
epoch 9000  training loss: 0.18709397315979004
epoch 9000  clean testing loss: 6.976248264312744
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_sin_size500_noise1.00e+00_invop1_lr5e-05 ...
epoch 9100  training loss: 0.18003864586353302

  9%|██████▎                                                             | 9203/100000 [02:05<20:56, 72.27it/s]
epoch 9200  training loss: 0.17662350833415985

  9%|██████▎                                                             | 9355/100000 [02:07<20:47, 72.68it/s]
epoch 9300  training loss: 0.17301101982593536

  9%|██████▍                                                             | 9499/100000 [02:09<20:48, 72.50it/s]
epoch 9400  training loss: 0.16937921941280365
epoch 9400  clean testing loss: 8.110122680664062
epoch 9500  training loss: 0.17308825254440308

 10%|██████▌                                                             | 9643/100000 [02:11<20:37, 73.00it/s]
epoch 9600  training loss: 0.1631370335817337

 10%|██████▋                                                             | 9787/100000 [02:13<20:54, 71.91it/s]
epoch 9700  training loss: 0.15871213376522064
epoch 9700  clean testing loss: 9.047887802124023
epoch 9800  training loss: 0.1558338850736618

 10%|██████▊                                                             | 9931/100000 [02:15<20:37, 72.77it/s]
epoch 9900  training loss: 0.1517062485218048

 10%|██████▊                                                            | 10075/100000 [02:17<20:50, 71.94it/s]
epoch 10000  training loss: 0.15397854149341583
epoch 10000  clean testing loss: 10.04616641998291
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_sin_size500_noise1.00e+00_invop1_lr5e-05 ...
epoch 10100  training loss: 0.14625495672225952

 10%|██████▊                                                            | 10227/100000 [02:19<20:46, 72.01it/s]
epoch 10200  training loss: 0.14285889267921448

 10%|██████▉                                                            | 10371/100000 [02:21<20:35, 72.55it/s]
epoch 10300  training loss: 0.139032781124115
epoch 10300  clean testing loss: 11.022686004638672
epoch 10400  training loss: 0.1538628339767456

 11%|███████                                                            | 10515/100000 [02:23<20:45, 71.85it/s]
epoch 10500  training loss: 0.1447789967060089

 11%|███████▏                                                           | 10659/100000 [02:25<20:31, 72.56it/s]
epoch 10600  training loss: 0.13114678859710693
epoch 10600  clean testing loss: 12.033918380737305
epoch 10700  training loss: 0.1287229061126709

 11%|███████▏                                                           | 10803/100000 [02:27<20:21, 73.01it/s]
epoch 10800  training loss: 0.12867377698421478

 11%|███████▎                                                           | 10955/100000 [02:29<20:21, 72.93it/s]
epoch 10900  training loss: 0.12464426457881927

 11%|███████▍                                                           | 11099/100000 [02:31<20:33, 72.07it/s]
epoch 11000  training loss: 0.12257572263479233
epoch 11000  clean testing loss: 13.417144775390625
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_sin_size500_noise1.00e+00_invop1_lr5e-05 ...
epoch 11100  training loss: 0.12117606401443481

 11%|███████▌                                                           | 11243/100000 [02:33<20:16, 72.94it/s]
epoch 11200  training loss: 0.11911874264478683

 11%|███████▋                                                           | 11387/100000 [02:35<20:14, 72.96it/s]
epoch 11300  training loss: 0.11744792014360428
epoch 11300  clean testing loss: 14.491639137268066
epoch 11400  training loss: 0.11518977582454681

 12%|███████▋                                                           | 11539/100000 [02:37<20:11, 73.03it/s]
epoch 11500  training loss: 0.11784932762384415

 12%|███████▊                                                           | 11683/100000 [02:39<20:26, 71.99it/s]
epoch 11600  training loss: 0.11201060563325882
epoch 11600  clean testing loss: 15.547471046447754
epoch 11700  training loss: 0.11056365072727203

 12%|███████▉                                                           | 11827/100000 [02:41<20:15, 72.55it/s]
epoch 11800  training loss: 0.10903243720531464

 12%|████████                                                           | 11971/100000 [02:43<20:07, 72.91it/s]
epoch 11900  training loss: 0.1122938022017479
epoch 11900  clean testing loss: 16.630266189575195
epoch 12000  training loss: 0.10850673168897629
epoch 12000  clean testing loss: 16.953462600708008

 12%|████████                                                           | 12123/100000 [02:46<20:15, 72.29it/s]
epoch 12100  training loss: 0.10516548156738281

 12%|████████▏                                                          | 12267/100000 [02:47<20:10, 72.49it/s]
epoch 12200  training loss: 0.1040283590555191
epoch 12200  clean testing loss: 17.556568145751953
epoch 12300  training loss: 0.10284372419118881

 12%|████████▎                                                          | 12411/100000 [02:49<19:52, 73.45it/s]
epoch 12400  training loss: 0.10161091387271881

 13%|████████▍                                                          | 12555/100000 [02:51<20:00, 72.85it/s]
epoch 12500  training loss: 0.10103487223386765

 13%|████████▌                                                          | 12707/100000 [02:54<19:52, 73.23it/s]
epoch 12600  training loss: 0.09915578365325928
epoch 12600  clean testing loss: 18.947317123413086
epoch 12700  training loss: 0.09797631949186325

 13%|████████▌                                                          | 12851/100000 [02:56<20:00, 72.62it/s]
epoch 12800  training loss: 0.09680823236703873

 13%|████████▋                                                          | 12995/100000 [02:57<19:50, 73.08it/s]
epoch 12900  training loss: 0.09557175636291504
epoch 12900  clean testing loss: 20.0599422454834
epoch 13000  training loss: 0.09772390872240067
epoch 13000  clean testing loss: 20.4337215423584

 13%|████████▊                                                          | 13139/100000 [02:59<19:58, 72.47it/s]
epoch 13100  training loss: 0.09339608252048492

 13%|████████▉                                                          | 13291/100000 [03:02<19:51, 72.79it/s]
epoch 13200  training loss: 0.09213887155056
epoch 13200  clean testing loss: 21.157432556152344
epoch 13300  training loss: 0.09096399694681168

 13%|█████████                                                          | 13435/100000 [03:04<19:50, 72.71it/s]
epoch 13400  training loss: 0.09254492819309235

 14%|█████████                                                          | 13579/100000 [03:06<20:00, 71.99it/s]
epoch 13500  training loss: 0.09373842179775238
epoch 13500  clean testing loss: 22.355520248413086
epoch 13600  training loss: 0.0878954529762268

 14%|█████████▏                                                         | 13723/100000 [03:08<19:52, 72.38it/s]
epoch 13700  training loss: 0.0869579017162323

 14%|█████████▎                                                         | 13867/100000 [03:09<19:46, 72.58it/s]
epoch 13800  training loss: 0.08583305776119232
epoch 13800  clean testing loss: 23.474700927734375
epoch 13900  training loss: 0.0848948284983635

 14%|█████████▍                                                         | 14019/100000 [03:12<19:34, 73.20it/s]
epoch 14000  training loss: 0.085024394094944
epoch 14000  clean testing loss: 24.234533309936523

 14%|█████████▍                                                         | 14163/100000 [03:14<19:43, 72.51it/s]
epoch 14100  training loss: 0.08294717967510223
epoch 14100  clean testing loss: 24.645145416259766
epoch 14200  training loss: 0.08191736787557602

 14%|█████████▌                                                         | 14307/100000 [03:16<19:34, 72.99it/s]
epoch 14300  training loss: 0.08123040944337845

 14%|█████████▋                                                         | 14451/100000 [03:18<19:31, 73.05it/s]
epoch 14400  training loss: 0.0799880102276802

 15%|█████████▊                                                         | 14603/100000 [03:20<19:47, 71.94it/s]
epoch 14500  training loss: 0.0798095315694809
epoch 14500  clean testing loss: 26.168384552001953
epoch 14600  training loss: 0.07866089791059494

 15%|█████████▉                                                         | 14747/100000 [03:22<19:28, 72.97it/s]
epoch 14700  training loss: 0.07764561474323273

 15%|█████████▉                                                         | 14891/100000 [03:24<19:35, 72.40it/s]
epoch 14800  training loss: 0.07665588706731796
epoch 14800  clean testing loss: 27.337112426757812
epoch 14900  training loss: 0.0756501704454422

 15%|██████████                                                         | 15035/100000 [03:26<19:28, 72.70it/s]
epoch 15000  training loss: 0.08127991110086441
epoch 15000  clean testing loss: 28.064361572265625

 15%|██████████▏                                                        | 15187/100000 [03:28<19:29, 72.53it/s]
epoch 15100  training loss: 0.07421126216650009
epoch 15100  clean testing loss: 28.318735122680664
epoch 15200  training loss: 0.07355216145515442

 15%|██████████▎                                                        | 15331/100000 [03:30<19:27, 72.54it/s]
epoch 15300  training loss: 0.07287118583917618

 15%|██████████▎                                                        | 15475/100000 [03:32<19:21, 72.76it/s]
epoch 15400  training loss: 0.0721682608127594
epoch 15400  clean testing loss: 29.264148712158203
epoch 15500  training loss: 0.0715145394206047

 16%|██████████▍                                                        | 15619/100000 [03:34<19:16, 72.98it/s]
epoch 15600  training loss: 0.07080235332250595

 16%|██████████▌                                                        | 15771/100000 [03:36<19:23, 72.41it/s]
epoch 15700  training loss: 0.07014106959104538
epoch 15700  clean testing loss: 30.188217163085938
epoch 15800  training loss: 0.06951618939638138

 16%|██████████▋                                                        | 15915/100000 [03:38<19:24, 72.22it/s]
epoch 15900  training loss: 0.06894326955080032

 16%|██████████▊                                                        | 16059/100000 [03:40<18:55, 73.95it/s]
epoch 16000  training loss: 0.06831886619329453
epoch 16000  clean testing loss: 30.982500076293945

 16%|██████████▊                                                        | 16203/100000 [03:42<19:19, 72.24it/s]
epoch 16100  training loss: 0.06771864742040634
epoch 16100  clean testing loss: 31.24964141845703
epoch 16200  training loss: 0.06809857487678528

 16%|██████████▉                                                        | 16347/100000 [03:44<19:08, 72.85it/s]
epoch 16300  training loss: 0.06654208153486252

 16%|███████████                                                        | 16499/100000 [03:46<19:06, 72.85it/s]
epoch 16400  training loss: 0.06611136347055435
epoch 16400  clean testing loss: 31.95340919494629
epoch 16500  training loss: 0.06648804247379303

 17%|███████████▏                                                       | 16643/100000 [03:48<19:06, 72.69it/s]
epoch 16600  training loss: 0.06491544097661972

 17%|███████████▏                                                       | 16787/100000 [03:50<19:09, 72.39it/s]
epoch 16700  training loss: 0.06464934349060059
epoch 16700  clean testing loss: 32.5968017578125
epoch 16800  training loss: 0.06438373774290085

 17%|███████████▎                                                       | 16931/100000 [03:52<19:04, 72.58it/s]
epoch 16900  training loss: 0.0636160671710968

 17%|███████████▍                                                       | 17083/100000 [03:54<18:59, 72.74it/s]
epoch 17000  training loss: 0.06354866921901703
epoch 17000  clean testing loss: 33.1846923828125
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_sin_size500_noise1.00e+00_invop1_lr5e-05 ...
epoch 17100  training loss: 0.06273917853832245

 17%|███████████▌                                                       | 17227/100000 [03:56<18:50, 73.20it/s]
epoch 17200  training loss: 0.06212441995739937

 17%|███████████▋                                                       | 17371/100000 [03:58<18:47, 73.30it/s]
epoch 17300  training loss: 0.06165895238518715
epoch 17300  clean testing loss: 33.661109924316406
epoch 17400  training loss: 0.0657850131392479

 18%|███████████▋                                                       | 17515/100000 [04:00<18:52, 72.84it/s]
epoch 17500  training loss: 0.06070810928940773

 18%|███████████▊                                                       | 17667/100000 [04:02<18:40, 73.50it/s]
epoch 17600  training loss: 0.06054588407278061

 18%|███████████▉                                                       | 17811/100000 [04:04<18:35, 73.65it/s]
epoch 17700  training loss: 0.06246430054306984
epoch 17700  clean testing loss: 34.26163864135742
epoch 17800  training loss: 0.05953019857406616

 18%|████████████                                                       | 17955/100000 [04:06<18:50, 72.57it/s]
epoch 17900  training loss: 0.05958254635334015

 18%|████████████▏                                                      | 18099/100000 [04:08<18:43, 72.87it/s]
epoch 18000  training loss: 0.058724142611026764
epoch 18000  clean testing loss: 34.65865707397461
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_sin_size500_noise1.00e+00_invop1_lr5e-05 ...
epoch 18100  training loss: 0.058146554976701736

 18%|████████████▏                                                      | 18251/100000 [04:10<18:34, 73.38it/s]
epoch 18200  training loss: 0.05779958516359329

 18%|████████████▎                                                      | 18395/100000 [04:12<18:27, 73.68it/s]
epoch 18300  training loss: 0.05743851512670517
epoch 18300  clean testing loss: 35.01938247680664
epoch 18400  training loss: 0.057064179331064224

 19%|████████████▍                                                      | 18539/100000 [04:14<18:45, 72.41it/s]
epoch 18500  training loss: 0.05669916421175003

 19%|████████████▌                                                      | 18691/100000 [04:16<18:38, 72.69it/s]
epoch 18600  training loss: 0.0564594604074955
epoch 18600  clean testing loss: 35.343997955322266
epoch 18700  training loss: 0.056373681873083115

 19%|████████████▌                                                      | 18835/100000 [04:18<18:32, 72.96it/s]
epoch 18800  training loss: 0.05592254921793938

 19%|████████████▋                                                      | 18979/100000 [04:20<18:43, 72.10it/s]
epoch 18900  training loss: 0.055310819298028946
epoch 18900  clean testing loss: 35.66038513183594
epoch 19000  training loss: 0.055484432727098465
epoch 19000  clean testing loss: 35.77159118652344

 19%|████████████▊                                                      | 19123/100000 [04:22<18:37, 72.40it/s]
epoch 19100  training loss: 0.054821211844682693

 19%|████████████▉                                                      | 19235/100000 [04:23<18:31, 72.66it/s]
epoch 19200  training loss: 0.054535068571567535

 19%|████████████▉                                                      | 19379/100000 [04:25<18:31, 72.51it/s]
epoch 19300  training loss: 0.05502677336335182
epoch 19300  clean testing loss: 36.042964935302734
epoch 19400  training loss: 0.05383120849728584

 20%|█████████████                                                      | 19523/100000 [04:27<18:30, 72.49it/s]
epoch 19500  training loss: 0.05346681550145149

 20%|█████████████▏                                                     | 19667/100000 [04:29<18:31, 72.27it/s]
epoch 19600  training loss: 0.05304992198944092
epoch 19600  clean testing loss: 36.31167984008789
epoch 19700  training loss: 0.054535966366529465

 20%|█████████████▎                                                     | 19819/100000 [04:31<18:20, 72.84it/s]
epoch 19800  training loss: 0.05251621454954147

 20%|█████████████▍                                                     | 19963/100000 [04:33<18:11, 73.31it/s]
epoch 19900  training loss: 0.05334152653813362
epoch 19900  clean testing loss: 36.58065414428711
epoch 20000  training loss: 0.05223297327756882
epoch 20000  clean testing loss: 36.66094970703125

 20%|█████████████▍                                                     | 20107/100000 [04:35<18:21, 72.56it/s]
epoch 20100  training loss: 0.051446545869112015

 20%|█████████████▌                                                     | 20259/100000 [04:37<18:14, 72.82it/s]
epoch 20200  training loss: 0.05124432593584061
epoch 20200  clean testing loss: 36.80919647216797
epoch 20300  training loss: 0.05217950791120529

 20%|█████████████▋                                                     | 20403/100000 [04:39<18:15, 72.63it/s]
epoch 20400  training loss: 0.050707824528217316

 21%|█████████████▊                                                     | 20547/100000 [04:41<18:03, 73.34it/s]
epoch 20500  training loss: 0.05100921913981438
epoch 20500  clean testing loss: 37.03444290161133
epoch 20600  training loss: 0.05035370960831642

 21%|█████████████▊                                                     | 20699/100000 [04:43<18:13, 72.50it/s]
epoch 20700  training loss: 0.05031528323888779

 21%|█████████████▉                                                     | 20843/100000 [04:45<18:06, 72.83it/s]
epoch 20800  training loss: 0.05139218270778656
epoch 20800  clean testing loss: 37.33330154418945
epoch 20900  training loss: 0.04908900335431099

 21%|██████████████                                                     | 20987/100000 [04:47<18:08, 72.59it/s]
epoch 21000  training loss: 0.048760201781988144
epoch 21000  clean testing loss: 37.43352508544922

 21%|██████████████▏                                                    | 21131/100000 [04:49<18:03, 72.80it/s]
epoch 21100  training loss: 0.04851669445633888

 21%|██████████████▎                                                    | 21283/100000 [04:51<18:05, 72.55it/s]
epoch 21200  training loss: 0.04827882722020149
epoch 21200  clean testing loss: 37.58316421508789
epoch 21300  training loss: 0.048030104488134384

 21%|██████████████▎                                                    | 21427/100000 [04:53<17:54, 73.12it/s]
epoch 21400  training loss: 0.04777104780077934

 22%|██████████████▍                                                    | 21571/100000 [04:55<17:54, 73.00it/s]
epoch 21500  training loss: 0.047545649111270905
epoch 21500  clean testing loss: 37.809425354003906
epoch 21600  training loss: 0.04725116118788719

 22%|██████████████▌                                                    | 21715/100000 [04:57<17:59, 72.54it/s]
epoch 21700  training loss: 0.04699061065912247

 22%|██████████████▋                                                    | 21867/100000 [04:59<17:41, 73.57it/s]
epoch 21800  training loss: 0.04673188179731369
epoch 21800  clean testing loss: 38.051307678222656
epoch 21900  training loss: 0.04672626778483391

 22%|██████████████▋                                                    | 22011/100000 [05:01<17:50, 72.84it/s]
epoch 22000  training loss: 0.0462690033018589
epoch 22000  clean testing loss: 38.19828414916992

 22%|██████████████▊                                                    | 22155/100000 [05:03<17:46, 72.98it/s]
epoch 22100  training loss: 0.04655075818300247
epoch 22100  clean testing loss: 38.265472412109375
epoch 22200  training loss: 0.04860920086503029

 22%|██████████████▉                                                    | 22307/100000 [05:05<17:38, 73.41it/s]
epoch 22300  training loss: 0.046092867851257324

 22%|███████████████                                                    | 22451/100000 [05:07<17:36, 73.39it/s]
epoch 22400  training loss: 0.04527362436056137
epoch 22400  clean testing loss: 38.52155685424805
epoch 22500  training loss: 0.0461517870426178

 23%|███████████████▏                                                   | 22595/100000 [05:09<17:41, 72.89it/s]
epoch 22600  training loss: 0.044782720506191254

 23%|███████████████▏                                                   | 22739/100000 [05:11<17:41, 72.81it/s]
epoch 22700  training loss: 0.04850216209888458

 23%|███████████████▎                                                   | 22883/100000 [05:13<17:39, 72.82it/s]
epoch 22800  training loss: 0.04428495094180107
epoch 22800  clean testing loss: 38.84113693237305
epoch 22900  training loss: 0.04407995939254761

 23%|███████████████▍                                                   | 23035/100000 [05:15<17:31, 73.23it/s]
epoch 23000  training loss: 0.044525425881147385
epoch 23000  clean testing loss: 38.99632263183594

 23%|███████████████▌                                                   | 23179/100000 [05:17<17:27, 73.33it/s]
epoch 23100  training loss: 0.04359360784292221
epoch 23100  clean testing loss: 39.08387756347656
epoch 23200  training loss: 0.04459282010793686

 23%|███████████████▋                                                   | 23323/100000 [05:19<17:35, 72.64it/s]
epoch 23300  training loss: 0.0431455634534359

 23%|███████████████▋                                                   | 23467/100000 [05:21<17:37, 72.40it/s]
epoch 23400  training loss: 0.042977746576070786
epoch 23400  clean testing loss: 39.358829498291016
epoch 23500  training loss: 0.0426953062415123

 24%|███████████████▊                                                   | 23619/100000 [05:23<17:27, 72.89it/s]
epoch 23600  training loss: 0.04303798824548721

 24%|███████████████▉                                                   | 23763/100000 [05:25<17:21, 73.22it/s]
epoch 23700  training loss: 0.04231174662709236
epoch 23700  clean testing loss: 39.60716247558594
epoch 23800  training loss: 0.04214992746710777

 24%|████████████████                                                   | 23907/100000 [05:27<17:22, 72.97it/s]
epoch 23900  training loss: 0.04188396781682968

 24%|████████████████                                                   | 24059/100000 [05:30<17:18, 73.13it/s]
epoch 24000  training loss: 0.042330022901296616
epoch 24000  clean testing loss: 39.86607360839844
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_sin_size500_noise1.00e+00_invop1_lr5e-05 ...
epoch 24100  training loss: 0.04140610620379448

 24%|████████████████▏                                                  | 24203/100000 [05:32<17:30, 72.15it/s]
epoch 24200  training loss: 0.04122206196188927

 24%|████████████████▎                                                  | 24347/100000 [05:33<17:26, 72.26it/s]
epoch 24300  training loss: 0.04103035107254982
epoch 24300  clean testing loss: 40.123504638671875
epoch 24400  training loss: 0.04082991182804108

 24%|████████████████▍                                                  | 24491/100000 [05:35<17:04, 73.71it/s]
epoch 24500  training loss: 0.04092762991786003

 25%|████████████████▌                                                  | 24635/100000 [05:37<17:25, 72.08it/s]
epoch 24600  training loss: 0.04043423756957054

 25%|████████████████▌                                                  | 24787/100000 [05:40<17:08, 73.15it/s]
epoch 24700  training loss: 0.04054350033402443
epoch 24700  clean testing loss: 40.50154113769531
epoch 24800  training loss: 0.04183587804436684

 25%|████████████████▋                                                  | 24931/100000 [05:42<17:15, 72.52it/s]
epoch 24900  training loss: 0.03990556672215462

 25%|████████████████▊                                                  | 25075/100000 [05:44<17:05, 73.05it/s]
epoch 25000  training loss: 0.03965475782752037
epoch 25000  clean testing loss: 40.77413558959961
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_sin_size500_noise1.00e+00_invop1_lr5e-05 ...
epoch 25100  training loss: 0.03952169045805931

 25%|████████████████▉                                                  | 25219/100000 [05:45<17:08, 72.68it/s]
epoch 25200  training loss: 0.03928271308541298

 25%|████████████████▉                                                  | 25371/100000 [05:48<17:10, 72.39it/s]
epoch 25300  training loss: 0.039914973080158234
epoch 25300  clean testing loss: 41.12263488769531
epoch 25400  training loss: 0.039307694882154465

 26%|█████████████████                                                  | 25515/100000 [05:50<16:56, 73.27it/s]
epoch 25500  training loss: 0.038726747035980225

 26%|█████████████████▏                                                 | 25659/100000 [05:52<16:55, 73.20it/s]
epoch 25600  training loss: 0.03862330690026283
epoch 25600  clean testing loss: 41.34872055053711
epoch 25700  training loss: 0.03844649717211723

 26%|█████████████████▎                                                 | 25811/100000 [05:54<16:50, 73.43it/s]
epoch 25800  training loss: 0.038204677402973175

 26%|█████████████████▍                                                 | 25955/100000 [05:56<17:15, 71.47it/s]
epoch 25900  training loss: 0.03843885287642479
epoch 25900  clean testing loss: 41.65822982788086
epoch 26000  training loss: 0.038952410221099854
epoch 26000  clean testing loss: 41.690391540527344

 26%|█████████████████▍                                                 | 26099/100000 [05:58<16:51, 73.04it/s]
epoch 26100  training loss: 0.03764461725950241

 26%|█████████████████▌                                                 | 26243/100000 [06:00<16:50, 72.96it/s]
epoch 26200  training loss: 0.037531401962041855

 26%|█████████████████▋                                                 | 26395/100000 [06:02<16:47, 73.07it/s]
epoch 26300  training loss: 0.03731187433004379
epoch 26300  clean testing loss: 42.04375457763672
epoch 26400  training loss: 0.0371956042945385

 27%|█████████████████▊                                                 | 26539/100000 [06:04<16:40, 73.43it/s]
epoch 26500  training loss: 0.03717673569917679

 27%|█████████████████▉                                                 | 26683/100000 [06:06<16:44, 72.97it/s]
epoch 26600  training loss: 0.037861716002225876
epoch 26600  clean testing loss: 42.344051361083984
epoch 26700  training loss: 0.0366462804377079

 27%|█████████████████▉                                                 | 26835/100000 [06:08<16:58, 71.82it/s]
epoch 26800  training loss: 0.036736324429512024

 27%|██████████████████                                                 | 26979/100000 [06:10<16:50, 72.30it/s]
epoch 26900  training loss: 0.03639253228902817

 27%|██████████████████▏                                                | 27123/100000 [06:12<16:45, 72.49it/s]
epoch 27000  training loss: 0.03625824302434921
epoch 27000  clean testing loss: 42.71102523803711
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_sin_size500_noise1.00e+00_invop1_lr5e-05 ...
epoch 27100  training loss: 0.03598363697528839

 27%|██████████████████▎                                                | 27267/100000 [06:14<16:43, 72.50it/s]
epoch 27200  training loss: 0.035848092287778854

 27%|██████████████████▎                                                | 27411/100000 [06:16<16:37, 72.80it/s]
epoch 27300  training loss: 0.03570665046572685
epoch 27300  clean testing loss: 42.97924041748047
epoch 27400  training loss: 0.035559095442295074

 28%|██████████████████▍                                                | 27555/100000 [06:18<16:36, 72.66it/s]
epoch 27500  training loss: 0.03555400297045708

 28%|██████████████████▌                                                | 27707/100000 [06:20<16:28, 73.11it/s]
epoch 27600  training loss: 0.03530387952923775
epoch 27600  clean testing loss: 43.26205825805664
epoch 27700  training loss: 0.0351594015955925

 28%|██████████████████▋                                                | 27851/100000 [06:22<16:22, 73.45it/s]
epoch 27800  training loss: 0.034975629299879074

 28%|██████████████████▊                                                | 27995/100000 [06:24<16:43, 71.76it/s]
epoch 27900  training loss: 0.03592274338006973
epoch 27900  clean testing loss: 43.48286819458008
epoch 28000  training loss: 0.034713342785835266
epoch 28000  clean testing loss: 43.61676788330078

 28%|██████████████████▊                                                | 28139/100000 [06:26<16:17, 73.54it/s]
epoch 28100  training loss: 0.03476661816239357

 28%|██████████████████▉                                                | 28291/100000 [06:28<16:22, 73.01it/s]
epoch 28200  training loss: 0.03441770002245903

 28%|███████████████████                                                | 28435/100000 [06:30<16:33, 72.00it/s]
epoch 28300  training loss: 0.034276559948921204
epoch 28300  clean testing loss: 43.89232635498047
epoch 28400  training loss: 0.034973468631505966

 29%|███████████████████▏                                               | 28579/100000 [06:32<16:25, 72.44it/s]
epoch 28500  training loss: 0.034110698848962784

 29%|███████████████████▏                                               | 28723/100000 [06:34<16:12, 73.30it/s]
epoch 28600  training loss: 0.03516703099012375
epoch 28600  clean testing loss: 44.19226837158203
epoch 28700  training loss: 0.033779047429561615

 29%|███████████████████▎                                               | 28875/100000 [06:36<16:23, 72.29it/s]
epoch 28800  training loss: 0.033730339258909225

 29%|███████████████████▍                                               | 29019/100000 [06:38<16:27, 71.92it/s]
epoch 28900  training loss: 0.03346084803342819
epoch 28900  clean testing loss: 44.37124252319336
epoch 29000  training loss: 0.033344388008117676
epoch 29000  clean testing loss: 44.45207214355469

 29%|███████████████████▌                                               | 29163/100000 [06:40<16:19, 72.35it/s]
epoch 29100  training loss: 0.033796634525060654

 29%|███████████████████▋                                               | 29307/100000 [06:42<16:23, 71.87it/s]
epoch 29200  training loss: 0.03314921632409096
epoch 29200  clean testing loss: 44.597999572753906
epoch 29300  training loss: 0.03319154679775238

 29%|███████████████████▋                                               | 29451/100000 [06:44<16:07, 72.89it/s]
epoch 29400  training loss: 0.03295949846506119

 30%|███████████████████▊                                               | 29595/100000 [06:46<16:03, 73.04it/s]
epoch 29500  training loss: 0.033036697655916214
epoch 29500  clean testing loss: 44.81745529174805
epoch 29600  training loss: 0.032569773495197296

 30%|███████████████████▉                                               | 29747/100000 [06:48<16:10, 72.39it/s]
epoch 29700  training loss: 0.032971374690532684

 30%|████████████████████                                               | 29891/100000 [06:50<15:57, 73.23it/s]
epoch 29800  training loss: 0.032357051968574524

 30%|████████████████████                                               | 30035/100000 [06:52<15:55, 73.25it/s]
epoch 29900  training loss: 0.0323120579123497
epoch 29900  clean testing loss: 45.071929931640625
epoch 30000  training loss: 0.03212137892842293
epoch 30000  clean testing loss: 45.152923583984375

 30%|████████████████████▏                                              | 30179/100000 [06:54<15:52, 73.30it/s]
epoch 30100  training loss: 0.031977713108062744

 30%|████████████████████▎                                              | 30331/100000 [06:56<15:57, 72.77it/s]
epoch 30200  training loss: 0.03187362477183342
epoch 30200  clean testing loss: 45.271080017089844
epoch 30300  training loss: 0.03176582232117653

 30%|████████████████████▍                                              | 30475/100000 [06:58<15:52, 73.02it/s]
epoch 30400  training loss: 0.03165215998888016

 31%|████████████████████▌                                              | 30619/100000 [07:00<15:54, 72.66it/s]
epoch 30500  training loss: 0.03153681010007858
epoch 30500  clean testing loss: 45.45301055908203
epoch 30600  training loss: 0.03186391666531563

 31%|████████████████████▌                                              | 30771/100000 [07:02<15:53, 72.59it/s]
epoch 30700  training loss: 0.031314633786678314

 31%|████████████████████▋                                              | 30915/100000 [07:04<15:42, 73.29it/s]
epoch 30800  training loss: 0.03119654953479767
epoch 30800  clean testing loss: 45.616241455078125
epoch 30900  training loss: 0.031118810176849365

 31%|████████████████████▊                                              | 31059/100000 [07:06<15:48, 72.72it/s]
epoch 31000  training loss: 0.031209532171487808
epoch 31000  clean testing loss: 45.7441520690918

 31%|████████████████████▉                                              | 31211/100000 [07:08<15:45, 72.78it/s]
epoch 31100  training loss: 0.030948612838983536
epoch 31100  clean testing loss: 45.78618240356445
epoch 31200  training loss: 0.03077300265431404

 31%|█████████████████████                                              | 31355/100000 [07:10<15:46, 72.56it/s]
epoch 31300  training loss: 0.03095391020178795

 31%|█████████████████████                                              | 31499/100000 [07:12<15:46, 72.39it/s]
epoch 31400  training loss: 0.0305140670388937
epoch 31400  clean testing loss: 45.901954650878906
epoch 31500  training loss: 0.03152304142713547

 32%|█████████████████████▏                                             | 31643/100000 [07:14<15:33, 73.21it/s]
epoch 31600  training loss: 0.030367806553840637

 32%|█████████████████████▎                                             | 31787/100000 [07:16<15:40, 72.51it/s]
epoch 31700  training loss: 0.030195925384759903
epoch 31700  clean testing loss: 46.02126693725586
epoch 31800  training loss: 0.030081167817115784

 32%|█████████████████████▍                                             | 31939/100000 [07:18<15:34, 72.86it/s]
epoch 31900  training loss: 0.030614834278821945

 32%|█████████████████████▍                                             | 32083/100000 [07:20<15:37, 72.41it/s]
epoch 32000  training loss: 0.029872119426727295
epoch 32000  clean testing loss: 46.13142776489258

 32%|█████████████████████▌                                             | 32227/100000 [07:22<15:30, 72.84it/s]
epoch 32100  training loss: 0.029776593670248985
epoch 32100  clean testing loss: 46.16240692138672
epoch 32200  training loss: 0.02966313250362873

 32%|█████████████████████▋                                             | 32371/100000 [07:24<15:33, 72.47it/s]
epoch 32300  training loss: 0.02956179901957512

 33%|█████████████████████▊                                             | 32523/100000 [07:26<15:26, 72.83it/s]
epoch 32400  training loss: 0.029447464272379875
epoch 32400  clean testing loss: 46.251930236816406
epoch 32500  training loss: 0.029473798349499702

 33%|█████████████████████▉                                             | 32659/100000 [07:28<15:35, 71.95it/s]
epoch 32600  training loss: 0.029247021302580833

 33%|█████████████████████▉                                             | 32811/100000 [07:30<15:33, 71.95it/s]
epoch 32700  training loss: 0.029477903619408607
epoch 32700  clean testing loss: 46.35733413696289
epoch 32800  training loss: 0.029522590339183807

 33%|██████████████████████                                             | 32955/100000 [07:32<15:17, 73.09it/s]
epoch 32900  training loss: 0.028949743136763573

 33%|██████████████████████▏                                            | 33099/100000 [07:34<15:21, 72.61it/s]
epoch 33000  training loss: 0.029301322996616364
epoch 33000  clean testing loss: 46.401283264160156
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_sin_size500_noise1.00e+00_invop1_lr5e-05 ...
epoch 33100  training loss: 0.02873639203608036

 33%|██████████████████████▎                                            | 33251/100000 [07:36<15:11, 73.24it/s]
epoch 33200  training loss: 0.028648877516388893

 33%|██████████████████████▎                                            | 33395/100000 [07:38<15:17, 72.60it/s]
epoch 33300  training loss: 0.02855767123401165

 34%|██████████████████████▍                                            | 33539/100000 [07:40<15:08, 73.14it/s]
epoch 33400  training loss: 0.02846187725663185
epoch 33400  clean testing loss: 46.465301513671875
epoch 33500  training loss: 0.02908967435359955

 34%|██████████████████████▌                                            | 33691/100000 [07:42<15:10, 72.86it/s]
epoch 33600  training loss: 0.028312498703598976

 34%|██████████████████████▋                                            | 33835/100000 [07:44<15:09, 72.77it/s]
epoch 33700  training loss: 0.02816484495997429
epoch 33700  clean testing loss: 46.50572204589844
epoch 33800  training loss: 0.028084855526685715

 34%|██████████████████████▊                                            | 33979/100000 [07:46<15:16, 72.07it/s]
epoch 33900  training loss: 0.028039276599884033

 34%|██████████████████████▊                                            | 34123/100000 [07:48<15:03, 72.95it/s]
epoch 34000  training loss: 0.027886291965842247
epoch 34000  clean testing loss: 46.5319709777832
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_sin_size500_noise1.00e+00_invop1_lr5e-05 ...
epoch 34100  training loss: 0.027766890823841095

 34%|██████████████████████▉                                            | 34267/100000 [07:50<15:06, 72.51it/s]
epoch 34200  training loss: 0.027698300778865814

 34%|███████████████████████                                            | 34419/100000 [07:52<15:04, 72.47it/s]
epoch 34300  training loss: 0.02761930227279663
epoch 34300  clean testing loss: 46.54704284667969
epoch 34400  training loss: 0.027529580518603325

 35%|███████████████████████▏                                           | 34563/100000 [07:54<14:59, 72.77it/s]
epoch 34500  training loss: 0.027379831299185753

 35%|███████████████████████▎                                           | 34707/100000 [07:56<15:03, 72.24it/s]
epoch 34600  training loss: 0.027346737682819366
epoch 34600  clean testing loss: 46.55900192260742
epoch 34700  training loss: 0.02721760794520378

 35%|███████████████████████▎                                           | 34851/100000 [07:58<14:55, 72.72it/s]
epoch 34800  training loss: 0.027086377143859863

 35%|███████████████████████▍                                           | 35003/100000 [08:00<15:01, 72.09it/s]
epoch 34900  training loss: 0.027376217767596245

 35%|███████████████████████▌                                           | 35147/100000 [08:02<14:50, 72.87it/s]
epoch 35000  training loss: 0.02690163068473339
epoch 35000  clean testing loss: 46.53174591064453
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_sin_size500_noise1.00e+00_invop1_lr5e-05 ...
epoch 35100  training loss: 0.026818934828042984

 35%|███████████████████████▋                                           | 35291/100000 [08:04<14:41, 73.43it/s]
epoch 35200  training loss: 0.026786165311932564

 35%|███████████████████████▋                                           | 35435/100000 [08:06<15:05, 71.30it/s]
epoch 35300  training loss: 0.026676030829548836
epoch 35300  clean testing loss: 46.51791763305664
epoch 35400  training loss: 0.026524638757109642

 36%|███████████████████████▊                                           | 35579/100000 [08:08<14:53, 72.09it/s]
epoch 35500  training loss: 0.02657814510166645

 36%|███████████████████████▉                                           | 35731/100000 [08:10<14:53, 71.90it/s]
epoch 35600  training loss: 0.026412907987833023
epoch 35600  clean testing loss: 46.46898651123047
epoch 35700  training loss: 0.026232820004224777

 36%|████████████████████████                                           | 35875/100000 [08:12<14:42, 72.68it/s]
epoch 35800  training loss: 0.026532350108027458

 36%|████████████████████████▏                                          | 36019/100000 [08:14<14:46, 72.20it/s]
epoch 35900  training loss: 0.026108140125870705
epoch 35900  clean testing loss: 46.41962432861328
epoch 36000  training loss: 0.02601935714483261
epoch 36000  clean testing loss: 46.39567565917969

 36%|████████████████████████▏                                          | 36163/100000 [08:16<14:42, 72.36it/s]
epoch 36100  training loss: 0.025862185284495354

 36%|████████████████████████▎                                          | 36315/100000 [08:18<14:36, 72.64it/s]
epoch 36200  training loss: 0.02578050270676613
epoch 36200  clean testing loss: 46.37966537475586
epoch 36300  training loss: 0.025694604963064194

 36%|████████████████████████▍                                          | 36459/100000 [08:20<14:38, 72.31it/s]
epoch 36400  training loss: 0.02560454234480858

 37%|████████████████████████▌                                          | 36603/100000 [08:22<14:45, 71.60it/s]
epoch 36500  training loss: 0.02551409974694252

 37%|████████████████████████▌                                          | 36747/100000 [08:24<14:27, 72.88it/s]
epoch 36600  training loss: 0.025441594421863556
epoch 36600  clean testing loss: 46.30499267578125
epoch 36700  training loss: 0.02574746124446392

 37%|████████████████████████▋                                          | 36899/100000 [08:26<14:25, 72.89it/s]
epoch 36800  training loss: 0.025244886055588722

 37%|████████████████████████▊                                          | 37043/100000 [08:28<14:35, 71.87it/s]
epoch 36900  training loss: 0.02515460178256035
epoch 36900  clean testing loss: 46.230648040771484
epoch 37000  training loss: 0.025070372968912125
epoch 37000  clean testing loss: 46.20986557006836

 37%|████████████████████████▉                                          | 37187/100000 [08:30<14:22, 72.80it/s]
epoch 37100  training loss: 0.02497982233762741

 37%|█████████████████████████                                          | 37331/100000 [08:32<14:15, 73.23it/s]
epoch 37200  training loss: 0.024979624897241592
epoch 37200  clean testing loss: 46.16041564941406
epoch 37300  training loss: 0.024771805852651596

 37%|█████████████████████████                                          | 37475/100000 [08:34<14:17, 72.94it/s]
epoch 37400  training loss: 0.0246830265969038

 38%|█████████████████████████▏                                         | 37619/100000 [08:36<14:29, 71.76it/s]
epoch 37500  training loss: 0.02462656982243061
epoch 37500  clean testing loss: 46.0807991027832
epoch 37600  training loss: 0.02449660189449787

 38%|█████████████████████████▎                                         | 37771/100000 [08:38<14:21, 72.27it/s]
epoch 37700  training loss: 0.024402720853686333

 38%|█████████████████████████▍                                         | 37915/100000 [08:40<14:21, 72.08it/s]
epoch 37800  training loss: 0.024325842037796974
epoch 37800  clean testing loss: 45.97510528564453
epoch 37900  training loss: 0.02422861009836197

 38%|█████████████████████████▍                                         | 38059/100000 [08:42<14:12, 72.68it/s]
epoch 38000  training loss: 0.024168843403458595
epoch 38000  clean testing loss: 45.92103958129883

 38%|█████████████████████████▌                                         | 38171/100000 [08:44<14:09, 72.76it/s]
epoch 38100  training loss: 0.024112509563565254

 38%|█████████████████████████▋                                         | 38315/100000 [08:46<14:05, 72.92it/s]
epoch 38200  training loss: 0.02395327389240265
epoch 38200  clean testing loss: 45.84365463256836
epoch 38300  training loss: 0.023857930675148964

 38%|█████████████████████████▊                                         | 38459/100000 [08:48<14:12, 72.17it/s]
epoch 38400  training loss: 0.02378365583717823

 39%|█████████████████████████▊                                         | 38603/100000 [08:50<13:56, 73.40it/s]
epoch 38500  training loss: 0.02368950843811035
epoch 38500  clean testing loss: 45.7425651550293
epoch 38600  training loss: 0.02357526123523712

 39%|█████████████████████████▉                                         | 38755/100000 [08:52<13:58, 73.02it/s]
epoch 38700  training loss: 0.023736191913485527

 39%|██████████████████████████                                         | 38899/100000 [08:54<13:57, 72.98it/s]
epoch 38800  training loss: 0.0234718956053257
epoch 38800  clean testing loss: 45.63715362548828
epoch 38900  training loss: 0.023351537063717842

 39%|██████████████████████████▏                                        | 39043/100000 [08:56<13:50, 73.37it/s]
epoch 39000  training loss: 0.02321043238043785
epoch 39000  clean testing loss: 45.571102142333984

 39%|██████████████████████████▎                                        | 39187/100000 [08:58<13:55, 72.77it/s]
epoch 39100  training loss: 0.023130277171730995
epoch 39100  clean testing loss: 45.54036331176758
epoch 39200  training loss: 0.02304977923631668

 39%|██████████████████████████▎                                        | 39339/100000 [09:00<14:03, 71.90it/s]
epoch 39300  training loss: 0.022966008633375168

 39%|██████████████████████████▍                                        | 39483/100000 [09:02<13:43, 73.51it/s]
epoch 39400  training loss: 0.022877858951687813
epoch 39400  clean testing loss: 45.44441604614258
epoch 39500  training loss: 0.02280268259346485

 40%|██████████████████████████▌                                        | 39627/100000 [09:04<13:46, 73.08it/s]
epoch 39600  training loss: 0.022719191387295723

 40%|██████████████████████████▋                                        | 39771/100000 [09:06<13:52, 72.33it/s]
epoch 39700  training loss: 0.02260534092783928

 40%|██████████████████████████▋                                        | 39923/100000 [09:08<13:50, 72.34it/s]
epoch 39800  training loss: 0.022644005715847015
epoch 39800  clean testing loss: 45.30436325073242
epoch 39900  training loss: 0.022423863410949707

 40%|██████████████████████████▊                                        | 40067/100000 [09:10<13:46, 72.55it/s]
epoch 40000  training loss: 0.02238774299621582
epoch 40000  clean testing loss: 45.236454010009766

 40%|██████████████████████████▉                                        | 40211/100000 [09:12<13:35, 73.33it/s]
epoch 40100  training loss: 0.022242257371544838
epoch 40100  clean testing loss: 45.198158264160156
epoch 40200  training loss: 0.022169437259435654

 40%|███████████████████████████                                        | 40355/100000 [09:14<13:35, 73.10it/s]
epoch 40300  training loss: 0.022060276940464973

 41%|███████████████████████████▏                                       | 40507/100000 [09:16<13:33, 73.14it/s]
epoch 40400  training loss: 0.022005761042237282
epoch 40400  clean testing loss: 45.08083724975586
epoch 40500  training loss: 0.02187735214829445

 41%|███████████████████████████▏                                       | 40651/100000 [09:18<13:41, 72.25it/s]
epoch 40600  training loss: 0.021891377866268158

 41%|███████████████████████████▎                                       | 40795/100000 [09:20<13:26, 73.39it/s]
epoch 40700  training loss: 0.02170351892709732
epoch 40700  clean testing loss: 44.99928665161133
epoch 40800  training loss: 0.02161213383078575

 41%|███████████████████████████▍                                       | 40947/100000 [09:22<13:27, 73.09it/s]
epoch 40900  training loss: 0.021525226533412933

 41%|███████████████████████████▌                                       | 41091/100000 [09:24<13:31, 72.64it/s]
epoch 41000  training loss: 0.021430322900414467
epoch 41000  clean testing loss: 44.90629959106445
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_sin_size500_noise1.00e+00_invop1_lr5e-05 ...
epoch 41100  training loss: 0.021338071674108505

 41%|███████████████████████████▋                                       | 41235/100000 [09:26<13:34, 72.15it/s]
epoch 41200  training loss: 0.021251212805509567

 41%|███████████████████████████▋                                       | 41379/100000 [09:28<13:25, 72.77it/s]
epoch 41300  training loss: 0.021169498562812805
epoch 41300  clean testing loss: 44.820892333984375
epoch 41400  training loss: 0.02107885479927063

 42%|███████████████████████████▊                                       | 41523/100000 [09:30<13:41, 71.20it/s]
epoch 41500  training loss: 0.021183781325817108

 42%|███████████████████████████▉                                       | 41667/100000 [09:32<13:20, 72.84it/s]
epoch 41600  training loss: 0.020919449627399445

 42%|████████████████████████████                                       | 41819/100000 [09:34<13:13, 73.32it/s]
epoch 41700  training loss: 0.02082209102809429
epoch 41700  clean testing loss: 44.721923828125
epoch 41800  training loss: 0.02072780765593052

 42%|████████████████████████████                                       | 41963/100000 [09:36<13:13, 73.17it/s]
epoch 41900  training loss: 0.020630789920687675

 42%|████████████████████████████▏                                      | 42107/100000 [09:38<13:22, 72.12it/s]
epoch 42000  training loss: 0.020534612238407135
epoch 42000  clean testing loss: 44.65576171875
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_sin_size500_noise1.00e+00_invop1_lr5e-05 ...
epoch 42100  training loss: 0.020461510866880417

 42%|████████████████████████████▎                                      | 42259/100000 [09:40<13:09, 73.16it/s]
epoch 42200  training loss: 0.020384972915053368

 42%|████████████████████████████▍                                      | 42403/100000 [09:42<13:18, 72.12it/s]
epoch 42300  training loss: 0.020304733887314796
epoch 42300  clean testing loss: 44.608280181884766
epoch 42400  training loss: 0.020220329985022545

 43%|████████████████████████████▌                                      | 42547/100000 [09:44<13:19, 71.90it/s]
epoch 42500  training loss: 0.020136693492531776

 43%|████████████████████████████▌                                      | 42691/100000 [09:46<13:11, 72.40it/s]
epoch 42600  training loss: 0.020136449486017227
epoch 42600  clean testing loss: 44.55263900756836
epoch 42700  training loss: 0.019962461665272713

 43%|████████████████████████████▋                                      | 42835/100000 [09:48<13:12, 72.14it/s]
epoch 42800  training loss: 0.019873186945915222

 43%|████████████████████████████▊                                      | 42987/100000 [09:50<13:04, 72.64it/s]
epoch 42900  training loss: 0.0197878647595644
epoch 42900  clean testing loss: 44.52311325073242
epoch 43000  training loss: 0.019714128226041794
epoch 43000  clean testing loss: 44.513580322265625

 43%|████████████████████████████▉                                      | 43131/100000 [09:52<13:08, 72.11it/s]
epoch 43100  training loss: 0.01962459646165371

 43%|████████████████████████████▉                                      | 43275/100000 [09:54<12:54, 73.28it/s]
epoch 43200  training loss: 0.019530247896909714

 43%|█████████████████████████████                                      | 43419/100000 [09:56<12:54, 73.10it/s]
epoch 43300  training loss: 0.01944168098270893
epoch 43300  clean testing loss: 44.488494873046875
epoch 43400  training loss: 0.01935545913875103

 44%|█████████████████████████████▏                                     | 43563/100000 [09:58<12:52, 73.04it/s]
epoch 43500  training loss: 0.01928374171257019

 44%|█████████████████████████████▎                                     | 43715/100000 [10:00<12:48, 73.22it/s]
epoch 43600  training loss: 0.019194211810827255
epoch 43600  clean testing loss: 44.47571563720703
epoch 43700  training loss: 0.019109223037958145

 44%|█████████████████████████████▍                                     | 43859/100000 [10:02<12:42, 73.62it/s]
epoch 43800  training loss: 0.019019337370991707

 44%|█████████████████████████████▍                                     | 44003/100000 [10:04<13:17, 70.22it/s]
epoch 43900  training loss: 0.018930533900856972
epoch 43900  clean testing loss: 44.476837158203125
epoch 44000  training loss: 0.01886593922972679
epoch 44000  clean testing loss: 44.47760009765625

 44%|█████████████████████████████▌                                     | 44155/100000 [10:06<12:45, 72.96it/s]
epoch 44100  training loss: 0.01887938752770424

 44%|█████████████████████████████▋                                     | 44299/100000 [10:08<12:52, 72.08it/s]
epoch 44200  training loss: 0.018732380121946335
epoch 44200  clean testing loss: 44.49177551269531
epoch 44300  training loss: 0.018599295988678932

 44%|█████████████████████████████▊                                     | 44443/100000 [10:10<12:55, 71.60it/s]
epoch 44400  training loss: 0.018608151003718376

 45%|█████████████████████████████▊                                     | 44587/100000 [10:12<12:45, 72.42it/s]
epoch 44500  training loss: 0.018421242013573647
epoch 44500  clean testing loss: 44.505638122558594
epoch 44600  training loss: 0.018338140100240707

 45%|█████████████████████████████▉                                     | 44731/100000 [10:14<12:38, 72.90it/s]
epoch 44700  training loss: 0.018252072855830193

 45%|██████████████████████████████                                     | 44883/100000 [10:16<12:33, 73.14it/s]
epoch 44800  training loss: 0.018229356035590172

 45%|██████████████████████████████▏                                    | 45027/100000 [10:18<12:34, 72.87it/s]
epoch 44900  training loss: 0.018086854368448257
epoch 44900  clean testing loss: 44.54790115356445
epoch 45000  training loss: 0.018072709441184998
epoch 45000  clean testing loss: 44.55735397338867

 45%|██████████████████████████████▎                                    | 45171/100000 [10:20<12:33, 72.74it/s]
epoch 45100  training loss: 0.017934929579496384

 45%|██████████████████████████████▎                                    | 45315/100000 [10:22<12:30, 72.91it/s]
epoch 45200  training loss: 0.017863400280475616
epoch 45200  clean testing loss: 44.58842849731445
epoch 45300  training loss: 0.017789047211408615

 45%|██████████████████████████████▍                                    | 45467/100000 [10:24<12:27, 72.97it/s]
epoch 45400  training loss: 0.01771146059036255

 46%|██████████████████████████████▌                                    | 45611/100000 [10:26<12:32, 72.26it/s]
epoch 45500  training loss: 0.017648616805672646
epoch 45500  clean testing loss: 44.639041900634766
epoch 45600  training loss: 0.017568375915288925

 46%|██████████████████████████████▋                                    | 45755/100000 [10:28<12:29, 72.37it/s]
epoch 45700  training loss: 0.01746881939470768

 46%|██████████████████████████████▊                                    | 45899/100000 [10:30<12:25, 72.57it/s]
epoch 45800  training loss: 0.01738717406988144
epoch 45800  clean testing loss: 44.6954460144043
epoch 45900  training loss: 0.01730574108660221

 46%|██████████████████████████████▊                                    | 46036/100000 [10:32<15:23, 58.45it/s]
epoch 46000  training loss: 0.01722549833357334
epoch 46000  clean testing loss: 44.74001693725586

 46%|██████████████████████████████▉                                    | 46144/100000 [10:34<15:48, 56.81it/s]
epoch 46100  training loss: 0.017144612967967987

 46%|██████████████████████████████▉                                    | 46259/100000 [10:36<16:03, 55.77it/s]
epoch 46200  training loss: 0.017072293907403946

 46%|███████████████████████████████                                    | 46374/100000 [10:38<15:35, 57.35it/s]
epoch 46300  training loss: 0.016984496265649796

 46%|███████████████████████████████▏                                   | 46485/100000 [10:40<15:43, 56.72it/s]
epoch 46400  training loss: 0.016903860494494438

 47%|███████████████████████████████▏                                   | 46599/100000 [10:42<16:01, 55.52it/s]
epoch 46500  training loss: 0.016825981438159943
epoch 46500  clean testing loss: 44.86814880371094
epoch 46600  training loss: 0.01682271808385849

 47%|███████████████████████████████▎                                   | 46709/100000 [10:44<15:46, 56.30it/s]
epoch 46700  training loss: 0.01666884310543537

 47%|███████████████████████████████▎                                   | 46823/100000 [10:46<15:53, 55.75it/s]
epoch 46800  training loss: 0.0165858194231987

 47%|███████████████████████████████▍                                   | 46937/100000 [10:48<16:01, 55.17it/s]
epoch 46900  training loss: 0.016529595479369164

 47%|███████████████████████████████▌                                   | 47051/100000 [10:50<15:20, 57.51it/s]
epoch 47000  training loss: 0.016428044065833092
epoch 47000  clean testing loss: 45.015438079833984

 47%|███████████████████████████████▌                                   | 47160/100000 [10:52<15:54, 55.34it/s]
epoch 47100  training loss: 0.016349244862794876

 47%|███████████████████████████████▋                                   | 47274/100000 [10:54<15:14, 57.68it/s]
epoch 47200  training loss: 0.016275787726044655

 47%|███████████████████████████████▊                                   | 47389/100000 [10:56<15:27, 56.75it/s]
epoch 47300  training loss: 0.01619076170027256

 48%|███████████████████████████████▊                                   | 47504/100000 [10:58<15:07, 57.86it/s]
epoch 47400  training loss: 0.016185594722628593
epoch 47400  clean testing loss: 45.15267562866211
epoch 47500  training loss: 0.01605440117418766

 48%|███████████████████████████████▉                                   | 47614/100000 [11:00<15:39, 55.76it/s]
epoch 47600  training loss: 0.01595975086092949

 48%|███████████████████████████████▉                                   | 47728/100000 [11:02<15:32, 56.06it/s]
epoch 47700  training loss: 0.015881795436143875

 48%|████████████████████████████████                                   | 47844/100000 [11:04<14:50, 58.56it/s]
epoch 47800  training loss: 0.015804102644324303

 48%|████████████████████████████████▏                                  | 47958/100000 [11:06<15:12, 57.02it/s]
epoch 47900  training loss: 0.015729213133454323

 48%|████████████████████████████████▏                                  | 48092/100000 [11:08<12:00, 72.00it/s]
epoch 48000  training loss: 0.01566201262176037
epoch 48000  clean testing loss: 45.37220001220703

 48%|████████████████████████████████▎                                  | 48236/100000 [11:10<11:53, 72.52it/s]
epoch 48100  training loss: 0.015588386915624142
epoch 48100  clean testing loss: 45.40791320800781
epoch 48200  training loss: 0.015521706081926823

 48%|████████████████████████████████▍                                  | 48380/100000 [11:12<11:53, 72.34it/s]
epoch 48300  training loss: 0.015452280640602112

 49%|████████████████████████████████▌                                  | 48532/100000 [11:14<11:46, 72.82it/s]
epoch 48400  training loss: 0.015379340387880802
epoch 48400  clean testing loss: 45.52165222167969
epoch 48500  training loss: 0.015305851586163044

 49%|████████████████████████████████▌                                  | 48676/100000 [11:16<12:02, 71.08it/s]
epoch 48600  training loss: 0.0152342664077878

 49%|████████████████████████████████▋                                  | 48820/100000 [11:18<11:40, 73.11it/s]
epoch 48700  training loss: 0.01515967771410942
epoch 48700  clean testing loss: 45.64852523803711
epoch 48800  training loss: 0.01508655771613121

 49%|████████████████████████████████▊                                  | 48964/100000 [11:20<11:43, 72.58it/s]
epoch 48900  training loss: 0.015012087300419807

 49%|████████████████████████████████▉                                  | 49108/100000 [11:22<11:42, 72.42it/s]
epoch 49000  training loss: 0.014948074705898762
epoch 49000  clean testing loss: 45.778987884521484
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_sin_size500_noise1.00e+00_invop1_lr5e-05 ...
epoch 49100  training loss: 0.014866658486425877

 49%|████████████████████████████████▉                                  | 49224/100000 [11:24<15:14, 55.51it/s]
epoch 49200  training loss: 0.01479326281696558

 49%|█████████████████████████████████                                  | 49338/100000 [11:26<15:26, 54.66it/s]
epoch 49300  training loss: 0.014721859246492386

 49%|█████████████████████████████████▏                                 | 49472/100000 [11:28<11:34, 72.78it/s]
epoch 49400  training loss: 0.014646141789853573

 50%|█████████████████████████████████▏                                 | 49616/100000 [11:30<11:43, 71.57it/s]
epoch 49500  training loss: 0.014579365029931068
epoch 49500  clean testing loss: 46.01700210571289
epoch 49600  training loss: 0.014503327198326588

 50%|█████████████████████████████████▎                                 | 49760/100000 [11:32<11:27, 73.06it/s]
epoch 49700  training loss: 0.014433757402002811

 50%|█████████████████████████████████▍                                 | 49904/100000 [11:34<11:31, 72.46it/s]
epoch 49800  training loss: 0.014376829378306866
epoch 49800  clean testing loss: 46.163780212402344
epoch 49900  training loss: 0.01430840976536274

 50%|█████████████████████████████████▌                                 | 50048/100000 [11:36<11:28, 72.57it/s]
epoch 50000  training loss: 0.014223153702914715
epoch 50000  clean testing loss: 46.266822814941406

 50%|█████████████████████████████████▋                                 | 50200/100000 [11:38<11:19, 73.28it/s]
epoch 50100  training loss: 0.014152243733406067
epoch 50100  clean testing loss: 46.31972122192383
epoch 50200  training loss: 0.01407698541879654

 50%|█████████████████████████████████▋                                 | 50344/100000 [11:40<11:22, 72.76it/s]
epoch 50300  training loss: 0.01400685403496027

 50%|█████████████████████████████████▊                                 | 50488/100000 [11:42<11:15, 73.28it/s]
epoch 50400  training loss: 0.01399347186088562

 51%|█████████████████████████████████▉                                 | 50632/100000 [11:44<11:27, 71.76it/s]
epoch 50500  training loss: 0.013869780115783215
epoch 50500  clean testing loss: 46.535404205322266
epoch 50600  training loss: 0.013806292787194252

 51%|██████████████████████████████████                                 | 50784/100000 [11:46<11:21, 72.26it/s]
epoch 50700  training loss: 0.013731276616454124

 51%|██████████████████████████████████                                 | 50928/100000 [11:48<11:13, 72.89it/s]
epoch 50800  training loss: 0.01366374734789133
epoch 50800  clean testing loss: 46.702781677246094
epoch 50900  training loss: 0.013602045364677906

 51%|██████████████████████████████████▏                                | 51072/100000 [11:50<11:11, 72.87it/s]
epoch 51000  training loss: 0.013531312346458435
epoch 51000  clean testing loss: 46.81703186035156

 51%|██████████████████████████████████▎                                | 51216/100000 [11:52<11:10, 72.77it/s]
epoch 51100  training loss: 0.01347030233591795
epoch 51100  clean testing loss: 46.8630485534668
epoch 51200  training loss: 0.013412119820713997

 51%|██████████████████████████████████▍                                | 51360/100000 [11:54<11:12, 72.32it/s]
epoch 51300  training loss: 0.013351278379559517

 52%|██████████████████████████████████▌                                | 51504/100000 [11:56<11:15, 71.78it/s]
epoch 51400  training loss: 0.013287228532135487
epoch 51400  clean testing loss: 47.02287292480469
epoch 51500  training loss: 0.013222828507423401

 52%|██████████████████████████████████▌                                | 51656/100000 [11:58<11:04, 72.79it/s]
epoch 51600  training loss: 0.013156548142433167

 52%|██████████████████████████████████▋                                | 51800/100000 [12:00<11:01, 72.85it/s]
epoch 51700  training loss: 0.013097988441586494

 52%|██████████████████████████████████▊                                | 51944/100000 [12:02<11:15, 71.16it/s]
epoch 51800  training loss: 0.013027436099946499
epoch 51800  clean testing loss: 47.259063720703125
epoch 51900  training loss: 0.012962608598172665

 52%|██████████████████████████████████▉                                | 52088/100000 [12:04<10:59, 72.64it/s]
epoch 52000  training loss: 0.012898514978587627
epoch 52000  clean testing loss: 47.38091278076172

 52%|██████████████████████████████████▉                                | 52232/100000 [12:06<10:59, 72.48it/s]
epoch 52100  training loss: 0.012835533358156681
epoch 52100  clean testing loss: 47.44163513183594
epoch 52200  training loss: 0.012785998173058033

 52%|███████████████████████████████████                                | 52384/100000 [12:08<10:59, 72.17it/s]
epoch 52300  training loss: 0.012708044610917568

 53%|███████████████████████████████████▏                               | 52528/100000 [12:10<10:52, 72.77it/s]
epoch 52400  training loss: 0.012643987312912941
epoch 52400  clean testing loss: 47.632694244384766
epoch 52500  training loss: 0.012582867406308651

 53%|███████████████████████████████████▎                               | 52672/100000 [12:12<10:44, 73.44it/s]
epoch 52600  training loss: 0.01251912210136652

 53%|███████████████████████████████████▍                               | 52816/100000 [12:14<10:54, 72.07it/s]
epoch 52700  training loss: 0.012459185905754566
epoch 52700  clean testing loss: 47.825233459472656
epoch 52800  training loss: 0.012391826137900352

 53%|███████████████████████████████████▍                               | 52968/100000 [12:16<10:42, 73.15it/s]
epoch 52900  training loss: 0.012331157922744751

 53%|███████████████████████████████████▌                               | 53112/100000 [12:18<10:44, 72.72it/s]
epoch 53000  training loss: 0.012280371971428394
epoch 53000  clean testing loss: 48.02320098876953
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_sin_size500_noise1.00e+00_invop1_lr5e-05 ...
epoch 53100  training loss: 0.012209942564368248

 53%|███████████████████████████████████▋                               | 53256/100000 [12:20<10:43, 72.69it/s]
epoch 53200  training loss: 0.012148657813668251

 53%|███████████████████████████████████▊                               | 53400/100000 [12:22<10:38, 72.98it/s]
epoch 53300  training loss: 0.012085095047950745

 54%|███████████████████████████████████▊                               | 53544/100000 [12:24<10:34, 73.23it/s]
epoch 53400  training loss: 0.01207326166331768
epoch 53400  clean testing loss: 48.29054260253906
epoch 53500  training loss: 0.011978160589933395

 54%|███████████████████████████████████▉                               | 53696/100000 [12:26<10:26, 73.92it/s]
epoch 53600  training loss: 0.011904364451766014

 54%|████████████████████████████████████                               | 53840/100000 [12:28<10:27, 73.56it/s]
epoch 53700  training loss: 0.0118488147854805
epoch 53700  clean testing loss: 48.49373245239258
epoch 53800  training loss: 0.011792225763201714

 54%|████████████████████████████████████▏                              | 53984/100000 [12:30<10:26, 73.49it/s]
epoch 53900  training loss: 0.011724992655217648

 54%|████████████████████████████████████▎                              | 54136/100000 [12:32<10:40, 71.65it/s]
epoch 54000  training loss: 0.011675887741148472
epoch 54000  clean testing loss: 48.70502471923828
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_sin_size500_noise1.00e+00_invop1_lr5e-05 ...
epoch 54100  training loss: 0.011615731753408909

 54%|████████████████████████████████████▎                              | 54280/100000 [12:34<10:32, 72.23it/s]
epoch 54200  training loss: 0.011564882472157478

 54%|████████████████████████████████████▍                              | 54424/100000 [12:36<10:30, 72.32it/s]
epoch 54300  training loss: 0.011511814780533314
epoch 54300  clean testing loss: 48.892269134521484
epoch 54400  training loss: 0.011456051841378212

 55%|████████████████████████████████████▌                              | 54568/100000 [12:38<10:25, 72.68it/s]
epoch 54500  training loss: 0.011402968317270279

 55%|████████████████████████████████████▋                              | 54712/100000 [12:40<10:24, 72.55it/s]
epoch 54600  training loss: 0.011342758312821388
epoch 54600  clean testing loss: 49.1031608581543
epoch 54700  training loss: 0.011286180466413498

 55%|████████████████████████████████████▊                              | 54864/100000 [12:42<10:24, 72.29it/s]
epoch 54800  training loss: 0.011237220838665962

 55%|████████████████████████████████████▊                              | 55008/100000 [12:44<10:22, 72.32it/s]
epoch 54900  training loss: 0.011187687516212463
epoch 54900  clean testing loss: 49.319541931152344
epoch 55000  training loss: 0.01111840084195137
epoch 55000  clean testing loss: 49.389442443847656

 55%|████████████████████████████████████▉                              | 55152/100000 [12:46<10:16, 72.78it/s]
epoch 55100  training loss: 0.011065653525292873

 55%|█████████████████████████████████████                              | 55304/100000 [12:48<10:14, 72.76it/s]
epoch 55200  training loss: 0.011008109897375107

 55%|█████████████████████████████████████▏                             | 55448/100000 [12:50<10:12, 72.78it/s]
epoch 55300  training loss: 0.010956131853163242
epoch 55300  clean testing loss: 49.609615325927734
epoch 55400  training loss: 0.010900505818426609

 56%|█████████████████████████████████████▏                             | 55592/100000 [12:52<10:07, 73.07it/s]
epoch 55500  training loss: 0.010843085125088692

 56%|█████████████████████████████████████▎                             | 55736/100000 [12:54<10:18, 71.60it/s]
epoch 55600  training loss: 0.010788911953568459
epoch 55600  clean testing loss: 49.8315544128418
epoch 55700  training loss: 0.010740700177848339

 56%|█████████████████████████████████████▍                             | 55880/100000 [12:56<10:00, 73.53it/s]
epoch 55800  training loss: 0.010682075284421444

 56%|█████████████████████████████████████▌                             | 56032/100000 [12:58<10:07, 72.38it/s]
epoch 55900  training loss: 0.010630642995238304
epoch 55900  clean testing loss: 50.05520248413086
epoch 56000  training loss: 0.01057791244238615
epoch 56000  clean testing loss: 50.130455017089844

 56%|█████████████████████████████████████▋                             | 56176/100000 [13:00<10:02, 72.71it/s]
epoch 56100  training loss: 0.010526880621910095

 56%|█████████████████████████████████████▋                             | 56320/100000 [13:02<09:59, 72.80it/s]
epoch 56200  training loss: 0.010472027584910393
epoch 56200  clean testing loss: 50.28205871582031
epoch 56300  training loss: 0.010419191792607307

 56%|█████████████████████████████████████▊                             | 56432/100000 [13:04<10:00, 72.61it/s]
epoch 56400  training loss: 0.010370089672505856

 57%|█████████████████████████████████████▉                             | 56576/100000 [13:06<09:52, 73.34it/s]
epoch 56500  training loss: 0.010314351879060268
epoch 56500  clean testing loss: 50.51187515258789
epoch 56600  training loss: 0.01026272214949131

 57%|██████████████████████████████████████                             | 56720/100000 [13:08<09:58, 72.31it/s]
epoch 56700  training loss: 0.01021340861916542

 57%|██████████████████████████████████████                             | 56864/100000 [13:10<09:51, 72.89it/s]
epoch 56800  training loss: 0.010162037797272205

 57%|██████████████████████████████████████▏                            | 57016/100000 [13:12<10:00, 71.60it/s]
epoch 56900  training loss: 0.010110192000865936
epoch 56900  clean testing loss: 50.819766998291016
epoch 57000  training loss: 0.010061386972665787
epoch 57000  clean testing loss: 50.89785385131836

 57%|██████████████████████████████████████▎                            | 57160/100000 [13:14<09:49, 72.68it/s]
epoch 57100  training loss: 0.010017850436270237

 57%|██████████████████████████████████████▍                            | 57304/100000 [13:16<09:44, 73.04it/s]
epoch 57200  training loss: 0.009975044056773186
epoch 57200  clean testing loss: 51.027870178222656
epoch 57300  training loss: 0.009930475614964962

 57%|██████████████████████████████████████▍                            | 57448/100000 [13:18<09:42, 73.04it/s]
epoch 57400  training loss: 0.009883861988782883

 58%|██████████████████████████████████████▌                            | 57600/100000 [13:20<09:46, 72.31it/s]
epoch 57500  training loss: 0.009854035452008247
epoch 57500  clean testing loss: 51.246646881103516
epoch 57600  training loss: 0.009789673611521721

 58%|██████████████████████████████████████▋                            | 57744/100000 [13:22<09:33, 73.68it/s]
epoch 57700  training loss: 0.009739735163748264

 58%|██████████████████████████████████████▊                            | 57888/100000 [13:24<09:32, 73.51it/s]
epoch 57800  training loss: 0.009691303595900536
epoch 57800  clean testing loss: 51.47653579711914
epoch 57900  training loss: 0.00964610930532217

 58%|██████████████████████████████████████▉                            | 58032/100000 [13:26<09:37, 72.67it/s]
epoch 58000  training loss: 0.00959676131606102
epoch 58000  clean testing loss: 51.63154602050781

 58%|██████████████████████████████████████▉                            | 58184/100000 [13:28<09:34, 72.83it/s]
epoch 58100  training loss: 0.009550081565976143
epoch 58100  clean testing loss: 51.70817947387695
epoch 58200  training loss: 0.009502336382865906

 58%|███████████████████████████████████████                            | 58328/100000 [13:30<09:27, 73.39it/s]
epoch 58300  training loss: 0.009457029402256012

 58%|███████████████████████████████████████▏                           | 58472/100000 [13:32<09:25, 73.41it/s]
epoch 58400  training loss: 0.009408081881701946

 59%|███████████████████████████████████████▎                           | 58616/100000 [13:34<09:23, 73.42it/s]
epoch 58500  training loss: 0.009363710880279541
epoch 58500  clean testing loss: 52.01818084716797
epoch 58600  training loss: 0.00931675173342228

 59%|███████████████████████████████████████▎                           | 58768/100000 [13:36<09:30, 72.21it/s]
epoch 58700  training loss: 0.00929285492748022

 59%|███████████████████████████████████████▍                           | 58912/100000 [13:38<09:28, 72.34it/s]
epoch 58800  training loss: 0.009224743582308292
epoch 58800  clean testing loss: 52.251922607421875
epoch 58900  training loss: 0.00917932391166687

 59%|███████████████████████████████████████▌                           | 59056/100000 [13:40<09:20, 73.08it/s]
epoch 59000  training loss: 0.009133972227573395
epoch 59000  clean testing loss: 52.407352447509766

 59%|███████████████████████████████████████▋                           | 59200/100000 [13:42<09:22, 72.58it/s]
epoch 59100  training loss: 0.009095300920307636
epoch 59100  clean testing loss: 52.485877990722656
epoch 59200  training loss: 0.009042774327099323

 59%|███████████████████████████████████████▊                           | 59352/100000 [13:44<09:17, 72.94it/s]
epoch 59300  training loss: 0.008998501114547253

 59%|███████████████████████████████████████▊                           | 59496/100000 [13:46<09:22, 71.98it/s]
epoch 59400  training loss: 0.008954660966992378
epoch 59400  clean testing loss: 52.7224235534668
epoch 59500  training loss: 0.008909105323255062

 60%|███████████████████████████████████████▉                           | 59640/100000 [13:48<09:15, 72.67it/s]
epoch 59600  training loss: 0.00886511243879795

 60%|████████████████████████████████████████                           | 59784/100000 [13:50<09:12, 72.73it/s]
epoch 59700  training loss: 0.008820043876767159
epoch 59700  clean testing loss: 52.9561882019043
epoch 59800  training loss: 0.00877801887691021

 60%|████████████████████████████████████████▏                          | 59928/100000 [13:52<09:12, 72.55it/s]
epoch 59900  training loss: 0.008733589202165604

 60%|████████████████████████████████████████▎                          | 60080/100000 [13:54<09:04, 73.36it/s]
epoch 60000  training loss: 0.008699744008481503
epoch 60000  clean testing loss: 53.1934700012207

 60%|████████████████████████████████████████▎                          | 60224/100000 [13:56<09:04, 72.99it/s]
epoch 60100  training loss: 0.008652821183204651
epoch 60100  clean testing loss: 53.25692367553711
epoch 60200  training loss: 0.008615964092314243

 60%|████████████████████████████████████████▍                          | 60368/100000 [13:58<09:02, 73.07it/s]
epoch 60300  training loss: 0.008577229455113411

 61%|████████████████████████████████████████▌                          | 60520/100000 [14:00<08:59, 73.13it/s]
epoch 60400  training loss: 0.008537042886018753
epoch 60400  clean testing loss: 53.467918395996094
epoch 60500  training loss: 0.008495389483869076

 61%|████████████████████████████████████████▋                          | 60664/100000 [14:02<09:03, 72.33it/s]
epoch 60600  training loss: 0.008453535847365856

 61%|████████████████████████████████████████▋                          | 60808/100000 [14:04<08:53, 73.51it/s]
epoch 60700  training loss: 0.00841155368834734
epoch 60700  clean testing loss: 53.697227478027344
epoch 60800  training loss: 0.00836983136832714

 61%|████████████████████████████████████████▊                          | 60952/100000 [14:06<09:00, 72.29it/s]
epoch 60900  training loss: 0.008328448049724102

 61%|████████████████████████████████████████▉                          | 61104/100000 [14:08<08:55, 72.61it/s]
epoch 61000  training loss: 0.00828811526298523
epoch 61000  clean testing loss: 53.926300048828125
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_sin_size500_noise1.00e+00_invop1_lr5e-05 ...
epoch 61100  training loss: 0.008247858844697475

 61%|█████████████████████████████████████████                          | 61248/100000 [14:10<08:54, 72.55it/s]
epoch 61200  training loss: 0.008205526508390903

 61%|█████████████████████████████████████████▏                         | 61392/100000 [14:12<08:51, 72.70it/s]
epoch 61300  training loss: 0.008166234008967876
epoch 61300  clean testing loss: 54.15656661987305
epoch 61400  training loss: 0.008124122396111488

 62%|█████████████████████████████████████████▏                         | 61536/100000 [14:14<08:57, 71.58it/s]
epoch 61500  training loss: 0.008084704168140888

 62%|█████████████████████████████████████████▎                         | 61680/100000 [14:16<08:41, 73.50it/s]
epoch 61600  training loss: 0.008045145310461521
epoch 61600  clean testing loss: 54.3814697265625
epoch 61700  training loss: 0.008005420677363873

 62%|█████████████████████████████████████████▍                         | 61832/100000 [14:18<08:44, 72.72it/s]
epoch 61800  training loss: 0.007967546582221985

 62%|█████████████████████████████████████████▌                         | 61976/100000 [14:20<08:41, 72.88it/s]
epoch 61900  training loss: 0.00792431179434061

 62%|█████████████████████████████████████████▌                         | 62120/100000 [14:22<08:41, 72.63it/s]
epoch 62000  training loss: 0.007885903120040894
epoch 62000  clean testing loss: 54.684226989746094
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_sin_size500_noise1.00e+00_invop1_lr5e-05 ...
epoch 62100  training loss: 0.007844524458050728

 62%|█████████████████████████████████████████▋                         | 62264/100000 [14:24<08:35, 73.23it/s]
epoch 62200  training loss: 0.007808680180460215

 62%|█████████████████████████████████████████▊                         | 62416/100000 [14:26<08:29, 73.76it/s]
epoch 62300  training loss: 0.007765801623463631
epoch 62300  clean testing loss: 54.91243362426758
epoch 62400  training loss: 0.0077286893501877785

 63%|█████████████████████████████████████████▉                         | 62560/100000 [14:28<08:35, 72.64it/s]
epoch 62500  training loss: 0.007687089499086142

 63%|██████████████████████████████████████████                         | 62704/100000 [14:30<08:31, 72.94it/s]
epoch 62600  training loss: 0.007647812366485596
epoch 62600  clean testing loss: 55.13774490356445
epoch 62700  training loss: 0.007609257940202951

 63%|██████████████████████████████████████████                         | 62848/100000 [14:32<08:30, 72.78it/s]
epoch 62800  training loss: 0.007570311892777681

 63%|██████████████████████████████████████████▏                        | 63000/100000 [14:34<08:29, 72.62it/s]
epoch 62900  training loss: 0.007533689495176077
epoch 62900  clean testing loss: 55.361305236816406
epoch 63000  training loss: 0.007493604905903339
epoch 63000  clean testing loss: 55.43641662597656

 63%|██████████████████████████████████████████▎                        | 63144/100000 [14:36<08:20, 73.63it/s]
epoch 63100  training loss: 0.007461166009306908

 63%|██████████████████████████████████████████▍                        | 63288/100000 [14:38<08:22, 73.06it/s]
epoch 63200  training loss: 0.007428489625453949
epoch 63200  clean testing loss: 55.56015396118164
epoch 63300  training loss: 0.007394334301352501

 63%|██████████████████████████████████████████▌                        | 63440/100000 [14:40<08:21, 72.84it/s]
epoch 63400  training loss: 0.007358551491051912

 64%|██████████████████████████████████████████▌                        | 63584/100000 [14:42<08:17, 73.17it/s]
epoch 63500  training loss: 0.007322411984205246

 64%|██████████████████████████████████████████▋                        | 63728/100000 [14:44<08:21, 72.30it/s]
epoch 63600  training loss: 0.007285423576831818
epoch 63600  clean testing loss: 55.8380126953125
epoch 63700  training loss: 0.007248121779412031

 64%|██████████████████████████████████████████▊                        | 63872/100000 [14:46<08:15, 72.94it/s]
epoch 63800  training loss: 0.007210638374090195

 64%|██████████████████████████████████████████▉                        | 64024/100000 [14:48<08:20, 71.83it/s]
epoch 63900  training loss: 0.007177126128226519
epoch 63900  clean testing loss: 56.05173873901367
epoch 64000  training loss: 0.00713724922388792
epoch 64000  clean testing loss: 56.122596740722656

 64%|██████████████████████████████████████████▉                        | 64168/100000 [14:50<08:10, 73.02it/s]
epoch 64100  training loss: 0.00710034416988492

 64%|███████████████████████████████████████████                        | 64312/100000 [14:52<08:14, 72.24it/s]
epoch 64200  training loss: 0.007075243629515171
epoch 64200  clean testing loss: 56.262943267822266
epoch 64300  training loss: 0.007028602994978428

 64%|███████████████████████████████████████████▏                       | 64456/100000 [14:54<08:00, 73.91it/s]
epoch 64400  training loss: 0.0069912876933813095

 65%|███████████████████████████████████████████▎                       | 64608/100000 [14:56<08:09, 72.27it/s]
epoch 64500  training loss: 0.006956442259252071
epoch 64500  clean testing loss: 56.47480392456055
epoch 64600  training loss: 0.006919454783201218

 65%|███████████████████████████████████████████▍                       | 64752/100000 [14:58<07:58, 73.66it/s]
epoch 64700  training loss: 0.006882895715534687

 65%|███████████████████████████████████████████▍                       | 64896/100000 [15:00<08:04, 72.42it/s]
epoch 64800  training loss: 0.006846757140010595
epoch 64800  clean testing loss: 56.68521499633789
epoch 64900  training loss: 0.006810564547777176

 65%|███████████████████████████████████████████▌                       | 65040/100000 [15:02<08:05, 72.08it/s]
epoch 65000  training loss: 0.006775176152586937
epoch 65000  clean testing loss: 56.822792053222656

 65%|███████████████████████████████████████████▋                       | 65184/100000 [15:04<08:01, 72.31it/s]
epoch 65100  training loss: 0.006743237841874361

 65%|███████████████████████████████████████████▊                       | 65336/100000 [15:06<07:54, 72.99it/s]
epoch 65200  training loss: 0.006703583989292383
epoch 65200  clean testing loss: 56.96088790893555
epoch 65300  training loss: 0.006671021692454815

 65%|███████████████████████████████████████████▊                       | 65480/100000 [15:08<07:53, 72.83it/s]
epoch 65400  training loss: 0.006631305906921625

 66%|███████████████████████████████████████████▉                       | 65624/100000 [15:10<07:53, 72.56it/s]
epoch 65500  training loss: 0.006595862563699484
epoch 65500  clean testing loss: 57.16740798950195
epoch 65600  training loss: 0.006560753099620342

 66%|████████████████████████████████████████████                       | 65776/100000 [15:12<07:48, 73.03it/s]
epoch 65700  training loss: 0.00652878824621439

 66%|████████████████████████████████████████████▏                      | 65920/100000 [15:14<07:52, 72.05it/s]
epoch 65800  training loss: 0.00649415235966444
epoch 65800  clean testing loss: 57.371337890625
epoch 65900  training loss: 0.006454227492213249

 66%|████████████████████████████████████████████▎                      | 66064/100000 [15:16<07:44, 73.11it/s]
epoch 66000  training loss: 0.006418617907911539
epoch 66000  clean testing loss: 57.50652313232422

 66%|████████████████████████████████████████████▎                      | 66208/100000 [15:18<07:43, 72.91it/s]
epoch 66100  training loss: 0.006389816291630268
epoch 66100  clean testing loss: 57.56175994873047
epoch 66200  training loss: 0.006359787192195654

 66%|████████████████████████████████████████████▍                      | 66360/100000 [15:20<07:45, 72.20it/s]
epoch 66300  training loss: 0.006328078452497721

 67%|████████████████████████████████████████████▌                      | 66504/100000 [15:22<07:35, 73.60it/s]
epoch 66400  training loss: 0.006295269355177879
epoch 66400  clean testing loss: 57.741416931152344
epoch 66500  training loss: 0.006261711940169334

 67%|████████████████████████████████████████████▋                      | 66648/100000 [15:24<07:39, 72.65it/s]
epoch 66600  training loss: 0.0062276870012283325

 67%|████████████████████████████████████████████▊                      | 66792/100000 [15:26<07:36, 72.76it/s]
epoch 66700  training loss: 0.006194006651639938
epoch 66700  clean testing loss: 57.93367385864258
epoch 66800  training loss: 0.006160294637084007

 67%|████████████████████████████████████████████▊                      | 66944/100000 [15:28<07:29, 73.52it/s]
epoch 66900  training loss: 0.006125620100647211

 67%|████████████████████████████████████████████▉                      | 67088/100000 [15:30<07:32, 72.81it/s]
epoch 67000  training loss: 0.00609218142926693
epoch 67000  clean testing loss: 58.12457275390625

 67%|█████████████████████████████████████████████                      | 67232/100000 [15:32<07:33, 72.19it/s]
epoch 67100  training loss: 0.006058556027710438
epoch 67100  clean testing loss: 58.18804931640625
epoch 67200  training loss: 0.006024024449288845

 67%|█████████████████████████████████████████████▏                     | 67376/100000 [15:34<07:27, 72.84it/s]
epoch 67300  training loss: 0.005990433041006327

 68%|█████████████████████████████████████████████▏                     | 67528/100000 [15:36<07:27, 72.54it/s]
epoch 67400  training loss: 0.005959217436611652
epoch 67400  clean testing loss: 58.3769416809082
epoch 67500  training loss: 0.005922863259911537

 68%|█████████████████████████████████████████████▎                     | 67672/100000 [15:38<07:23, 72.92it/s]
epoch 67600  training loss: 0.005891745910048485

 68%|█████████████████████████████████████████████▍                     | 67816/100000 [15:40<07:16, 73.70it/s]
epoch 67700  training loss: 0.0058565265499055386
epoch 67700  clean testing loss: 58.563690185546875
epoch 67800  training loss: 0.005823473446071148

 68%|█████████████████████████████████████████████▌                     | 67960/100000 [15:42<07:20, 72.69it/s]
epoch 67900  training loss: 0.005789407528936863

 68%|█████████████████████████████████████████████▋                     | 68112/100000 [15:44<07:18, 72.77it/s]
epoch 68000  training loss: 0.005756288766860962
epoch 68000  clean testing loss: 58.74802017211914
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_sin_size500_noise1.00e+00_invop1_lr5e-05 ...
epoch 68100  training loss: 0.005723152309656143

 68%|█████████████████████████████████████████████▋                     | 68256/100000 [15:46<07:15, 72.81it/s]
epoch 68200  training loss: 0.005689291749149561

 68%|█████████████████████████████████████████████▊                     | 68400/100000 [15:48<07:16, 72.38it/s]
epoch 68300  training loss: 0.005659183021634817
epoch 68300  clean testing loss: 58.93152618408203
epoch 68400  training loss: 0.005623334087431431

 69%|█████████████████████████████████████████████▉                     | 68544/100000 [15:50<07:18, 71.78it/s]
epoch 68500  training loss: 0.005590311251580715

 69%|██████████████████████████████████████████████                     | 68696/100000 [15:52<07:04, 73.78it/s]
epoch 68600  training loss: 0.005556893534958363

 69%|██████████████████████████████████████████████                     | 68840/100000 [15:54<07:07, 72.83it/s]
epoch 68700  training loss: 0.005524838343262672
epoch 68700  clean testing loss: 59.17204284667969
epoch 68800  training loss: 0.00549025135114789

 69%|██████████████████████████████████████████████▏                    | 68984/100000 [15:56<07:05, 72.97it/s]
epoch 68900  training loss: 0.005459633655846119

 69%|██████████████████████████████████████████████▎                    | 69136/100000 [15:58<07:07, 72.18it/s]
epoch 69000  training loss: 0.005424738395959139
epoch 69000  clean testing loss: 59.35118103027344
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_sin_size500_noise1.00e+00_invop1_lr5e-05 ...
epoch 69100  training loss: 0.005397268570959568

 69%|██████████████████████████████████████████████▍                    | 69280/100000 [16:00<06:58, 73.38it/s]
epoch 69200  training loss: 0.005369812250137329

 69%|██████████████████████████████████████████████▌                    | 69424/100000 [16:02<07:07, 71.51it/s]
epoch 69300  training loss: 0.005340675823390484
epoch 69300  clean testing loss: 59.50078582763672
epoch 69400  training loss: 0.0053104860708117485

 70%|██████████████████████████████████████████████▌                    | 69568/100000 [16:04<06:56, 73.06it/s]
epoch 69500  training loss: 0.005279392935335636

 70%|██████████████████████████████████████████████▋                    | 69712/100000 [16:06<06:59, 72.16it/s]
epoch 69600  training loss: 0.00524781784042716
epoch 69600  clean testing loss: 59.666080474853516
epoch 69700  training loss: 0.005216259974986315

 70%|██████████████████████████████████████████████▊                    | 69864/100000 [16:08<06:59, 71.85it/s]
epoch 69800  training loss: 0.005184501875191927

 70%|██████████████████████████████████████████████▉                    | 70008/100000 [16:10<06:59, 71.53it/s]
epoch 69900  training loss: 0.005153133533895016
epoch 69900  clean testing loss: 59.8332633972168
epoch 70000  training loss: 0.005123767536133528
epoch 70000  clean testing loss: 59.88923645019531

 70%|███████████████████████████████████████████████                    | 70152/100000 [16:12<06:48, 73.05it/s]
epoch 70100  training loss: 0.005090032704174519

 70%|███████████████████████████████████████████████                    | 70296/100000 [16:14<06:48, 72.71it/s]
epoch 70200  training loss: 0.00505951140075922
epoch 70200  clean testing loss: 59.9987907409668
epoch 70300  training loss: 0.005026873666793108

 70%|███████████████████████████████████████████████▏                   | 70448/100000 [16:17<06:45, 72.87it/s]
epoch 70400  training loss: 0.0049969954416155815

 71%|███████████████████████████████████████████████▎                   | 70592/100000 [16:18<06:42, 73.12it/s]
epoch 70500  training loss: 0.004965071100741625

 71%|███████████████████████████████████████████████▍                   | 70736/100000 [16:20<06:41, 72.92it/s]
epoch 70600  training loss: 0.004933221265673637
epoch 70600  clean testing loss: 60.21751022338867
epoch 70700  training loss: 0.004902678541839123

 71%|███████████████████████████████████████████████▍                   | 70880/100000 [16:22<06:39, 72.94it/s]
epoch 70800  training loss: 0.004870579112321138

 71%|███████████████████████████████████████████████▌                   | 71024/100000 [16:24<06:36, 73.03it/s]
epoch 70900  training loss: 0.004840002860873938
epoch 70900  clean testing loss: 60.37998580932617
epoch 71000  training loss: 0.004807852208614349
epoch 71000  clean testing loss: 60.43309020996094

 71%|███████████████████████████████████████████████▋                   | 71176/100000 [16:27<06:38, 72.25it/s]
epoch 71100  training loss: 0.0047780415043234825

 71%|███████████████████████████████████████████████▊                   | 71320/100000 [16:29<06:32, 73.04it/s]
epoch 71200  training loss: 0.004745684564113617
epoch 71200  clean testing loss: 60.54004669189453
epoch 71300  training loss: 0.004715513437986374

 71%|███████████████████████████████████████████████▉                   | 71464/100000 [16:30<06:31, 72.81it/s]
epoch 71400  training loss: 0.00468369061127305

 72%|███████████████████████████████████████████████▉                   | 71608/100000 [16:32<06:31, 72.48it/s]
epoch 71500  training loss: 0.004653905052691698
epoch 71500  clean testing loss: 60.69903564453125
epoch 71600  training loss: 0.00462130643427372

 72%|████████████████████████████████████████████████                   | 71752/100000 [16:34<06:29, 72.46it/s]
epoch 71700  training loss: 0.0045908112078905106

 72%|████████████████████████████████████████████████▏                  | 71904/100000 [16:37<06:23, 73.33it/s]
epoch 71800  training loss: 0.004559115506708622
epoch 71800  clean testing loss: 60.85719680786133
epoch 71900  training loss: 0.004528495948761702

 72%|████████████████████████████████████████████████▎                  | 72048/100000 [16:39<06:20, 73.52it/s]
epoch 72000  training loss: 0.004497619811445475
epoch 72000  clean testing loss: 60.96095657348633

 72%|████████████████████████████████████████████████▎                  | 72192/100000 [16:41<06:21, 72.95it/s]
epoch 72100  training loss: 0.004472056403756142

 72%|████████████████████████████████████████████████▍                  | 72344/100000 [16:43<06:18, 73.15it/s]
epoch 72200  training loss: 0.004446262493729591
epoch 72200  clean testing loss: 61.047698974609375
epoch 72300  training loss: 0.004419000819325447

 72%|████████████████████████████████████████████████▌                  | 72488/100000 [16:45<06:18, 72.76it/s]
epoch 72400  training loss: 0.0043903193436563015

 73%|████████████████████████████████████████████████▋                  | 72632/100000 [16:47<06:19, 72.08it/s]
epoch 72500  training loss: 0.004361719358712435
epoch 72500  clean testing loss: 61.18958282470703
epoch 72600  training loss: 0.0043332683853805065

 73%|████████████████████████████████████████████████▊                  | 72776/100000 [16:49<06:11, 73.24it/s]
epoch 72700  training loss: 0.004303334280848503

 73%|████████████████████████████████████████████████▊                  | 72920/100000 [16:51<06:11, 72.94it/s]
epoch 72800  training loss: 0.004274711012840271
epoch 72800  clean testing loss: 61.335205078125
epoch 72900  training loss: 0.004247660282999277

 73%|████████████████████████████████████████████████▉                  | 73072/100000 [16:53<06:09, 72.88it/s]
epoch 73000  training loss: 0.0042161354795098305
epoch 73000  clean testing loss: 61.431034088134766

 73%|█████████████████████████████████████████████████                  | 73216/100000 [16:55<06:07, 72.93it/s]
epoch 73100  training loss: 0.004187420010566711
epoch 73100  clean testing loss: 61.47911071777344
epoch 73200  training loss: 0.00415847497060895

 73%|█████████████████████████████████████████████████▏                 | 73360/100000 [16:57<06:08, 72.29it/s]
epoch 73300  training loss: 0.004129635635763407

 74%|█████████████████████████████████████████████████▏                 | 73504/100000 [16:59<06:08, 71.96it/s]
epoch 73400  training loss: 0.004101135767996311
epoch 73400  clean testing loss: 61.620948791503906
epoch 73500  training loss: 0.004072561394423246

 74%|█████████████████████████████████████████████████▎                 | 73656/100000 [17:01<06:02, 72.69it/s]
epoch 73600  training loss: 0.0040440126322209835

 74%|█████████████████████████████████████████████████▍                 | 73800/100000 [17:03<05:55, 73.71it/s]
epoch 73700  training loss: 0.004016307182610035

 74%|█████████████████████████████████████████████████▌                 | 73944/100000 [17:05<05:57, 72.94it/s]
epoch 73800  training loss: 0.003987107425928116
epoch 73800  clean testing loss: 61.8088493347168
epoch 73900  training loss: 0.003958352375775576

 74%|█████████████████████████████████████████████████▋                 | 74088/100000 [17:07<05:54, 73.20it/s]
epoch 74000  training loss: 0.003929840866476297
epoch 74000  clean testing loss: 61.90238571166992

 74%|█████████████████████████████████████████████████▋                 | 74240/100000 [17:09<05:51, 73.35it/s]
epoch 74100  training loss: 0.003901274409145117
epoch 74100  clean testing loss: 61.9488639831543
epoch 74200  training loss: 0.003873989684507251

 74%|█████████████████████████████████████████████████▊                 | 74384/100000 [17:11<05:50, 73.14it/s]
epoch 74300  training loss: 0.0038445915561169386

 75%|█████████████████████████████████████████████████▉                 | 74528/100000 [17:13<05:50, 72.57it/s]
epoch 74400  training loss: 0.00381658342666924
epoch 74400  clean testing loss: 62.088199615478516
epoch 74500  training loss: 0.003790112677961588

 75%|██████████████████████████████████████████████████                 | 74672/100000 [17:15<05:49, 72.46it/s]
epoch 74600  training loss: 0.003760208608582616

 75%|██████████████████████████████████████████████████▏                | 74824/100000 [17:17<05:49, 71.97it/s]
epoch 74700  training loss: 0.0037319103721529245
epoch 74700  clean testing loss: 62.22600173950195
epoch 74800  training loss: 0.003704011905938387

 75%|██████████████████████████████████████████████████▏                | 74968/100000 [17:19<05:45, 72.53it/s]
epoch 74900  training loss: 0.003676454769447446

 75%|██████████████████████████████████████████████████▎                | 75112/100000 [17:21<05:45, 72.00it/s]
epoch 75000  training loss: 0.0036499572452157736
epoch 75000  clean testing loss: 62.362823486328125
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_sin_size500_noise1.00e+00_invop1_lr5e-05 ...
epoch 75100  training loss: 0.0036257407627999783

 75%|██████████████████████████████████████████████████▍                | 75256/100000 [17:23<05:35, 73.69it/s]
epoch 75200  training loss: 0.0036024178843945265

 75%|██████████████████████████████████████████████████▌                | 75408/100000 [17:25<05:38, 72.71it/s]
epoch 75300  training loss: 0.0035781071055680513

 76%|██████████████████████████████████████████████████▌                | 75552/100000 [17:27<05:32, 73.61it/s]
epoch 75400  training loss: 0.0035534074995666742
epoch 75400  clean testing loss: 62.5173454284668
epoch 75500  training loss: 0.003527545602992177

 76%|██████████████████████████████████████████████████▋                | 75664/100000 [17:28<05:33, 72.95it/s]
epoch 75600  training loss: 0.0035015942994505167

 76%|██████████████████████████████████████████████████▊                | 75808/100000 [17:30<05:32, 72.66it/s]
epoch 75700  training loss: 0.0034756662789732218
epoch 75700  clean testing loss: 62.644020080566406
epoch 75800  training loss: 0.0034498674795031548

 76%|██████████████████████████████████████████████████▉                | 75952/100000 [17:32<05:28, 73.25it/s]
epoch 75900  training loss: 0.0034238253720104694

 76%|██████████████████████████████████████████████████▉                | 76096/100000 [17:34<05:26, 73.19it/s]
epoch 76000  training loss: 0.0033983064349740744
epoch 76000  clean testing loss: 62.770103454589844
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_sin_size500_noise1.00e+00_invop1_lr5e-05 ...
epoch 76100  training loss: 0.003372615436092019

 76%|███████████████████████████████████████████████████                | 76248/100000 [17:36<05:25, 72.90it/s]
epoch 76200  training loss: 0.0033497661352157593

 76%|███████████████████████████████████████████████████▏               | 76392/100000 [17:38<05:24, 72.76it/s]
epoch 76300  training loss: 0.003321417374536395
epoch 76300  clean testing loss: 62.894737243652344
epoch 76400  training loss: 0.003296387381851673

 77%|███████████████████████████████████████████████████▎               | 76536/100000 [17:40<05:19, 73.45it/s]
epoch 76500  training loss: 0.0032710705418139696

 77%|███████████████████████████████████████████████████▍               | 76680/100000 [17:42<05:18, 73.23it/s]
epoch 76600  training loss: 0.0032453779131174088
epoch 76600  clean testing loss: 63.01864242553711
epoch 76700  training loss: 0.0032208759803324938

 77%|███████████████████████████████████████████████████▍               | 76832/100000 [17:44<05:17, 73.05it/s]
epoch 76800  training loss: 0.0031953779980540276

 77%|███████████████████████████████████████████████████▌               | 76976/100000 [17:46<05:16, 72.81it/s]
epoch 76900  training loss: 0.003170613432303071

 77%|███████████████████████████████████████████████████▋               | 77120/100000 [17:48<05:14, 72.76it/s]
epoch 77000  training loss: 0.0031453296542167664
epoch 77000  clean testing loss: 63.182861328125
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_sin_size500_noise1.00e+00_invop1_lr5e-05 ...
epoch 77100  training loss: 0.0031202388927340508

 77%|███████████████████████████████████████████████████▊               | 77264/100000 [17:50<05:11, 73.03it/s]
epoch 77200  training loss: 0.003095613792538643

 77%|███████████████████████████████████████████████████▊               | 77416/100000 [17:52<05:09, 72.98it/s]
epoch 77300  training loss: 0.0030707689002156258
epoch 77300  clean testing loss: 63.30486297607422
epoch 77400  training loss: 0.0030459952540695667

 78%|███████████████████████████████████████████████████▉               | 77560/100000 [17:54<05:07, 72.92it/s]
epoch 77500  training loss: 0.0030217082239687443

 78%|████████████████████████████████████████████████████               | 77704/100000 [17:56<05:07, 72.54it/s]
epoch 77600  training loss: 0.0029969133902341127
epoch 77600  clean testing loss: 63.42596435546875
epoch 77700  training loss: 0.002972518792375922

 78%|████████████████████████████████████████████████████▏              | 77848/100000 [17:58<05:04, 72.69it/s]
epoch 77800  training loss: 0.002949258079752326

 78%|████████████████████████████████████████████████████▎              | 78000/100000 [18:00<05:03, 72.55it/s]
epoch 77900  training loss: 0.002924148226156831
epoch 77900  clean testing loss: 63.54631423950195
epoch 78000  training loss: 0.0028999203350394964
epoch 78000  clean testing loss: 63.58640670776367

 78%|████████████████████████████████████████████████████▎              | 78144/100000 [18:02<04:58, 73.29it/s]
epoch 78100  training loss: 0.002879920182749629

 78%|████████████████████████████████████████████████████▍              | 78288/100000 [18:04<04:57, 72.87it/s]
epoch 78200  training loss: 0.0028597088530659676
epoch 78200  clean testing loss: 63.65336227416992
epoch 78300  training loss: 0.0028385063633322716

 78%|████████████████████████████████████████████████████▌              | 78432/100000 [18:06<04:59, 71.95it/s]
epoch 78400  training loss: 0.002816949039697647

 79%|████████████████████████████████████████████████████▋              | 78584/100000 [18:08<04:57, 71.99it/s]
epoch 78500  training loss: 0.002794973086565733
epoch 78500  clean testing loss: 63.762020111083984
epoch 78600  training loss: 0.0027724071405828

 79%|████████████████████████████████████████████████████▋              | 78728/100000 [18:10<04:52, 72.65it/s]
epoch 78700  training loss: 0.002750369254499674

 79%|████████████████████████████████████████████████████▊              | 78872/100000 [18:12<04:50, 72.67it/s]
epoch 78800  training loss: 0.002728105755522847

 79%|████████████████████████████████████████████████████▉              | 79016/100000 [18:14<04:51, 71.90it/s]
epoch 78900  training loss: 0.002706297906115651
epoch 78900  clean testing loss: 63.90936279296875
epoch 79000  training loss: 0.0026847482658922672
epoch 79000  clean testing loss: 63.94652557373047

 79%|█████████████████████████████████████████████████████              | 79168/100000 [18:16<04:46, 72.74it/s]
epoch 79100  training loss: 0.0026632219087332487

 79%|█████████████████████████████████████████████████████▏             | 79312/100000 [18:18<04:42, 73.21it/s]
epoch 79200  training loss: 0.0026414429303258657
epoch 79200  clean testing loss: 64.0190658569336
epoch 79300  training loss: 0.0026199251879006624

 79%|█████████████████████████████████████████████████████▏             | 79456/100000 [18:20<04:39, 73.40it/s]
epoch 79400  training loss: 0.002598831197246909

 80%|█████████████████████████████████████████████████████▎             | 79600/100000 [18:22<04:43, 72.05it/s]
epoch 79500  training loss: 0.002576954895630479
epoch 79500  clean testing loss: 64.12794494628906
epoch 79600  training loss: 0.002555761719122529

 80%|█████████████████████████████████████████████████████▍             | 79752/100000 [18:24<04:36, 73.26it/s]
epoch 79700  training loss: 0.0025347962509840727

 80%|█████████████████████████████████████████████████████▌             | 79896/100000 [18:26<04:38, 72.21it/s]
epoch 79800  training loss: 0.002513687591999769
epoch 79800  clean testing loss: 64.23648834228516
epoch 79900  training loss: 0.00249232049100101

 80%|█████████████████████████████████████████████████████▋             | 80040/100000 [18:28<04:34, 72.59it/s]
epoch 80000  training loss: 0.002471786690875888
epoch 80000  clean testing loss: 64.30894470214844

 80%|█████████████████████████████████████████████████████▋             | 80184/100000 [18:30<04:36, 71.74it/s]
epoch 80100  training loss: 0.002450676169246435
epoch 80100  clean testing loss: 64.34459686279297
epoch 80200  training loss: 0.0024299349170178175

 80%|█████████████████████████████████████████████████████▊             | 80336/100000 [18:32<04:28, 73.15it/s]
epoch 80300  training loss: 0.002409275621175766

 80%|█████████████████████████████████████████████████████▉             | 80480/100000 [18:34<04:23, 74.09it/s]
epoch 80400  training loss: 0.0023886722046881914

 81%|██████████████████████████████████████████████████████             | 80624/100000 [18:36<04:24, 73.12it/s]
epoch 80500  training loss: 0.0023682708851993084
epoch 80500  clean testing loss: 64.48811340332031
epoch 80600  training loss: 0.0023476406931877136

 81%|██████████████████████████████████████████████████████             | 80776/100000 [18:38<04:21, 73.58it/s]
epoch 80700  training loss: 0.0023275113198906183

 81%|██████████████████████████████████████████████████████▏            | 80920/100000 [18:40<04:21, 72.93it/s]
epoch 80800  training loss: 0.0023077235091477633
epoch 80800  clean testing loss: 64.59471893310547
epoch 80900  training loss: 0.002287455601617694

 81%|██████████████████████████████████████████████████████▎            | 81064/100000 [18:42<04:22, 72.27it/s]
epoch 81000  training loss: 0.002267402596771717
epoch 81000  clean testing loss: 64.66510009765625

 81%|██████████████████████████████████████████████████████▍            | 81216/100000 [18:44<04:16, 73.11it/s]
epoch 81100  training loss: 0.0022512993309646845
epoch 81100  clean testing loss: 64.69389343261719
epoch 81200  training loss: 0.0022344652097672224

 81%|██████████████████████████████████████████████████████▌            | 81360/100000 [18:46<04:13, 73.66it/s]
epoch 81300  training loss: 0.002217562636360526

 82%|██████████████████████████████████████████████████████▌            | 81504/100000 [18:48<04:15, 72.28it/s]
epoch 81400  training loss: 0.002199849346652627
epoch 81400  clean testing loss: 64.78639221191406
epoch 81500  training loss: 0.0021819595713168383

 82%|██████████████████████████████████████████████████████▋            | 81648/100000 [18:50<04:13, 72.28it/s]
epoch 81600  training loss: 0.0021635673474520445

 82%|██████████████████████████████████████████████████████▊            | 81792/100000 [18:52<04:08, 73.19it/s]
epoch 81700  training loss: 0.0021457888651639223
epoch 81700  clean testing loss: 64.88424682617188
epoch 81800  training loss: 0.0021275950130075216

 82%|██████████████████████████████████████████████████████▉            | 81944/100000 [18:54<04:09, 72.28it/s]
epoch 81900  training loss: 0.0021099511068314314

 82%|██████████████████████████████████████████████████████▉            | 82088/100000 [18:56<04:04, 73.24it/s]
epoch 82000  training loss: 0.002091815695166588
epoch 82000  clean testing loss: 64.98175811767578
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_sin_size500_noise1.00e+00_invop1_lr5e-05 ...
epoch 82100  training loss: 0.0020742539782077074

 82%|███████████████████████████████████████████████████████            | 82232/100000 [18:58<04:06, 72.18it/s]
epoch 82200  training loss: 0.0020568969193845987

 82%|███████████████████████████████████████████████████████▏           | 82376/100000 [19:00<04:01, 73.11it/s]
epoch 82300  training loss: 0.0020393214654177427

 83%|███████████████████████████████████████████████████████▎           | 82528/100000 [19:03<04:00, 72.78it/s]
epoch 82400  training loss: 0.002022012136876583
epoch 82400  clean testing loss: 65.11040496826172
epoch 82500  training loss: 0.002004605019465089

 83%|███████████████████████████████████████████████████████▎           | 82600/100000 [19:04<04:00, 72.20it/s]
epoch 82600  training loss: 0.0019876337610185146
epoch 82600  clean testing loss: 65.1745834350586
Validation loss variation < 1e-6, trained to interpolation, stop
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_sin_size500_noise1.00e+00_invop1_lr5e-05 ...