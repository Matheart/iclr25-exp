epoch 0  training loss: 98.51303100585938
epoch 0  clean testing loss: 41.55171203613281
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise5.00e+01_invop1 ...
  0%|                                                                                 | 36/300000 [00:01<1:54:54, 43.51it/s]
epoch 100  training loss: 46.82087326049805
epoch 100  clean testing loss: 1.507490634918213
epoch 200  training loss: 44.92941665649414

  0%|                                                                                | 169/300000 [00:03<1:15:15, 66.40it/s]
epoch 300  training loss: 44.112152099609375

  0%|                                                                                | 302/300000 [00:05<1:14:55, 66.67it/s]
epoch 400  training loss: 43.1085090637207

  0%|                                                                                | 443/300000 [00:07<1:06:20, 75.26it/s]
epoch 500  training loss: 41.87763214111328
epoch 500  clean testing loss: 2.2856924533843994
epoch 600  training loss: 40.5460090637207


  0%|▏                                                                               | 819/300000 [00:12<1:16:15, 65.39it/s]
epoch 700  training loss: 38.74052047729492
epoch 700  clean testing loss: 4.597594261169434
epoch 800  training loss: 36.98154830932617
epoch 800  clean testing loss: 6.429154396057129
epoch 900  training loss: 34.35593795776367


  0%|▎                                                                              | 1085/300000 [00:17<1:15:42, 65.80it/s]
epoch 1000  training loss: 32.94561767578125
epoch 1000  clean testing loss: 10.447958946228027

  0%|▎                                                                              | 1218/300000 [00:19<1:14:58, 66.42it/s]
epoch 1100  training loss: 30.34722328186035
epoch 1100  clean testing loss: 12.187578201293945
epoch 1200  training loss: 29.386837005615234

  0%|▎                                                                              | 1351/300000 [00:21<1:16:10, 65.35it/s]
epoch 1300  training loss: 27.10569190979004
epoch 1300  clean testing loss: 17.698219299316406
epoch 1400  training loss: 26.524051666259766

  0%|▍                                                                              | 1484/300000 [00:23<1:15:38, 65.77it/s]
epoch 1500  training loss: 25.291244506835938
epoch 1500  clean testing loss: 20.982173919677734
epoch 1600  training loss: 25.49860191345215


  1%|▍                                                                              | 1750/300000 [00:27<1:14:20, 66.87it/s]
epoch 1700  training loss: 22.976289749145508

  1%|▍                                                                              | 1883/300000 [00:29<1:15:08, 66.13it/s]
epoch 1800  training loss: 24.985685348510742

  1%|▌                                                                              | 2009/300000 [00:31<1:16:25, 64.99it/s]
epoch 1900  training loss: 24.524707794189453
epoch 1900  clean testing loss: 25.735740661621094
epoch 2000  training loss: 22.759746551513672
epoch 2000  clean testing loss: 25.417001724243164
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise5.00e+01_invop1 ...
epoch 2100  training loss: 23.223552703857422

  1%|▌                                                                              | 2142/300000 [00:33<1:15:30, 65.75it/s]
epoch 2200  training loss: 22.444915771484375

  1%|▌                                                                              | 2275/300000 [00:35<1:15:17, 65.91it/s]
epoch 2300  training loss: 20.700531005859375
epoch 2300  clean testing loss: 25.347822189331055
epoch 2400  training loss: 22.093875885009766


  1%|▋                                                                              | 2541/300000 [00:39<1:14:39, 66.40it/s]
epoch 2500  training loss: 21.386571884155273
epoch 2500  clean testing loss: 26.64440155029297
epoch 2600  training loss: 20.10515785217285


  1%|▋                                                                              | 2807/300000 [00:43<1:14:08, 66.81it/s]
epoch 2700  training loss: 19.63218879699707
epoch 2700  clean testing loss: 28.66877555847168
epoch 2800  training loss: 19.87833595275879
epoch 2800  clean testing loss: 29.638826370239258
epoch 2900  training loss: 22.014610290527344

  1%|▊                                                                              | 2940/300000 [00:45<1:14:45, 66.23it/s]
epoch 3000  training loss: 20.751184463500977
epoch 3000  clean testing loss: 29.42041015625
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise5.00e+01_invop1 ...
epoch 3100  training loss: 18.38298225402832


  1%|▊                                                                              | 3255/300000 [00:53<1:14:47, 66.12it/s]
epoch 3200  training loss: 18.40777015686035

  1%|▉                                                                              | 3381/300000 [00:55<1:17:17, 63.96it/s]
epoch 3300  training loss: 17.279937744140625

  1%|▉                                                                              | 3514/300000 [00:57<1:16:15, 64.80it/s]
epoch 3400  training loss: 18.268606185913086
epoch 3400  clean testing loss: 35.32085418701172
epoch 3500  training loss: 18.48370361328125
epoch 3500  clean testing loss: 35.3109016418457
epoch 3600  training loss: 15.78933334350586

  1%|▉                                                                              | 3647/300000 [00:59<1:15:14, 65.65it/s]
epoch 3700  training loss: 18.60024070739746

  1%|▉                                                                              | 3780/300000 [01:01<1:13:49, 66.88it/s]
epoch 3800  training loss: 16.97231674194336
epoch 3800  clean testing loss: 37.51570510864258
epoch 3900  training loss: 14.75120735168457

  1%|█                                                                              | 3913/300000 [01:03<1:13:48, 66.86it/s]
epoch 4000  training loss: 16.726512908935547
epoch 4000  clean testing loss: 40.04171371459961

  1%|█                                                                              | 4046/300000 [01:05<1:14:59, 65.77it/s]
epoch 4100  training loss: 16.37470817565918

  1%|█                                                                              | 4179/300000 [01:07<1:15:53, 64.96it/s]
epoch 4200  training loss: 16.930025100708008
epoch 4200  clean testing loss: 37.0325813293457
epoch 4300  training loss: 14.737287521362305

  1%|█▏                                                                             | 4312/300000 [01:09<1:14:55, 65.77it/s]
epoch 4400  training loss: 16.325748443603516

  1%|█▏                                                                             | 4445/300000 [01:11<1:14:16, 66.32it/s]
epoch 4500  training loss: 14.84704875946045

  2%|█▏                                                                             | 4578/300000 [01:13<1:13:45, 66.75it/s]
epoch 4600  training loss: 15.684124946594238

  2%|█▏                                                                             | 4711/300000 [01:15<1:14:34, 65.99it/s]
epoch 4700  training loss: 14.064233779907227
epoch 4700  clean testing loss: 41.0298957824707
epoch 4800  training loss: 16.41537857055664

  2%|█▎                                                                             | 4837/300000 [01:17<1:15:54, 64.81it/s]
epoch 4900  training loss: 15.075983047485352

  2%|█▎                                                                             | 4970/300000 [01:19<1:14:11, 66.27it/s]
epoch 5000  training loss: 15.10659122467041
epoch 5000  clean testing loss: 42.35173797607422


  2%|█▍                                                                             | 5236/300000 [01:23<1:13:29, 66.84it/s]
epoch 5100  training loss: 15.546080589294434
epoch 5100  clean testing loss: 39.596012115478516
epoch 5200  training loss: 15.752828598022461
epoch 5200  clean testing loss: 39.54421615600586
epoch 5300  training loss: 15.453927993774414

  2%|█▍                                                                             | 5376/300000 [01:25<1:13:34, 66.74it/s]
epoch 5400  training loss: 15.22653579711914

  2%|█▍                                                                             | 5509/300000 [01:27<1:14:09, 66.18it/s]
epoch 5500  training loss: 16.9449405670166
epoch 5500  clean testing loss: 38.757850646972656
epoch 5600  training loss: 20.831422805786133

  2%|█▍                                                                             | 5642/300000 [01:29<1:15:05, 65.33it/s]
epoch 5700  training loss: 22.33441162109375


  2%|█▌                                                                             | 5908/300000 [01:33<1:13:56, 66.29it/s]
epoch 5800  training loss: 21.263731002807617

  2%|█▌                                                                             | 5999/300000 [01:34<1:13:39, 66.52it/s]
epoch 5900  training loss: 20.842336654663086
epoch 5900  clean testing loss: 30.41634178161621
epoch 6000  training loss: 19.94066619873047
epoch 6000  clean testing loss: 30.29912567138672

  2%|█▌                                                                             | 6118/300000 [01:37<1:14:14, 65.97it/s]
epoch 6100  training loss: 19.941009521484375

  2%|█▋                                                                             | 6251/300000 [01:39<1:13:47, 66.35it/s]
epoch 6200  training loss: 17.949438095092773
epoch 6200  clean testing loss: 31.649240493774414
epoch 6300  training loss: 18.602336883544922

  2%|█▋                                                                             | 6354/300000 [01:40<1:12:12, 67.78it/s]
epoch 6400  training loss: 18.73003578186035
epoch 6400  clean testing loss: 30.566553115844727
epoch 6500  training loss: 18.20587921142578

  2%|█▋                                                                             | 6487/300000 [01:42<1:13:44, 66.33it/s]
epoch 6600  training loss: 17.083906173706055


  2%|█▊                                                                             | 6753/300000 [01:46<1:14:26, 65.65it/s]
epoch 6700  training loss: 16.128971099853516

  2%|█▊                                                                             | 6886/300000 [01:48<1:14:13, 65.81it/s]
epoch 6800  training loss: 16.47808074951172
epoch 6800  clean testing loss: 35.03396224975586
epoch 6900  training loss: 16.788894653320312

  2%|█▊                                                                             | 6998/300000 [01:50<1:14:05, 65.91it/s]
epoch 7000  training loss: 17.40367317199707
epoch 7000  clean testing loss: 35.88374710083008

  2%|█▊                                                                             | 7096/300000 [01:53<1:17:21, 63.10it/s]
epoch 7100  training loss: 16.8397216796875
epoch 7100  clean testing loss: 35.50782775878906
epoch 7200  training loss: 15.785735130310059


  2%|█▉                                                                             | 7362/300000 [01:57<1:13:22, 66.48it/s]
epoch 7300  training loss: 14.985340118408203
epoch 7300  clean testing loss: 34.135196685791016
epoch 7400  training loss: 15.759086608886719


  3%|██                                                                             | 7628/300000 [02:01<1:14:21, 65.53it/s]
epoch 7500  training loss: 13.920320510864258
epoch 7500  clean testing loss: 36.59709548950195
epoch 7600  training loss: 13.626330375671387
epoch 7600  clean testing loss: 37.3906135559082
epoch 7700  training loss: 15.243756294250488

  3%|██                                                                             | 7761/300000 [02:03<1:12:12, 67.46it/s]
epoch 7800  training loss: 13.78524398803711

  3%|██                                                                             | 7894/300000 [02:05<1:12:26, 67.20it/s]
epoch 7900  training loss: 13.822315216064453
epoch 7900  clean testing loss: 39.51787567138672
epoch 8000  training loss: 14.552023887634277
epoch 8000  clean testing loss: 37.04817199707031

  3%|██                                                                             | 8027/300000 [02:07<1:13:35, 66.13it/s]
epoch 8100  training loss: 14.792643547058105

  3%|██▏                                                                            | 8160/300000 [02:09<1:12:37, 66.97it/s]
epoch 8200  training loss: 13.409078598022461

  3%|██▏                                                                            | 8293/300000 [02:11<1:13:10, 66.43it/s]
epoch 8300  training loss: 14.1024808883667
epoch 8300  clean testing loss: 37.02287673950195
epoch 8400  training loss: 14.46651840209961

  3%|██▏                                                                            | 8426/300000 [02:13<1:12:45, 66.80it/s]
epoch 8500  training loss: 14.922663688659668

  3%|██▎                                                                            | 8559/300000 [02:15<1:12:36, 66.90it/s]
epoch 8600  training loss: 14.633824348449707
epoch 8600  clean testing loss: 40.38650894165039
epoch 8700  training loss: 14.579561233520508
epoch 8700  clean testing loss: 41.4745979309082
epoch 8800  training loss: 15.452262878417969
epoch 8800  clean testing loss: 42.24901580810547
epoch 8900  training loss: 13.177818298339844


  3%|██▍                                                                            | 9091/300000 [02:23<1:13:16, 66.17it/s]
epoch 9000  training loss: 14.253385543823242
epoch 9000  clean testing loss: 39.77564239501953

  3%|██▍                                                                            | 9224/300000 [02:25<1:12:38, 66.71it/s]
epoch 9100  training loss: 14.087775230407715
epoch 9100  clean testing loss: 37.32568359375
epoch 9200  training loss: 14.235803604125977

  3%|██▍                                                                            | 9357/300000 [02:27<1:13:09, 66.21it/s]
epoch 9300  training loss: 12.823326110839844
epoch 9300  clean testing loss: 36.727783203125
epoch 9400  training loss: 13.778800010681152

  3%|██▍                                                                            | 9490/300000 [02:29<1:12:36, 66.68it/s]
epoch 9500  training loss: 12.163784980773926
epoch 9500  clean testing loss: 37.30204772949219
epoch 9600  training loss: 12.99763298034668


  3%|██▌                                                                            | 9756/300000 [02:33<1:12:26, 66.78it/s]
epoch 9700  training loss: 14.325116157531738

  3%|██▌                                                                            | 9889/300000 [02:35<1:12:31, 66.67it/s]
epoch 9800  training loss: 14.232438087463379

  3%|██▌                                                                           | 10022/300000 [02:37<1:13:10, 66.04it/s]
epoch 9900  training loss: 14.689291954040527
epoch 9900  clean testing loss: 36.97159194946289
epoch 10000  training loss: 14.876357078552246
epoch 10000  clean testing loss: 35.96793746948242

  3%|██▋                                                                           | 10155/300000 [02:39<1:13:11, 66.01it/s]
epoch 10100  training loss: 14.187031745910645

  3%|██▋                                                                           | 10288/300000 [02:41<1:13:05, 66.07it/s]
epoch 10200  training loss: 14.531515121459961

  3%|██▋                                                                           | 10421/300000 [02:43<1:12:36, 66.48it/s]
epoch 10300  training loss: 13.076555252075195
epoch 10300  clean testing loss: 39.452999114990234
epoch 10400  training loss: 22.24551010131836

  4%|██▊                                                                           | 10652/300000 [02:47<1:13:28, 65.64it/s]
epoch 10500  training loss: 20.68949317932129
epoch 10500  clean testing loss: 32.282470703125
epoch 10600  training loss: 17.30298614501953
epoch 10600  clean testing loss: 34.09074401855469
epoch 10700  training loss: 15.568403244018555

  4%|██▊                                                                           | 10785/300000 [02:49<1:12:36, 66.38it/s]
epoch 10800  training loss: 15.565773963928223
epoch 10800  clean testing loss: 33.24949645996094
epoch 10900  training loss: 15.433629989624023


  4%|██▊                                                                           | 11051/300000 [02:53<1:12:18, 66.60it/s]
epoch 11000  training loss: 16.196537017822266
epoch 11000  clean testing loss: 35.32562255859375
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise5.00e+01_invop1 ...
epoch 11100  training loss: 14.873092651367188

  4%|██▉                                                                           | 11184/300000 [02:55<1:12:45, 66.15it/s]
epoch 11200  training loss: 15.547098159790039
epoch 11200  clean testing loss: 33.551815032958984
epoch 11300  training loss: 14.918534278869629

  4%|██▉                                                                           | 11324/300000 [02:57<1:12:00, 66.81it/s]
epoch 11400  training loss: 15.080269813537598

  4%|██▉                                                                           | 11457/300000 [02:59<1:12:28, 66.35it/s]
epoch 11500  training loss: 13.716240882873535

  4%|███                                                                           | 11590/300000 [03:01<1:11:50, 66.90it/s]
epoch 11600  training loss: 13.874706268310547
epoch 11600  clean testing loss: 37.80226135253906
epoch 11700  training loss: 13.657649040222168

  4%|███                                                                           | 11723/300000 [03:03<1:12:25, 66.34it/s]
epoch 11800  training loss: 13.040879249572754

  4%|███                                                                           | 11856/300000 [03:05<1:14:55, 64.09it/s]
epoch 11900  training loss: 14.639935493469238

  4%|███                                                                           | 11989/300000 [03:07<1:11:57, 66.70it/s]
epoch 12000  training loss: 14.15697956085205
epoch 12000  clean testing loss: 39.55671310424805
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise5.00e+01_invop1 ...
epoch 12100  training loss: 15.192835807800293

  4%|███▏                                                                          | 12122/300000 [03:09<1:11:22, 67.23it/s]
epoch 12200  training loss: 14.045414924621582

  4%|███▏                                                                          | 12248/300000 [03:11<1:12:56, 65.74it/s]
epoch 12300  training loss: 13.681379318237305

  4%|███▏                                                                          | 12381/300000 [03:13<1:12:23, 66.22it/s]
epoch 12400  training loss: 14.41716194152832
epoch 12400  clean testing loss: 44.44765853881836
epoch 12500  training loss: 14.14220142364502

  4%|███▎                                                                          | 12514/300000 [03:15<1:12:36, 65.99it/s]
epoch 12600  training loss: 14.275100708007812

  4%|███▎                                                                          | 12647/300000 [03:17<1:12:14, 66.29it/s]
epoch 12700  training loss: 13.367974281311035

  4%|███▎                                                                          | 12773/300000 [03:19<1:15:55, 63.05it/s]
epoch 12800  training loss: 14.349007606506348

  4%|███▎                                                                          | 12906/300000 [03:21<1:12:22, 66.11it/s]
epoch 12900  training loss: 11.544102668762207
epoch 12900  clean testing loss: 43.07646942138672
epoch 13000  training loss: 13.105881690979004
epoch 13000  clean testing loss: 43.04789352416992

  4%|███▍                                                                          | 12997/300000 [03:22<1:11:50, 66.59it/s]
epoch 13100  training loss: 12.824440956115723

  4%|███▍                                                                          | 13109/300000 [03:34<1:22:09, 58.20it/s]
epoch 13200  training loss: 13.055233001708984


  4%|███▍                                                                          | 13375/300000 [03:38<1:11:31, 66.79it/s]
epoch 13300  training loss: 13.321791648864746

  5%|███▌                                                                          | 13508/300000 [03:40<1:12:30, 65.85it/s]
epoch 13400  training loss: 13.564970970153809

  5%|███▌                                                                          | 13641/300000 [03:42<1:16:19, 62.52it/s]
epoch 13500  training loss: 13.194572448730469
epoch 13500  clean testing loss: 42.448001861572266
epoch 13600  training loss: 14.08186149597168

  5%|███▌                                                                          | 13767/300000 [03:44<1:11:23, 66.82it/s]
epoch 13700  training loss: 13.22948169708252

  5%|███▌                                                                          | 13900/300000 [03:46<1:11:52, 66.34it/s]
epoch 13800  training loss: 15.033195495605469

  5%|███▋                                                                          | 13998/300000 [03:47<1:11:44, 66.44it/s]
epoch 13900  training loss: 13.537887573242188
epoch 13900  clean testing loss: 41.281620025634766
epoch 14000  training loss: 13.473926544189453
epoch 14000  clean testing loss: 42.157466888427734
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise5.00e+01_invop1 ...
epoch 14100  training loss: 13.1534423828125

  5%|███▋                                                                          | 14110/300000 [03:51<1:13:16, 65.03it/s]
epoch 14200  training loss: 14.133960723876953

  5%|███▋                                                                          | 14243/300000 [03:53<1:12:08, 66.02it/s]
epoch 14300  training loss: 12.814011573791504

  5%|███▋                                                                          | 14376/300000 [03:55<1:11:04, 66.97it/s]
epoch 14400  training loss: 12.79316234588623
epoch 14400  clean testing loss: 40.01018142700195
epoch 14500  training loss: 13.906149864196777

  5%|███▊                                                                          | 14509/300000 [03:57<1:11:59, 66.10it/s]
epoch 14600  training loss: 14.056143760681152

  5%|███▊                                                                          | 14642/300000 [03:59<1:11:49, 66.22it/s]
epoch 14700  training loss: 14.20535945892334

  5%|███▊                                                                          | 14775/300000 [04:01<1:10:57, 66.99it/s]
epoch 14800  training loss: 13.638432502746582
epoch 14800  clean testing loss: 43.11198043823242
epoch 14900  training loss: 14.591233253479004


  5%|███▉                                                                          | 15118/300000 [04:08<1:12:24, 65.57it/s]
epoch 15000  training loss: 14.423095703125
epoch 15000  clean testing loss: 42.51258087158203
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise5.00e+01_invop1 ...
epoch 15100  training loss: 15.791728973388672

  5%|███▉                                                                          | 15251/300000 [04:10<1:11:55, 65.98it/s]
epoch 15200  training loss: 14.03707504272461

  5%|███▉                                                                          | 15384/300000 [04:12<1:11:28, 66.37it/s]
epoch 15300  training loss: 15.971574783325195

  5%|████                                                                          | 15517/300000 [04:14<1:11:01, 66.75it/s]
epoch 15400  training loss: 14.973665237426758
epoch 15400  clean testing loss: 44.055477142333984
epoch 15500  training loss: 16.93596649169922

  5%|████                                                                          | 15650/300000 [04:16<1:11:27, 66.32it/s]
epoch 15600  training loss: 16.214553833007812
epoch 15600  clean testing loss: 42.547672271728516
epoch 15700  training loss: 15.256309509277344


  5%|████▏                                                                         | 15916/300000 [04:20<1:11:44, 66.00it/s]
epoch 15800  training loss: 13.592252731323242
epoch 15800  clean testing loss: 43.10218811035156
epoch 15900  training loss: 16.458953857421875

  5%|████▏                                                                         | 16049/300000 [04:22<1:11:51, 65.85it/s]
epoch 16000  training loss: 15.83949089050293
epoch 16000  clean testing loss: 41.510799407958984

  5%|████▏                                                                         | 16182/300000 [04:24<1:11:00, 66.61it/s]
epoch 16100  training loss: 15.200274467468262

  5%|████▏                                                                         | 16322/300000 [04:26<1:11:07, 66.48it/s]
epoch 16200  training loss: 14.155348777770996

  5%|████▎                                                                         | 16448/300000 [04:28<1:11:43, 65.89it/s]
epoch 16300  training loss: 13.648916244506836
epoch 16300  clean testing loss: 41.01914978027344
epoch 16400  training loss: 13.295350074768066

  6%|████▎                                                                         | 16581/300000 [04:30<1:11:09, 66.39it/s]
epoch 16500  training loss: 14.051356315612793

  6%|████▎                                                                         | 16714/300000 [04:32<1:11:08, 66.37it/s]
epoch 16600  training loss: 14.025330543518066

  6%|████▍                                                                         | 16847/300000 [04:34<1:12:11, 65.36it/s]
epoch 16700  training loss: 15.270684242248535
epoch 16700  clean testing loss: 40.216453552246094
epoch 16800  training loss: 14.328362464904785

  6%|████▍                                                                         | 16980/300000 [04:36<1:10:53, 66.54it/s]
epoch 16900  training loss: 13.540640830993652

  6%|████▍                                                                         | 17113/300000 [04:38<1:10:19, 67.04it/s]
epoch 17000  training loss: 13.974355697631836
epoch 17000  clean testing loss: 42.25839614868164

  6%|████▍                                                                         | 17246/300000 [04:40<1:10:59, 66.39it/s]
epoch 17100  training loss: 14.452543258666992
epoch 17100  clean testing loss: 43.64839172363281
epoch 17200  training loss: 14.641733169555664

  6%|████▌                                                                         | 17379/300000 [04:42<1:11:14, 66.12it/s]
epoch 17300  training loss: 15.969697952270508

  6%|████▌                                                                         | 17512/300000 [04:44<1:10:53, 66.42it/s]
epoch 17400  training loss: 15.596738815307617

  6%|████▌                                                                         | 17645/300000 [04:46<1:11:34, 65.75it/s]
epoch 17500  training loss: 14.301580429077148
epoch 17500  clean testing loss: 44.9581413269043
epoch 17600  training loss: 15.863531112670898

  6%|████▌                                                                         | 17778/300000 [04:48<1:10:26, 66.78it/s]
epoch 17700  training loss: 15.45126724243164

  6%|████▋                                                                         | 18107/300000 [04:58<1:16:02, 61.79it/s]
epoch 17800  training loss: 14.633079528808594
epoch 17800  clean testing loss: 42.3977165222168
epoch 17900  training loss: 15.458043098449707
epoch 17900  clean testing loss: 41.475730895996094
epoch 18000  training loss: 15.444375038146973
epoch 18000  clean testing loss: 43.78801727294922
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise5.00e+01_invop1 ...
epoch 18100  training loss: 14.017133712768555
epoch 18100  clean testing loss: 43.85110855102539
epoch 18200  training loss: 14.365630149841309


  6%|████▊                                                                         | 18373/300000 [05:02<1:10:34, 66.50it/s]
epoch 18300  training loss: 15.041454315185547

  6%|████▊                                                                         | 18506/300000 [05:04<1:11:29, 65.63it/s]
epoch 18400  training loss: 15.32351016998291
epoch 18400  clean testing loss: 41.07977294921875
epoch 18500  training loss: 15.018914222717285

  6%|████▊                                                                         | 18639/300000 [05:06<1:10:31, 66.50it/s]
epoch 18600  training loss: 15.34531307220459

  6%|████▉                                                                         | 18772/300000 [05:08<1:10:40, 66.32it/s]
epoch 18700  training loss: 16.20035743713379

  6%|████▉                                                                         | 18905/300000 [05:10<1:10:35, 66.37it/s]
epoch 18800  training loss: 15.984357833862305
epoch 18800  clean testing loss: 42.531700134277344
epoch 18900  training loss: 15.690725326538086

  6%|████▉                                                                         | 18996/300000 [05:12<1:10:37, 66.32it/s]
epoch 19000  training loss: 15.495455741882324
epoch 19000  clean testing loss: 43.76769256591797

  6%|████▉                                                                         | 19122/300000 [05:15<1:10:57, 65.97it/s]
epoch 19100  training loss: 15.328057289123535

  6%|█████                                                                         | 19255/300000 [05:17<1:10:31, 66.35it/s]
epoch 19200  training loss: 13.637081146240234
epoch 19200  clean testing loss: 43.78255844116211
epoch 19300  training loss: 13.745084762573242


  7%|█████                                                                         | 19521/300000 [05:21<1:10:18, 66.49it/s]
epoch 19400  training loss: 12.79223918914795
epoch 19400  clean testing loss: 43.57237243652344
epoch 19500  training loss: 14.139032363891602

  7%|█████                                                                         | 19654/300000 [05:23<1:11:08, 65.68it/s]
epoch 19600  training loss: 15.416889190673828

  7%|█████▏                                                                        | 19787/300000 [05:25<1:10:19, 66.41it/s]
epoch 19700  training loss: 12.960124015808105

  7%|█████▏                                                                        | 19920/300000 [05:27<1:10:35, 66.13it/s]
epoch 19800  training loss: 14.017221450805664
epoch 19800  clean testing loss: 40.86728286743164
epoch 19900  training loss: 13.602943420410156

  7%|█████▏                                                                        | 20053/300000 [05:29<1:10:10, 66.48it/s]
epoch 20000  training loss: 14.084893226623535
epoch 20000  clean testing loss: 42.29677963256836

  7%|█████▏                                                                        | 20186/300000 [05:31<1:10:10, 66.45it/s]
epoch 20100  training loss: 13.93557071685791

  7%|█████▎                                                                        | 20319/300000 [05:33<1:10:35, 66.03it/s]
epoch 20200  training loss: 12.415318489074707
epoch 20200  clean testing loss: 42.952579498291016
epoch 20300  training loss: 12.809355735778809

  7%|█████▎                                                                        | 20452/300000 [05:35<1:09:57, 66.59it/s]
epoch 20400  training loss: 13.459659576416016

  7%|█████▎                                                                        | 20585/300000 [05:37<1:10:25, 66.12it/s]
epoch 20500  training loss: 13.124481201171875
epoch 20500  clean testing loss: 42.562191009521484
epoch 20600  training loss: 13.574250221252441
epoch 20600  clean testing loss: 42.810367584228516
epoch 20700  training loss: 13.420929908752441
epoch 20700  clean testing loss: 42.10987091064453
epoch 20800  training loss: 12.640975952148438
epoch 20800  clean testing loss: 42.92656326293945
epoch 20900  training loss: 12.887450218200684
epoch 20900  clean testing loss: 46.361820220947266
epoch 21000  training loss: 13.630350112915039
epoch 21000  clean testing loss: 44.6309700012207

  7%|█████▍                                                                        | 21047/300000 [05:44<1:11:09, 65.33it/s]
epoch 21100  training loss: 13.918622016906738


  7%|█████▌                                                                        | 21313/300000 [05:48<1:09:42, 66.63it/s]
epoch 21200  training loss: 13.919421195983887
epoch 21200  clean testing loss: 47.969444274902344
epoch 21300  training loss: 12.591371536254883

  7%|█████▌                                                                        | 21446/300000 [05:50<1:10:29, 65.86it/s]
epoch 21400  training loss: 14.003039360046387

  7%|█████▋                                                                        | 21950/300000 [05:57<1:09:22, 66.80it/s]
epoch 21500  training loss: 14.065401077270508
epoch 21500  clean testing loss: 48.621978759765625
epoch 21600  training loss: 13.842833518981934
epoch 21600  clean testing loss: 47.673309326171875
epoch 21700  training loss: 13.382189750671387
epoch 21700  clean testing loss: 46.883445739746094
epoch 21800  training loss: 13.304713249206543
epoch 21800  clean testing loss: 45.55111312866211
epoch 21900  training loss: 13.141517639160156

  7%|█████▋                                                                        | 22076/300000 [05:59<1:10:25, 65.77it/s]
epoch 22000  training loss: 13.469582557678223
epoch 22000  clean testing loss: 50.15998077392578

  7%|█████▊                                                                        | 22209/300000 [06:01<1:09:55, 66.21it/s]
epoch 22100  training loss: 13.469178199768066
epoch 22100  clean testing loss: 49.42049026489258
epoch 22200  training loss: 14.023127555847168

  7%|█████▊                                                                        | 22342/300000 [06:03<1:11:18, 64.90it/s]
epoch 22300  training loss: 12.940142631530762

  7%|█████▊                                                                        | 22475/300000 [06:05<1:09:59, 66.09it/s]
epoch 22400  training loss: 14.074853897094727

  8%|█████▉                                                                        | 22608/300000 [06:07<1:10:19, 65.74it/s]
epoch 22500  training loss: 13.898758888244629
epoch 22500  clean testing loss: 46.65864944458008
epoch 22600  training loss: 12.986010551452637

  8%|█████▉                                                                        | 22741/300000 [06:09<1:10:56, 65.13it/s]
epoch 22700  training loss: 12.17919635772705

  8%|█████▉                                                                        | 22874/300000 [06:11<1:09:37, 66.35it/s]
epoch 22800  training loss: 12.639491081237793

  8%|█████▉                                                                        | 23007/300000 [06:13<1:11:47, 64.30it/s]
epoch 22900  training loss: 12.860973358154297
epoch 22900  clean testing loss: 48.345863342285156
epoch 23000  training loss: 13.605313301086426
epoch 23000  clean testing loss: 49.41025161743164

  8%|██████                                                                        | 23140/300000 [06:15<1:09:20, 66.55it/s]
epoch 23100  training loss: 14.433979034423828

  8%|██████                                                                        | 23273/300000 [06:17<1:09:39, 66.21it/s]
epoch 23200  training loss: 14.69961166381836

  8%|██████                                                                        | 23399/300000 [06:19<1:09:38, 66.19it/s]
epoch 23300  training loss: 13.3104248046875

  8%|██████                                                                        | 23532/300000 [06:21<1:12:39, 63.42it/s]
epoch 23400  training loss: 13.102062225341797
epoch 23400  clean testing loss: 51.121307373046875
epoch 23500  training loss: 12.957212448120117

  8%|██████▏                                                                       | 23665/300000 [06:23<1:09:11, 66.56it/s]
epoch 23600  training loss: 13.63476848602295

  8%|██████▏                                                                       | 23798/300000 [06:26<1:11:48, 64.10it/s]
epoch 23700  training loss: 13.38867473602295

  8%|██████▏                                                                       | 23924/300000 [06:28<1:11:18, 64.53it/s]
epoch 23800  training loss: 13.296576499938965
epoch 23800  clean testing loss: 48.82399368286133
epoch 23900  training loss: 13.9766845703125

  8%|██████▎                                                                       | 24050/300000 [06:29<1:10:18, 65.41it/s]
epoch 24000  training loss: 13.947592735290527
epoch 24000  clean testing loss: 47.590431213378906

  8%|██████▎                                                                       | 24183/300000 [06:31<1:13:28, 62.56it/s]
epoch 24100  training loss: 13.3873929977417

  8%|██████▎                                                                       | 24316/300000 [06:34<1:09:54, 65.72it/s]
epoch 24200  training loss: 12.926158905029297
epoch 24200  clean testing loss: 50.88943099975586
epoch 24300  training loss: 13.971945762634277

  8%|██████▎                                                                       | 24449/300000 [06:36<1:09:40, 65.92it/s]
epoch 24400  training loss: 14.045221328735352

  8%|██████▍                                                                       | 24582/300000 [06:38<1:08:39, 66.86it/s]
epoch 24500  training loss: 13.952332496643066

  8%|██████▍                                                                       | 24680/300000 [06:39<1:09:09, 66.35it/s]
epoch 24600  training loss: 13.9918212890625
epoch 24600  clean testing loss: 50.899261474609375
epoch 24700  training loss: 13.721471786499023

  8%|██████▍                                                                       | 24813/300000 [06:41<1:09:03, 66.41it/s]
epoch 24800  training loss: 14.30411434173584

  8%|██████▍                                                                       | 24946/300000 [06:43<1:09:47, 65.69it/s]
epoch 24900  training loss: 13.442643165588379

  8%|██████▌                                                                       | 25079/300000 [06:45<1:08:35, 66.81it/s]
epoch 25000  training loss: 11.668362617492676
epoch 25000  clean testing loss: 51.194339752197266
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise5.00e+01_invop1 ...
epoch 25100  training loss: 12.283517837524414

  8%|██████▌                                                                       | 25212/300000 [06:47<1:08:23, 66.97it/s]
epoch 25200  training loss: 12.994632720947266

  8%|██████▌                                                                       | 25345/300000 [06:49<1:09:18, 66.05it/s]
epoch 25300  training loss: 12.86535358428955

  8%|██████▌                                                                       | 25478/300000 [06:51<1:08:53, 66.41it/s]
epoch 25400  training loss: 12.308404922485352
epoch 25400  clean testing loss: 49.47718048095703
epoch 25500  training loss: 12.713271141052246

  9%|██████▋                                                                       | 25611/300000 [06:53<1:08:31, 66.74it/s]
epoch 25600  training loss: 12.35118579864502

  9%|██████▋                                                                       | 25744/300000 [06:55<1:11:02, 64.34it/s]
epoch 25700  training loss: 11.863763809204102

  9%|██████▋                                                                       | 25870/300000 [06:57<1:08:38, 66.57it/s]
epoch 25800  training loss: 11.493617057800293

  9%|██████▊                                                                       | 26003/300000 [06:59<1:10:17, 64.97it/s]
epoch 25900  training loss: 13.040353775024414
epoch 25900  clean testing loss: 49.634456634521484
epoch 26000  training loss: 13.007596015930176
epoch 26000  clean testing loss: 50.74042510986328

  9%|██████▊                                                                       | 26136/300000 [07:01<1:10:07, 65.09it/s]
epoch 26100  training loss: 11.64940071105957

  9%|██████▊                                                                       | 26269/300000 [07:03<1:09:02, 66.08it/s]
epoch 26200  training loss: 11.938738822937012

  9%|██████▊                                                                       | 26402/300000 [07:05<1:08:44, 66.33it/s]
epoch 26300  training loss: 10.613825798034668
epoch 26300  clean testing loss: 48.546390533447266
epoch 26400  training loss: 11.997493743896484

  9%|██████▉                                                                       | 26535/300000 [07:07<1:09:05, 65.97it/s]
epoch 26500  training loss: 11.21545124053955

  9%|██████▉                                                                       | 26668/300000 [07:09<1:08:01, 66.98it/s]
epoch 26600  training loss: 12.251965522766113
epoch 26600  clean testing loss: 50.1690559387207
epoch 26700  training loss: 11.143094062805176
epoch 26700  clean testing loss: 50.54012680053711
epoch 26800  training loss: 11.220154762268066
epoch 26800  clean testing loss: 49.62443923950195
epoch 26900  training loss: 12.186042785644531
epoch 26900  clean testing loss: 50.89311218261719
epoch 27000  training loss: 11.949766159057617
epoch 27000  clean testing loss: 51.5956916809082
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise5.00e+01_invop1 ...
epoch 27100  training loss: 11.39350414276123


  9%|███████                                                                       | 27235/300000 [07:22<1:08:53, 65.99it/s]
epoch 27200  training loss: 10.269064903259277

  9%|███████                                                                       | 27368/300000 [07:24<1:08:31, 66.31it/s]
epoch 27300  training loss: 11.578506469726562

  9%|███████▏                                                                      | 27501/300000 [07:26<1:08:33, 66.24it/s]
epoch 27400  training loss: 10.516716957092285
epoch 27400  clean testing loss: 53.3024787902832
epoch 27500  training loss: 10.641297340393066

  9%|███████▏                                                                      | 27634/300000 [07:28<1:09:09, 65.64it/s]
epoch 27600  training loss: 10.486104011535645

  9%|███████▏                                                                      | 27767/300000 [07:30<1:08:36, 66.13it/s]
epoch 27700  training loss: 11.824336051940918

  9%|███████▎                                                                      | 27900/300000 [07:32<1:08:27, 66.25it/s]
epoch 27800  training loss: 10.468937873840332
epoch 27800  clean testing loss: 53.952579498291016
epoch 27900  training loss: 11.191896438598633

  9%|███████▎                                                                      | 28033/300000 [07:34<1:08:36, 66.06it/s]
epoch 28000  training loss: 10.951086044311523
epoch 28000  clean testing loss: 53.5635986328125

  9%|███████▎                                                                      | 28166/300000 [07:36<1:08:03, 66.57it/s]
epoch 28100  training loss: 10.379006385803223

  9%|███████▎                                                                      | 28299/300000 [07:38<1:08:06, 66.50it/s]
epoch 28200  training loss: 9.821709632873535
epoch 28200  clean testing loss: 54.34575271606445
epoch 28300  training loss: 10.09777545928955

  9%|███████▍                                                                      | 28432/300000 [07:40<1:08:30, 66.06it/s]
epoch 28400  training loss: 11.463494300842285

 10%|███████▍                                                                      | 28565/300000 [07:42<1:08:03, 66.46it/s]
epoch 28500  training loss: 12.668871879577637

 10%|███████▍                                                                      | 28698/300000 [07:44<1:07:49, 66.67it/s]
epoch 28600  training loss: 13.65756607055664
epoch 28600  clean testing loss: 52.61344909667969
epoch 28700  training loss: 13.832045555114746

 10%|███████▍                                                                      | 28831/300000 [07:46<1:08:09, 66.30it/s]
epoch 28800  training loss: 14.904253959655762

 10%|███████▌                                                                      | 28964/300000 [07:48<1:07:14, 67.18it/s]
epoch 28900  training loss: 14.988845825195312
epoch 28900  clean testing loss: 53.641502380371094
epoch 29000  training loss: 15.417688369750977
epoch 29000  clean testing loss: 52.12632751464844
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise5.00e+01_invop1 ...
epoch 29100  training loss: 13.944926261901855

 10%|███████▌                                                                      | 29111/300000 [07:59<1:16:16, 59.19it/s]
epoch 29200  training loss: 14.141276359558105

 10%|███████▌                                                                      | 29244/300000 [08:01<1:08:02, 66.31it/s]
epoch 29300  training loss: 13.744824409484863

 10%|███████▋                                                                      | 29377/300000 [08:03<1:07:36, 66.71it/s]
epoch 29400  training loss: 13.606461524963379
epoch 29400  clean testing loss: 52.0329475402832
epoch 29500  training loss: 14.164020538330078

 10%|███████▋                                                                      | 29510/300000 [08:05<1:08:36, 65.71it/s]
epoch 29600  training loss: 12.613086700439453

 10%|███████▋                                                                      | 29643/300000 [08:07<1:08:27, 65.82it/s]
epoch 29700  training loss: 13.109334945678711

 10%|███████▋                                                                      | 29776/300000 [08:09<1:07:37, 66.60it/s]
epoch 29800  training loss: 13.73268985748291
epoch 29800  clean testing loss: 53.58716583251953
epoch 29900  training loss: 11.968460083007812

 10%|███████▊                                                                      | 29902/300000 [08:11<1:07:50, 66.35it/s]
epoch 30000  training loss: 12.366515159606934
epoch 30000  clean testing loss: 52.857757568359375
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise5.00e+01_invop1 ...
epoch 30100  training loss: 10.694087982177734

 10%|███████▊                                                                      | 30126/300000 [08:17<1:09:02, 65.15it/s]
epoch 30200  training loss: 11.198371887207031

 10%|███████▊                                                                      | 30259/300000 [08:19<1:07:27, 66.65it/s]
epoch 30300  training loss: 11.687079429626465

 10%|███████▉                                                                      | 30392/300000 [08:21<1:08:13, 65.86it/s]
epoch 30400  training loss: 11.352356910705566
epoch 30400  clean testing loss: 52.22508239746094
epoch 30500  training loss: 10.828991889953613

 10%|███████▉                                                                      | 30525/300000 [08:23<1:08:45, 65.32it/s]
epoch 30600  training loss: 10.411015510559082

 10%|███████▉                                                                      | 30658/300000 [08:25<1:07:43, 66.29it/s]
epoch 30700  training loss: 10.777254104614258

 10%|████████                                                                      | 30791/300000 [08:27<1:07:38, 66.34it/s]
epoch 30800  training loss: 11.188861846923828
epoch 30800  clean testing loss: 50.18059539794922
epoch 30900  training loss: 11.437762260437012

 10%|████████                                                                      | 30924/300000 [08:29<1:07:52, 66.07it/s]
epoch 31000  training loss: 11.814175605773926
epoch 31000  clean testing loss: 48.07478332519531

 10%|████████                                                                      | 31057/300000 [08:31<1:07:47, 66.11it/s]
epoch 31100  training loss: 12.348980903625488

 10%|████████                                                                      | 31190/300000 [08:33<1:08:16, 65.62it/s]
epoch 31200  training loss: 12.29098129272461

 10%|████████▏                                                                     | 31323/300000 [08:35<1:07:23, 66.45it/s]
epoch 31300  training loss: 12.787738800048828
epoch 31300  clean testing loss: 50.32123565673828
epoch 31400  training loss: 15.187116622924805

 10%|████████▏                                                                     | 31456/300000 [08:37<1:07:54, 65.91it/s]
epoch 31500  training loss: 14.922752380371094

 11%|████████▏                                                                     | 31589/300000 [08:39<1:07:34, 66.21it/s]
epoch 31600  training loss: 14.28795051574707


 11%|████████▎                                                                     | 32114/300000 [08:54<1:11:39, 62.31it/s]
epoch 31700  training loss: 15.913081169128418
epoch 31700  clean testing loss: 50.8835334777832
epoch 31800  training loss: 16.181177139282227
epoch 31800  clean testing loss: 51.24087142944336
epoch 31900  training loss: 16.89272117614746
epoch 31900  clean testing loss: 48.70547103881836
epoch 32000  training loss: 16.172595977783203
epoch 32000  clean testing loss: 48.45473098754883
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise5.00e+01_invop1 ...
epoch 32100  training loss: 16.5956974029541

 11%|████████▍                                                                     | 32247/300000 [08:56<1:07:07, 66.49it/s]
epoch 32200  training loss: 16.78610610961914

 11%|████████▍                                                                     | 32380/300000 [08:58<1:07:04, 66.50it/s]
epoch 32300  training loss: 17.173870086669922

 11%|████████▍                                                                     | 32513/300000 [09:00<1:07:53, 65.66it/s]
epoch 32400  training loss: 15.862791061401367
epoch 32400  clean testing loss: 48.83881378173828
epoch 32500  training loss: 13.55654525756836

 11%|████████▍                                                                     | 32646/300000 [09:02<1:06:54, 66.59it/s]
epoch 32600  training loss: 14.151331901550293

 11%|████████▌                                                                     | 32779/300000 [09:04<1:07:12, 66.26it/s]
epoch 32700  training loss: 13.542619705200195

 11%|████████▌                                                                     | 32912/300000 [09:06<1:07:59, 65.47it/s]
epoch 32800  training loss: 14.93996524810791
epoch 32800  clean testing loss: 47.102962493896484
epoch 32900  training loss: 15.150297164916992

 11%|████████▌                                                                     | 33045/300000 [09:08<1:07:03, 66.35it/s]
epoch 33000  training loss: 15.630012512207031
epoch 33000  clean testing loss: 47.598533630371094

 11%|████████▋                                                                     | 33178/300000 [09:10<1:07:34, 65.81it/s]
epoch 33100  training loss: 15.569694519042969

 11%|████████▋                                                                     | 33311/300000 [09:13<1:06:27, 66.87it/s]
epoch 33200  training loss: 14.534814834594727
epoch 33200  clean testing loss: 47.57677459716797
epoch 33300  training loss: 13.235551834106445

 11%|████████▋                                                                     | 33444/300000 [09:15<1:07:22, 65.94it/s]
epoch 33400  training loss: 13.529072761535645

 11%|████████▋                                                                     | 33577/300000 [09:17<1:06:46, 66.50it/s]
epoch 33500  training loss: 14.667228698730469

 11%|████████▊                                                                     | 33710/300000 [09:19<1:07:31, 65.73it/s]
epoch 33600  training loss: 16.310546875
epoch 33600  clean testing loss: 44.146141052246094
epoch 33700  training loss: 15.521153450012207

 11%|████████▊                                                                     | 33843/300000 [09:21<1:06:52, 66.33it/s]
epoch 33800  training loss: 15.445404052734375

 11%|████████▊                                                                     | 33976/300000 [09:23<1:07:06, 66.07it/s]
epoch 33900  training loss: 15.38554859161377

 11%|████████▊                                                                     | 34130/300000 [09:27<1:07:25, 65.72it/s]
epoch 34000  training loss: 16.189342498779297
epoch 34000  clean testing loss: 47.30724334716797
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise5.00e+01_invop1 ...
epoch 34100  training loss: 15.283912658691406

 11%|████████▉                                                                     | 34263/300000 [09:29<1:06:29, 66.60it/s]
epoch 34200  training loss: 14.810086250305176

 11%|████████▉                                                                     | 34396/300000 [09:31<1:06:34, 66.49it/s]
epoch 34300  training loss: 15.271644592285156

 12%|████████▉                                                                     | 34529/300000 [09:33<1:06:49, 66.21it/s]
epoch 34400  training loss: 15.247493743896484
epoch 34400  clean testing loss: 44.341102600097656
epoch 34500  training loss: 14.017696380615234

 12%|█████████                                                                     | 34662/300000 [09:35<1:06:32, 66.46it/s]
epoch 34600  training loss: 13.998979568481445

 12%|█████████                                                                     | 34760/300000 [09:37<1:07:22, 65.62it/s]
epoch 34700  training loss: 14.21337604522705

 12%|█████████                                                                     | 34893/300000 [09:39<1:05:57, 66.99it/s]
epoch 34800  training loss: 14.489422798156738
epoch 34800  clean testing loss: 46.37722396850586
epoch 34900  training loss: 14.747873306274414

 12%|█████████                                                                     | 35026/300000 [09:41<1:06:38, 66.26it/s]
epoch 35000  training loss: 14.748262405395508
epoch 35000  clean testing loss: 46.161495208740234

 12%|█████████▏                                                                    | 35159/300000 [09:43<1:06:04, 66.81it/s]
epoch 35100  training loss: 13.344099044799805

 12%|█████████▏                                                                    | 35292/300000 [09:45<1:06:12, 66.63it/s]
epoch 35200  training loss: 13.78507137298584
epoch 35200  clean testing loss: 47.07448196411133
epoch 35300  training loss: 14.19805908203125

 12%|█████████▏                                                                    | 35425/300000 [09:47<1:06:29, 66.32it/s]
epoch 35400  training loss: 14.477458000183105

 12%|█████████▏                                                                    | 35558/300000 [09:49<1:06:31, 66.24it/s]
epoch 35500  training loss: 14.4027681350708

 12%|█████████▎                                                                    | 35691/300000 [09:51<1:06:45, 65.99it/s]
epoch 35600  training loss: 13.208208084106445
epoch 35600  clean testing loss: 49.07415008544922
epoch 35700  training loss: 13.006490707397461

 12%|█████████▍                                                                    | 36104/300000 [09:57<1:06:31, 66.11it/s]
epoch 35800  training loss: 14.05117416381836
epoch 35800  clean testing loss: 48.9235954284668
epoch 35900  training loss: 12.388041496276855
epoch 35900  clean testing loss: 47.060447692871094
epoch 36000  training loss: 13.381507873535156
epoch 36000  clean testing loss: 47.29649353027344
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise5.00e+01_invop1 ...
epoch 36100  training loss: 13.013373374938965

 12%|█████████▍                                                                    | 36237/300000 [09:59<1:05:47, 66.81it/s]
epoch 36200  training loss: 13.151199340820312

 12%|█████████▍                                                                    | 36370/300000 [10:01<1:06:03, 66.51it/s]
epoch 36300  training loss: 13.707969665527344

 12%|█████████▍                                                                    | 36503/300000 [10:03<1:06:00, 66.53it/s]
epoch 36400  training loss: 14.79371166229248
epoch 36400  clean testing loss: 47.31816864013672
epoch 36500  training loss: 13.224698066711426

 12%|█████████▋                                                                    | 37098/300000 [10:12<1:06:45, 65.63it/s]
epoch 36600  training loss: 12.691161155700684
epoch 36600  clean testing loss: 46.9702262878418
epoch 36700  training loss: 12.330453872680664
epoch 36700  clean testing loss: 48.80255889892578
epoch 36800  training loss: 12.993864059448242
epoch 36800  clean testing loss: 48.31290054321289
epoch 36900  training loss: 13.31343936920166
epoch 36900  clean testing loss: 49.95824432373047
epoch 37000  training loss: 12.37554931640625
epoch 37000  clean testing loss: 49.899940490722656
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise5.00e+01_invop1 ...
epoch 37100  training loss: 12.01394271850586

 12%|█████████▋                                                                    | 37238/300000 [10:14<1:05:39, 66.69it/s]
epoch 37200  training loss: 12.128344535827637

 12%|█████████▋                                                                    | 37371/300000 [10:16<1:05:53, 66.43it/s]
epoch 37300  training loss: 12.224735260009766

 12%|█████████▋                                                                    | 37497/300000 [10:18<1:06:03, 66.23it/s]
epoch 37400  training loss: 11.380508422851562
epoch 37400  clean testing loss: 48.396209716796875
epoch 37500  training loss: 12.650665283203125

 13%|█████████▊                                                                    | 37630/300000 [10:20<1:06:07, 66.13it/s]
epoch 37600  training loss: 13.045679092407227

 13%|█████████▊                                                                    | 37770/300000 [10:22<1:05:19, 66.91it/s]
epoch 37700  training loss: 13.193857192993164

 13%|█████████▊                                                                    | 37903/300000 [10:24<1:05:58, 66.21it/s]
epoch 37800  training loss: 11.77336597442627
epoch 37800  clean testing loss: 49.657814025878906
epoch 37900  training loss: 12.45378589630127
epoch 37900  clean testing loss: 50.52328109741211
epoch 38000  training loss: 11.738200187683105
epoch 38000  clean testing loss: 50.49197769165039
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise5.00e+01_invop1 ...
epoch 38100  training loss: 11.981721878051758

 13%|█████████▉                                                                    | 38106/300000 [10:31<1:08:48, 63.44it/s]
epoch 38200  training loss: 12.568150520324707

 13%|█████████▉                                                                    | 38239/300000 [10:33<1:06:46, 65.33it/s]
epoch 38300  training loss: 11.546725273132324

 13%|█████████▉                                                                    | 38372/300000 [10:35<1:05:42, 66.36it/s]
epoch 38400  training loss: 12.015460968017578

 13%|██████████                                                                    | 38505/300000 [10:37<1:06:25, 65.61it/s]
epoch 38500  training loss: 12.091033935546875
epoch 38500  clean testing loss: 48.92001724243164
epoch 38600  training loss: 11.285811424255371

 13%|██████████                                                                    | 38638/300000 [10:39<1:06:21, 65.65it/s]
epoch 38700  training loss: 12.0545654296875

 13%|██████████                                                                    | 38771/300000 [10:41<1:05:31, 66.44it/s]
epoch 38800  training loss: 13.25246524810791

 13%|██████████                                                                    | 38904/300000 [10:43<1:05:42, 66.22it/s]
epoch 38900  training loss: 12.233658790588379
epoch 38900  clean testing loss: 51.206871032714844
epoch 39000  training loss: 12.019750595092773
epoch 39000  clean testing loss: 49.738040924072266

 13%|██████████▏                                                                   | 39037/300000 [10:45<1:05:39, 66.24it/s]
epoch 39100  training loss: 12.194659233093262

 13%|██████████▏                                                                   | 39170/300000 [10:47<1:06:45, 65.11it/s]
epoch 39200  training loss: 13.042976379394531

 13%|██████████▏                                                                   | 39303/300000 [10:49<1:05:31, 66.31it/s]
epoch 39300  training loss: 12.855489730834961
epoch 39300  clean testing loss: 50.17030715942383
epoch 39400  training loss: 12.001283645629883

 13%|██████████▎                                                                   | 39436/300000 [10:51<1:06:01, 65.78it/s]
epoch 39500  training loss: 12.354063034057617

 13%|██████████▎                                                                   | 39569/300000 [10:53<1:04:56, 66.84it/s]
epoch 39600  training loss: 12.683881759643555

 13%|██████████▎                                                                   | 39702/300000 [10:55<1:05:42, 66.03it/s]
epoch 39700  training loss: 11.636717796325684
epoch 39700  clean testing loss: 47.197566986083984
epoch 39800  training loss: 11.98934268951416

 13%|██████████▎                                                                   | 39835/300000 [10:57<1:05:59, 65.71it/s]
epoch 39900  training loss: 12.04210376739502

 13%|██████████▍                                                                   | 39968/300000 [10:59<1:05:01, 66.64it/s]
epoch 40000  training loss: 10.894203186035156
epoch 40000  clean testing loss: 47.231285095214844

 13%|██████████▍                                                                   | 40094/300000 [11:01<1:05:18, 66.33it/s]
epoch 40100  training loss: 11.330324172973633
epoch 40100  clean testing loss: 47.729305267333984
epoch 40200  training loss: 11.990379333496094

 13%|██████████▍                                                                   | 40234/300000 [11:03<1:04:53, 66.72it/s]
epoch 40300  training loss: 12.265863418579102

 13%|██████████▍                                                                   | 40360/300000 [11:05<1:05:55, 65.64it/s]
epoch 40400  training loss: 11.675305366516113

 13%|██████████▌                                                                   | 40493/300000 [11:07<1:05:23, 66.14it/s]
epoch 40500  training loss: 11.791914939880371
epoch 40500  clean testing loss: 49.297271728515625
epoch 40600  training loss: 11.883657455444336

 14%|██████████▌                                                                   | 40626/300000 [11:09<1:05:38, 65.86it/s]
epoch 40700  training loss: 11.945145606994629

 14%|██████████▌                                                                   | 40759/300000 [11:11<1:05:55, 65.53it/s]
epoch 40800  training loss: 11.482223510742188

 14%|██████████▋                                                                   | 40892/300000 [11:13<1:05:16, 66.16it/s]
epoch 40900  training loss: 11.94448471069336
epoch 40900  clean testing loss: 48.1212272644043
epoch 41000  training loss: 11.88982105255127
epoch 41000  clean testing loss: 48.939151763916016

 14%|██████████▋                                                                   | 41025/300000 [11:15<1:05:30, 65.88it/s]
epoch 41100  training loss: 11.687275886535645

 14%|██████████▋                                                                   | 41158/300000 [11:17<1:04:58, 66.39it/s]
epoch 41200  training loss: 11.881263732910156

 14%|██████████▋                                                                   | 41291/300000 [11:19<1:05:27, 65.87it/s]
epoch 41300  training loss: 12.057068824768066
epoch 41300  clean testing loss: 47.339385986328125
epoch 41400  training loss: 12.42723274230957

 14%|██████████▊                                                                   | 41424/300000 [11:21<1:05:17, 66.00it/s]
epoch 41500  training loss: 11.826311111450195

 14%|██████████▊                                                                   | 41557/300000 [11:23<1:05:30, 65.75it/s]
epoch 41600  training loss: 11.857239723205566

 14%|██████████▊                                                                   | 41690/300000 [11:25<1:05:23, 65.84it/s]
epoch 41700  training loss: 11.882390975952148
epoch 41700  clean testing loss: 48.02210235595703
epoch 41800  training loss: 12.733513832092285
epoch 41800  clean testing loss: 48.143714904785156
epoch 41900  training loss: 13.123888969421387
epoch 41900  clean testing loss: 48.565147399902344
epoch 42000  training loss: 12.590072631835938
epoch 42000  clean testing loss: 49.30678176879883

 14%|██████████▉                                                                   | 42054/300000 [11:31<1:05:09, 65.97it/s]
epoch 42100  training loss: 12.587672233581543
epoch 42100  clean testing loss: 48.608089447021484
epoch 42200  training loss: 12.338629722595215

 14%|██████████▉                                                                   | 42187/300000 [11:33<1:04:24, 66.72it/s]
epoch 42300  training loss: 12.353776931762695

 14%|███████████                                                                   | 42320/300000 [11:35<1:04:17, 66.79it/s]
epoch 42400  training loss: 11.440145492553711

 14%|███████████                                                                   | 42453/300000 [11:37<1:04:59, 66.05it/s]
epoch 42500  training loss: 11.76567268371582

 14%|███████████                                                                   | 42586/300000 [11:39<1:04:41, 66.31it/s]
epoch 42600  training loss: 11.688711166381836
epoch 42600  clean testing loss: 47.269474029541016
epoch 42700  training loss: 11.120039939880371

 14%|███████████                                                                   | 42719/300000 [11:41<1:04:02, 66.96it/s]
epoch 42800  training loss: 11.736614227294922

 14%|███████████▏                                                                  | 42852/300000 [11:43<1:04:00, 66.96it/s]
epoch 42900  training loss: 11.349088668823242

 14%|███████████▏                                                                  | 42985/300000 [11:45<1:04:39, 66.25it/s]
epoch 43000  training loss: 11.648743629455566
epoch 43000  clean testing loss: 47.97804641723633
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise5.00e+01_invop1 ...
epoch 43100  training loss: 11.620548248291016

 14%|███████████▏                                                                  | 43118/300000 [11:47<1:04:32, 66.34it/s]
epoch 43200  training loss: 10.320268630981445

 14%|███████████▏                                                                  | 43251/300000 [11:49<1:05:03, 65.77it/s]
epoch 43300  training loss: 10.60615348815918

 14%|███████████▎                                                                  | 43377/300000 [11:51<1:05:27, 65.33it/s]
epoch 43400  training loss: 11.590252876281738
epoch 43400  clean testing loss: 46.9806022644043
epoch 43500  training loss: 10.033660888671875

 15%|███████████▎                                                                  | 43510/300000 [11:53<1:03:57, 66.83it/s]
epoch 43600  training loss: 11.302417755126953

 15%|███████████▎                                                                  | 43650/300000 [11:55<1:03:54, 66.84it/s]
epoch 43700  training loss: 10.621010780334473


 15%|███████████▍                                                                  | 44077/300000 [12:01<1:04:55, 65.70it/s]
epoch 43800  training loss: 11.108424186706543
epoch 43800  clean testing loss: 46.906532287597656
epoch 43900  training loss: 10.852740287780762
epoch 43900  clean testing loss: 47.755226135253906
epoch 44000  training loss: 11.358962059020996
epoch 44000  clean testing loss: 48.293540954589844

 15%|███████████▍                                                                  | 44210/300000 [12:03<1:03:57, 66.65it/s]
epoch 44100  training loss: 10.90849494934082
epoch 44100  clean testing loss: 48.3845329284668
epoch 44200  training loss: 11.387350082397461

 15%|███████████▌                                                                  | 44343/300000 [12:05<1:04:09, 66.41it/s]
epoch 44300  training loss: 10.279305458068848

 15%|███████████▌                                                                  | 44476/300000 [12:07<1:03:28, 67.10it/s]
epoch 44400  training loss: 11.43268871307373

 15%|███████████▌                                                                  | 44609/300000 [12:09<1:04:45, 65.73it/s]
epoch 44500  training loss: 12.662277221679688
epoch 44500  clean testing loss: 47.798126220703125
epoch 44600  training loss: 9.724640846252441
epoch 44600  clean testing loss: 47.886104583740234
epoch 44700  training loss: 10.975526809692383

 15%|███████████▋                                                                  | 44742/300000 [12:11<1:04:56, 65.51it/s]
epoch 44800  training loss: 10.575562477111816

 15%|███████████▋                                                                  | 44875/300000 [12:13<1:03:38, 66.82it/s]
epoch 44900  training loss: 11.301885604858398
epoch 44900  clean testing loss: 47.832027435302734
epoch 45000  training loss: 11.71824836730957
epoch 45000  clean testing loss: 48.54316711425781

 15%|███████████▋                                                                  | 45008/300000 [12:15<1:04:33, 65.83it/s]
epoch 45100  training loss: 10.48107624053955

 15%|███████████▋                                                                  | 45141/300000 [12:17<1:03:33, 66.83it/s]
epoch 45200  training loss: 10.61379337310791

 15%|███████████▊                                                                  | 45274/300000 [12:19<1:03:43, 66.62it/s]
epoch 45300  training loss: 11.204951286315918
epoch 45300  clean testing loss: 49.655113220214844
epoch 45400  training loss: 11.135741233825684

 15%|███████████▊                                                                  | 45407/300000 [12:21<1:04:22, 65.92it/s]
epoch 45500  training loss: 11.57070541381836

 15%|███████████▊                                                                  | 45540/300000 [12:23<1:05:36, 64.64it/s]
epoch 45600  training loss: 11.156320571899414

 15%|███████████▊                                                                  | 45673/300000 [12:25<1:04:15, 65.96it/s]
epoch 45700  training loss: 11.326115608215332
epoch 45700  clean testing loss: 50.67221450805664
epoch 45800  training loss: 10.739592552185059

 15%|███████████▉                                                                  | 45806/300000 [12:27<1:04:06, 66.08it/s]
epoch 45900  training loss: 11.464850425720215

 15%|███████████▉                                                                  | 45939/300000 [12:29<1:04:38, 65.50it/s]
epoch 46000  training loss: 10.992401123046875
epoch 46000  clean testing loss: 51.049652099609375

 15%|███████████▉                                                                  | 46072/300000 [12:31<1:03:47, 66.34it/s]
epoch 46100  training loss: 11.188632011413574
epoch 46100  clean testing loss: 50.51704788208008
epoch 46200  training loss: 12.126656532287598

 15%|████████████                                                                  | 46205/300000 [12:33<1:04:20, 65.75it/s]
epoch 46300  training loss: 11.711041450500488

 15%|████████████                                                                  | 46338/300000 [12:35<1:04:13, 65.82it/s]
epoch 46400  training loss: 11.409587860107422
epoch 46400  clean testing loss: 50.05211639404297
epoch 46500  training loss: 10.302745819091797
epoch 46500  clean testing loss: 49.74275588989258
epoch 46600  training loss: 11.44371509552002
epoch 46600  clean testing loss: 50.64596176147461
epoch 46700  training loss: 10.628584861755371
epoch 46700  clean testing loss: 50.35148239135742
epoch 46800  training loss: 11.305950164794922

 16%|████████████▏                                                                 | 46800/300000 [12:42<1:03:17, 66.67it/s]
epoch 46900  training loss: 10.923321723937988

 16%|████████████▏                                                                 | 46933/300000 [12:44<1:03:15, 66.68it/s]
epoch 47000  training loss: 11.009553909301758
epoch 47000  clean testing loss: 49.6747932434082

 16%|████████████▏                                                                 | 47066/300000 [12:46<1:03:05, 66.82it/s]
epoch 47100  training loss: 10.96086597442627
epoch 47100  clean testing loss: 48.66102600097656
epoch 47200  training loss: 10.757842063903809

 16%|████████████▎                                                                 | 47199/300000 [12:48<1:03:28, 66.37it/s]
epoch 47300  training loss: 11.752237319946289

 16%|████████████▎                                                                 | 47332/300000 [12:50<1:03:43, 66.09it/s]
epoch 47400  training loss: 11.268279075622559


 16%|████████████▌                                                                 | 48130/300000 [13:04<1:03:39, 65.94it/s]
epoch 47500  training loss: 10.695852279663086
epoch 47500  clean testing loss: 50.12294006347656
epoch 47600  training loss: 10.795747756958008
epoch 47600  clean testing loss: 50.09198760986328
epoch 47700  training loss: 11.360198020935059
epoch 47700  clean testing loss: 49.16514205932617
epoch 47800  training loss: 10.725895881652832
epoch 47800  clean testing loss: 49.142478942871094
epoch 47900  training loss: 10.993879318237305
epoch 47900  clean testing loss: 48.834259033203125
epoch 48000  training loss: 11.063103675842285
epoch 48000  clean testing loss: 49.962120056152344
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise5.00e+01_invop1 ...
epoch 48100  training loss: 11.012655258178711

 16%|████████████▌                                                                 | 48263/300000 [13:06<1:03:18, 66.27it/s]
epoch 48200  training loss: 10.507389068603516

 16%|████████████▌                                                                 | 48396/300000 [13:08<1:03:11, 66.35it/s]
epoch 48300  training loss: 10.951533317565918

 16%|████████████▌                                                                 | 48522/300000 [13:10<1:03:30, 66.00it/s]
epoch 48400  training loss: 10.86572551727295
epoch 48400  clean testing loss: 49.13642501831055
epoch 48500  training loss: 10.665102005004883

 16%|████████████▋                                                                 | 48655/300000 [13:12<1:02:30, 67.01it/s]
epoch 48600  training loss: 10.687253952026367

 16%|████████████▋                                                                 | 48788/300000 [13:14<1:03:26, 66.00it/s]
epoch 48700  training loss: 11.25186538696289

 16%|████████████▊                                                                 | 49089/300000 [13:19<1:02:59, 66.39it/s]
epoch 48800  training loss: 11.815388679504395
epoch 48800  clean testing loss: 48.9736213684082
epoch 48900  training loss: 11.086816787719727
epoch 48900  clean testing loss: 48.72541809082031
epoch 49000  training loss: 10.751445770263672
epoch 49000  clean testing loss: 48.215789794921875

 16%|████████████▊                                                                 | 49222/300000 [13:21<1:03:10, 66.17it/s]
epoch 49100  training loss: 10.343541145324707
epoch 49100  clean testing loss: 49.08891677856445
epoch 49200  training loss: 11.32927131652832

 16%|████████████▊                                                                 | 49355/300000 [13:23<1:02:52, 66.44it/s]
epoch 49300  training loss: 11.03048324584961

 16%|████████████▊                                                                 | 49488/300000 [13:25<1:03:00, 66.27it/s]
epoch 49400  training loss: 10.581747055053711

 17%|████████████▉                                                                 | 49621/300000 [13:27<1:03:30, 65.70it/s]
epoch 49500  training loss: 10.908273696899414
epoch 49500  clean testing loss: 47.710838317871094
epoch 49600  training loss: 10.74393081665039

 17%|████████████▉                                                                 | 49754/300000 [13:29<1:02:23, 66.84it/s]
epoch 49700  training loss: 10.362110137939453

 17%|████████████▉                                                                 | 49887/300000 [13:31<1:02:57, 66.22it/s]
epoch 49800  training loss: 9.417778968811035

 17%|█████████████                                                                 | 50020/300000 [13:33<1:04:01, 65.07it/s]
epoch 49900  training loss: 11.082718849182129
epoch 49900  clean testing loss: 47.59452438354492
epoch 50000  training loss: 10.628260612487793
epoch 50000  clean testing loss: 47.45848846435547

 17%|█████████████                                                                 | 50153/300000 [13:35<1:02:35, 66.53it/s]
epoch 50100  training loss: 10.628281593322754

 17%|█████████████                                                                 | 50286/300000 [13:37<1:02:05, 67.03it/s]
epoch 50200  training loss: 10.547603607177734

 17%|█████████████▏                                                                | 50650/300000 [13:42<1:02:44, 66.23it/s]
epoch 50300  training loss: 10.174116134643555
epoch 50300  clean testing loss: 47.68830871582031
epoch 50400  training loss: 10.342480659484863
epoch 50400  clean testing loss: 48.12948226928711
epoch 50500  training loss: 10.81806468963623
epoch 50500  clean testing loss: 48.5467643737793
epoch 50600  training loss: 11.103826522827148

 17%|█████████████▏                                                                | 50783/300000 [13:45<1:04:24, 64.49it/s]
epoch 50700  training loss: 11.192816734313965

 17%|█████████████▏                                                                | 50916/300000 [13:47<1:03:24, 65.47it/s]
epoch 50800  training loss: 11.200531959533691
epoch 50800  clean testing loss: 48.24756622314453
epoch 50900  training loss: 10.590840339660645

 17%|█████████████▎                                                                | 51042/300000 [13:48<1:03:18, 65.54it/s]
epoch 51000  training loss: 10.28728199005127
epoch 51000  clean testing loss: 48.914241790771484
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise5.00e+01_invop1 ...
epoch 51100  training loss: 10.027462005615234

 17%|█████████████▎                                                                | 51167/300000 [13:51<1:07:20, 61.59it/s]
epoch 51200  training loss: 10.3694429397583

 17%|█████████████▎                                                                | 51300/300000 [13:53<1:02:43, 66.08it/s]
epoch 51300  training loss: 10.729854583740234
epoch 51300  clean testing loss: 49.533958435058594
epoch 51400  training loss: 10.901644706726074

 17%|█████████████▎                                                                | 51433/300000 [13:55<1:02:51, 65.91it/s]
epoch 51500  training loss: 11.456538200378418

 17%|█████████████▍                                                                | 51566/300000 [13:57<1:02:19, 66.44it/s]
epoch 51600  training loss: 11.042323112487793

 17%|█████████████▍                                                                | 51699/300000 [13:59<1:03:48, 64.85it/s]
epoch 51700  training loss: 11.65166187286377
epoch 51700  clean testing loss: 49.06587600708008
epoch 51800  training loss: 11.698188781738281

 17%|█████████████▍                                                                | 51832/300000 [14:01<1:02:29, 66.19it/s]
epoch 51900  training loss: 10.662419319152832

 17%|█████████████▌                                                                | 51965/300000 [14:03<1:02:36, 66.03it/s]
epoch 52000  training loss: 10.894121170043945
epoch 52000  clean testing loss: 49.66926574707031


 17%|█████████████▌                                                                | 52120/300000 [14:15<1:06:26, 62.19it/s]
epoch 52100  training loss: 10.851426124572754

 17%|█████████████▌                                                                | 52253/300000 [14:17<1:02:22, 66.19it/s]
epoch 52200  training loss: 10.99297046661377

 17%|█████████████▌                                                                | 52386/300000 [14:19<1:01:47, 66.79it/s]
epoch 52300  training loss: 10.149930000305176

 18%|█████████████▋                                                                | 52519/300000 [14:21<1:02:10, 66.34it/s]
epoch 52400  training loss: 10.365209579467773
epoch 52400  clean testing loss: 48.775978088378906
epoch 52500  training loss: 10.605365753173828
epoch 52500  clean testing loss: 48.974708557128906
epoch 52600  training loss: 10.43656063079834

 18%|█████████████▋                                                                | 52652/300000 [14:23<1:02:07, 66.35it/s]
epoch 52700  training loss: 10.747429847717285

 18%|█████████████▋                                                                | 52785/300000 [14:25<1:01:42, 66.76it/s]
epoch 52800  training loss: 11.749408721923828
epoch 52800  clean testing loss: 49.96997833251953
epoch 52900  training loss: 10.930994033813477

 18%|█████████████▊                                                                | 53100/300000 [14:39<1:13:16, 56.15it/s]
epoch 53000  training loss: 11.452075958251953
epoch 53000  clean testing loss: 50.755741119384766
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise5.00e+01_invop1 ...
epoch 53100  training loss: 10.336678504943848

 18%|█████████████▊                                                                | 53233/300000 [14:41<1:02:54, 65.38it/s]
epoch 53200  training loss: 10.971988677978516

 18%|█████████████▉                                                                | 53366/300000 [14:43<1:01:41, 66.63it/s]
epoch 53300  training loss: 10.515013694763184

 18%|█████████████▉                                                                | 53499/300000 [14:45<1:01:43, 66.57it/s]
epoch 53400  training loss: 10.677923202514648
epoch 53400  clean testing loss: 49.4181022644043
epoch 53500  training loss: 10.469658851623535

 18%|█████████████▉                                                                | 53632/300000 [14:47<1:01:44, 66.50it/s]
epoch 53600  training loss: 11.430495262145996

 18%|█████████████▉                                                                | 53758/300000 [14:49<1:02:21, 65.82it/s]
epoch 53700  training loss: 10.849359512329102

 18%|██████████████                                                                | 53891/300000 [14:51<1:03:41, 64.39it/s]
epoch 53800  training loss: 11.142097473144531
epoch 53800  clean testing loss: 49.254112243652344
epoch 53900  training loss: 9.451176643371582

 18%|██████████████                                                                | 54024/300000 [14:53<1:01:44, 66.41it/s]
epoch 54000  training loss: 10.406560897827148
epoch 54000  clean testing loss: 49.13203048706055

 18%|██████████████                                                                | 54157/300000 [14:55<1:01:33, 66.56it/s]
epoch 54100  training loss: 10.87891960144043

 18%|██████████████                                                                | 54290/300000 [14:57<1:01:24, 66.69it/s]
epoch 54200  training loss: 10.348336219787598
epoch 54200  clean testing loss: 49.621559143066406
epoch 54300  training loss: 10.062789916992188

 18%|██████████████▏                                                               | 54423/300000 [14:59<1:02:04, 65.93it/s]
epoch 54400  training loss: 10.033452033996582

 18%|██████████████▏                                                               | 54556/300000 [15:01<1:01:26, 66.59it/s]
epoch 54500  training loss: 10.892313003540039

 18%|██████████████▏                                                               | 54689/300000 [15:03<1:01:11, 66.82it/s]
epoch 54600  training loss: 10.878207206726074
epoch 54600  clean testing loss: 48.92864227294922
epoch 54700  training loss: 11.194080352783203

 18%|██████████████▎                                                               | 54822/300000 [15:05<1:01:51, 66.06it/s]
epoch 54800  training loss: 11.402227401733398

 18%|██████████████▎                                                               | 54955/300000 [15:07<1:01:24, 66.50it/s]
epoch 54900  training loss: 10.932892799377441

 18%|██████████████▎                                                               | 55088/300000 [15:09<1:01:23, 66.49it/s]
epoch 55000  training loss: 10.299139976501465
epoch 55000  clean testing loss: 49.792930603027344
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise5.00e+01_invop1 ...
epoch 55100  training loss: 10.266461372375488

 18%|██████████████▎                                                               | 55221/300000 [15:11<1:01:24, 66.44it/s]
epoch 55200  training loss: 10.601576805114746

 18%|██████████████▍                                                               | 55354/300000 [15:13<1:00:53, 66.96it/s]
epoch 55300  training loss: 10.291814804077148

 18%|██████████████▍                                                               | 55487/300000 [15:15<1:01:32, 66.23it/s]
epoch 55400  training loss: 11.047375679016113
epoch 55400  clean testing loss: 50.89149475097656
epoch 55500  training loss: 11.000223159790039

 19%|██████████████▍                                                               | 55620/300000 [15:17<1:01:15, 66.50it/s]
epoch 55600  training loss: 11.062402725219727

 19%|██████████████▍                                                               | 55753/300000 [15:19<1:00:52, 66.87it/s]
epoch 55700  training loss: 10.790651321411133

 19%|██████████████▌                                                               | 55886/300000 [15:21<1:02:35, 65.00it/s]
epoch 55800  training loss: 10.5367431640625
epoch 55800  clean testing loss: 50.17550277709961
epoch 55900  training loss: 10.830869674682617
epoch 55900  clean testing loss: 51.06576919555664
epoch 56000  training loss: 10.727150917053223
epoch 56000  clean testing loss: 50.33823776245117
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise5.00e+01_invop1 ...
epoch 56100  training loss: 10.873007774353027

 19%|██████████████▌                                                               | 56103/300000 [15:33<1:11:31, 56.83it/s]
epoch 56200  training loss: 10.144386291503906


 19%|██████████████▋                                                               | 56369/300000 [15:37<1:01:24, 66.12it/s]
epoch 56300  training loss: 9.823074340820312

 19%|██████████████▋                                                               | 56502/300000 [15:39<1:01:17, 66.21it/s]
epoch 56400  training loss: 11.002148628234863
epoch 56400  clean testing loss: 51.96337127685547
epoch 56500  training loss: 11.127175331115723
epoch 56500  clean testing loss: 52.083744049072266
epoch 56600  training loss: 10.631579399108887

 19%|██████████████▋                                                               | 56670/300000 [15:41<1:01:48, 65.61it/s]
epoch 56700  training loss: 10.777812957763672
epoch 56700  clean testing loss: 51.18889617919922
epoch 56800  training loss: 11.646103858947754

 19%|██████████████▊                                                               | 56803/300000 [15:43<1:00:32, 66.95it/s]
epoch 56900  training loss: 11.469754219055176

 19%|██████████████▊                                                               | 56936/300000 [15:45<1:01:16, 66.10it/s]
epoch 57000  training loss: 10.37629222869873
epoch 57000  clean testing loss: 51.30447006225586


 19%|██████████████▉                                                               | 57258/300000 [15:52<1:01:01, 66.30it/s]
epoch 57100  training loss: 10.541803359985352
epoch 57100  clean testing loss: 51.670982360839844
epoch 57200  training loss: 11.333131790161133

 19%|██████████████▉                                                               | 57391/300000 [15:54<1:01:14, 66.02it/s]
epoch 57300  training loss: 10.223627090454102

 19%|██████████████▉                                                               | 57524/300000 [15:56<1:00:50, 66.41it/s]
epoch 57400  training loss: 10.284902572631836

 19%|██████████████▉                                                               | 57657/300000 [15:58<1:01:55, 65.22it/s]
epoch 57500  training loss: 10.268386840820312
epoch 57500  clean testing loss: 51.33613967895508
epoch 57600  training loss: 9.713752746582031

 19%|███████████████                                                               | 57790/300000 [16:00<1:00:14, 67.00it/s]
epoch 57700  training loss: 10.382523536682129

 19%|███████████████                                                               | 57923/300000 [16:02<1:01:36, 65.48it/s]
epoch 57800  training loss: 10.57751750946045

 19%|███████████████                                                               | 58056/300000 [16:04<1:00:31, 66.63it/s]
epoch 57900  training loss: 9.754197120666504
epoch 57900  clean testing loss: 51.5970458984375
epoch 58000  training loss: 10.150147438049316
epoch 58000  clean testing loss: 51.940128326416016

 19%|███████████████▏                                                              | 58189/300000 [16:06<1:02:01, 64.98it/s]
epoch 58100  training loss: 10.295968055725098

 19%|███████████████▏                                                              | 58322/300000 [16:08<1:00:49, 66.22it/s]
epoch 58200  training loss: 10.617051124572754

 19%|███████████████▏                                                              | 58455/300000 [16:10<1:00:45, 66.25it/s]
epoch 58300  training loss: 10.079606056213379
epoch 58300  clean testing loss: 50.649105072021484
epoch 58400  training loss: 11.148664474487305

 20%|███████████████▏                                                              | 58588/300000 [16:12<1:00:30, 66.49it/s]
epoch 58500  training loss: 10.782194137573242

 20%|███████████████▎                                                              | 59106/300000 [16:22<1:01:57, 64.79it/s]
epoch 58600  training loss: 10.82518196105957
epoch 58600  clean testing loss: 51.61282730102539
epoch 58700  training loss: 11.106778144836426
epoch 58700  clean testing loss: 50.92864227294922
epoch 58800  training loss: 10.838484764099121
epoch 58800  clean testing loss: 51.36845779418945
epoch 58900  training loss: 10.797911643981934
epoch 58900  clean testing loss: 50.688594818115234
epoch 59000  training loss: 10.318520545959473
epoch 59000  clean testing loss: 50.502113342285156
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise5.00e+01_invop1 ...
epoch 59100  training loss: 11.017166137695312

 20%|███████████████▍                                                              | 59239/300000 [16:24<1:01:42, 65.03it/s]
epoch 59200  training loss: 11.540325164794922

 20%|███████████████▍                                                              | 59372/300000 [16:26<1:00:14, 66.57it/s]
epoch 59300  training loss: 10.971075057983398

 20%|███████████████▍                                                              | 59505/300000 [16:28<1:00:05, 66.70it/s]
epoch 59400  training loss: 10.434465408325195
epoch 59400  clean testing loss: 50.15880584716797
epoch 59500  training loss: 10.113539695739746

 20%|███████████████▉                                                                | 59638/300000 [16:30<59:53, 66.89it/s]
epoch 59600  training loss: 11.2125244140625

 20%|███████████████▌                                                              | 59771/300000 [16:32<1:00:42, 65.95it/s]
epoch 59700  training loss: 11.694732666015625

 20%|███████████████▌                                                              | 59904/300000 [16:34<1:00:38, 66.00it/s]
epoch 59800  training loss: 11.080156326293945
epoch 59800  clean testing loss: 50.76899337768555
epoch 59900  training loss: 11.935256004333496

 20%|███████████████▌                                                              | 60037/300000 [16:36<1:00:19, 66.29it/s]
epoch 60000  training loss: 11.32176685333252
epoch 60000  clean testing loss: 50.82709503173828

 20%|███████████████▋                                                              | 60170/300000 [16:38<1:00:16, 66.32it/s]
epoch 60100  training loss: 10.877287864685059

 20%|███████████████▋                                                              | 60303/300000 [16:40<1:00:27, 66.07it/s]
epoch 60200  training loss: 11.244973182678223
epoch 60200  clean testing loss: 51.152931213378906
epoch 60300  training loss: 11.301332473754883

 20%|███████████████▋                                                              | 60436/300000 [16:42<1:00:42, 65.77it/s]
epoch 60400  training loss: 11.841943740844727

 20%|███████████████▋                                                              | 60569/300000 [16:44<1:00:42, 65.74it/s]
epoch 60500  training loss: 12.056042671203613

 20%|████████████████▏                                                               | 60702/300000 [16:46<59:58, 66.50it/s]
epoch 60600  training loss: 11.656709671020508
epoch 60600  clean testing loss: 50.44905471801758
epoch 60700  training loss: 11.569123268127441
epoch 60700  clean testing loss: 50.70428466796875
epoch 60800  training loss: 11.53865909576416
epoch 60800  clean testing loss: 51.97743225097656
epoch 60900  training loss: 11.516593933105469
epoch 60900  clean testing loss: 51.038169860839844
epoch 61000  training loss: 12.05294132232666
epoch 61000  clean testing loss: 50.966064453125

 20%|████████████████▎                                                               | 61066/300000 [16:52<59:34, 66.84it/s]
epoch 61100  training loss: 11.768173217773438

 20%|████████████████▎                                                               | 61199/300000 [16:54<59:50, 66.50it/s]
epoch 61200  training loss: 12.082805633544922
epoch 61200  clean testing loss: 51.680789947509766
epoch 61300  training loss: 11.75279712677002

 20%|████████████████▎                                                               | 61332/300000 [16:56<59:24, 66.95it/s]
epoch 61400  training loss: 11.015543937683105

 20%|███████████████▉                                                              | 61465/300000 [16:58<1:00:44, 65.45it/s]
epoch 61500  training loss: 10.523149490356445

 21%|████████████████                                                              | 61598/300000 [17:00<1:00:06, 66.11it/s]
epoch 61600  training loss: 10.872174263000488
epoch 61600  clean testing loss: 51.92144775390625
epoch 61700  training loss: 10.995442390441895

 21%|████████████████▍                                                               | 61731/300000 [17:02<59:32, 66.69it/s]
epoch 61800  training loss: 10.631759643554688

 21%|████████████████                                                              | 61864/300000 [17:04<1:00:02, 66.11it/s]
epoch 61900  training loss: 11.399116516113281

 21%|████████████████                                                              | 61997/300000 [17:06<1:00:01, 66.08it/s]
epoch 62000  training loss: 11.273163795471191
epoch 62000  clean testing loss: 52.82063674926758
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise5.00e+01_invop1 ...
epoch 62100  training loss: 10.895336151123047

 21%|████████████████▌                                                               | 62130/300000 [17:08<59:29, 66.64it/s]
epoch 62200  training loss: 10.943137168884277

 21%|████████████████▌                                                               | 62263/300000 [17:10<59:43, 66.34it/s]
epoch 62300  training loss: 10.687521934509277

 21%|████████████████▋                                                               | 62396/300000 [17:12<59:46, 66.25it/s]
epoch 62400  training loss: 11.15610122680664
epoch 62400  clean testing loss: 50.296485900878906
epoch 62500  training loss: 11.35699462890625

 21%|████████████████▋                                                               | 62529/300000 [17:14<59:41, 66.31it/s]
epoch 62600  training loss: 10.967430114746094

 21%|████████████████▋                                                               | 62662/300000 [17:16<59:49, 66.12it/s]
epoch 62700  training loss: 10.586615562438965

 21%|████████████████▋                                                               | 62795/300000 [17:18<59:53, 66.02it/s]
epoch 62800  training loss: 11.295350074768066
epoch 62800  clean testing loss: 49.81550979614258
epoch 62900  training loss: 11.431624412536621

 21%|████████████████▊                                                               | 62928/300000 [17:20<59:47, 66.09it/s]
epoch 63000  training loss: 11.827917098999023
epoch 63000  clean testing loss: 49.89277267456055

 21%|████████████████▊                                                               | 63061/300000 [17:22<59:44, 66.10it/s]
epoch 63100  training loss: 11.597479820251465

 21%|████████████████▊                                                               | 63194/300000 [17:24<59:50, 65.96it/s]
epoch 63200  training loss: 11.336758613586426
epoch 63200  clean testing loss: 50.637962341308594
epoch 63300  training loss: 11.987957954406738

 21%|████████████████▉                                                               | 63327/300000 [17:26<59:40, 66.10it/s]
epoch 63400  training loss: 12.322793960571289

 21%|████████████████▍                                                             | 63460/300000 [17:28<1:00:32, 65.12it/s]
epoch 63500  training loss: 11.803497314453125
epoch 63500  clean testing loss: 51.582679748535156
epoch 63600  training loss: 11.882156372070312
epoch 63600  clean testing loss: 51.30005645751953
epoch 63700  training loss: 12.044905662536621
epoch 63700  clean testing loss: 51.032752990722656
epoch 63800  training loss: 11.702706336975098
epoch 63800  clean testing loss: 51.71138381958008
epoch 63900  training loss: 11.606806755065918
epoch 63900  clean testing loss: 51.01218032836914
epoch 64000  training loss: 11.43575382232666
epoch 64000  clean testing loss: 51.362892150878906
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise5.00e+01_invop1 ...
epoch 64100  training loss: 12.004361152648926

 21%|████████████████▋                                                             | 64118/300000 [17:40<1:00:51, 64.60it/s]
epoch 64200  training loss: 12.016470909118652

 21%|█████████████████▏                                                              | 64251/300000 [17:42<59:51, 65.65it/s]
epoch 64300  training loss: 11.317541122436523


 22%|█████████████████▏                                                              | 64517/300000 [17:46<59:10, 66.32it/s]
epoch 64400  training loss: 10.947139739990234
epoch 64400  clean testing loss: 51.25801086425781
epoch 64500  training loss: 11.795825958251953

 22%|█████████████████▏                                                              | 64650/300000 [17:48<58:49, 66.68it/s]
epoch 64600  training loss: 11.476459503173828

 22%|█████████████████▎                                                              | 64783/300000 [17:50<59:02, 66.41it/s]
epoch 64700  training loss: 11.654104232788086

 22%|█████████████████▎                                                              | 64916/300000 [17:52<58:50, 66.59it/s]
epoch 64800  training loss: 11.923222541809082
epoch 64800  clean testing loss: 51.061065673828125
epoch 64900  training loss: 11.577664375305176

 22%|█████████████████▎                                                              | 65049/300000 [17:54<59:17, 66.04it/s]
epoch 65000  training loss: 11.712400436401367
epoch 65000  clean testing loss: 51.36609649658203

 22%|█████████████████▍                                                              | 65175/300000 [17:56<58:51, 66.49it/s]
epoch 65100  training loss: 11.94601058959961

 22%|█████████████████▍                                                              | 65308/300000 [17:58<59:33, 65.67it/s]
epoch 65200  training loss: 12.176042556762695
epoch 65200  clean testing loss: 51.782413482666016
epoch 65300  training loss: 11.715999603271484

 22%|█████████████████▍                                                              | 65441/300000 [18:00<59:35, 65.61it/s]
epoch 65400  training loss: 12.1306791305542

 22%|█████████████████▍                                                              | 65574/300000 [18:02<58:32, 66.75it/s]
epoch 65500  training loss: 11.608428001403809

 22%|█████████████████▌                                                              | 65707/300000 [18:04<58:49, 66.39it/s]
epoch 65600  training loss: 11.637657165527344

 22%|█████████████████▌                                                              | 65840/300000 [18:06<58:52, 66.29it/s]
epoch 65700  training loss: 11.202483177185059
epoch 65700  clean testing loss: 51.247947692871094
epoch 65800  training loss: 10.901590347290039

 22%|█████████████████▌                                                              | 65973/300000 [18:08<58:56, 66.17it/s]
epoch 65900  training loss: 11.083459854125977

 22%|█████████████████▏                                                            | 66127/300000 [18:20<1:01:31, 63.36it/s]
epoch 66000  training loss: 11.431962013244629
epoch 66000  clean testing loss: 51.45692825317383
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise5.00e+01_invop1 ...
epoch 66100  training loss: 11.934679985046387

 22%|█████████████████▋                                                              | 66260/300000 [18:22<58:19, 66.79it/s]
epoch 66200  training loss: 12.26046371459961

 22%|█████████████████▋                                                              | 66393/300000 [18:24<58:24, 66.66it/s]
epoch 66300  training loss: 11.65079116821289

 22%|█████████████████▋                                                              | 66526/300000 [18:26<58:53, 66.08it/s]
epoch 66400  training loss: 12.152600288391113
epoch 66400  clean testing loss: 51.309268951416016
epoch 66500  training loss: 11.48337459564209
epoch 66500  clean testing loss: 51.67330551147461
epoch 66600  training loss: 11.818706512451172
epoch 66600  clean testing loss: 51.3171272277832
epoch 66700  training loss: 11.998945236206055
epoch 66700  clean testing loss: 51.55111312866211
epoch 66800  training loss: 11.450724601745605

 22%|█████████████████▊                                                              | 66855/300000 [18:31<58:39, 66.24it/s]
epoch 66900  training loss: 11.748351097106934

 22%|█████████████████▊                                                              | 66988/300000 [18:33<58:55, 65.91it/s]
epoch 67000  training loss: 11.552374839782715
epoch 67000  clean testing loss: 52.38833236694336
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise5.00e+01_invop1 ...
epoch 67100  training loss: 11.126749038696289

 22%|█████████████████▉                                                              | 67121/300000 [18:35<58:51, 65.95it/s]
epoch 67200  training loss: 12.182355880737305

 22%|█████████████████▉                                                              | 67289/300000 [18:38<58:41, 66.09it/s]
epoch 67300  training loss: 11.404328346252441

 22%|█████████████████▉                                                              | 67422/300000 [18:40<59:00, 65.69it/s]
epoch 67400  training loss: 11.616480827331543
epoch 67400  clean testing loss: 51.03158187866211
epoch 67500  training loss: 11.916667938232422

 23%|██████████████████                                                              | 67555/300000 [18:42<58:28, 66.25it/s]
epoch 67600  training loss: 11.258350372314453

 23%|██████████████████                                                              | 67688/300000 [18:44<58:27, 66.23it/s]
epoch 67700  training loss: 11.53989028930664

 23%|██████████████████                                                              | 67821/300000 [18:46<59:00, 65.59it/s]
epoch 67800  training loss: 11.598420143127441
epoch 67800  clean testing loss: 50.47872543334961
epoch 67900  training loss: 11.702130317687988

 23%|██████████████████                                                              | 67954/300000 [18:48<58:14, 66.41it/s]
epoch 68000  training loss: 12.22508430480957
epoch 68000  clean testing loss: 50.42089080810547

 23%|██████████████████▏                                                             | 68080/300000 [18:50<58:47, 65.75it/s]
epoch 68100  training loss: 11.593131065368652

 23%|██████████████████▏                                                             | 68213/300000 [18:52<58:07, 66.46it/s]
epoch 68200  training loss: 11.724424362182617

 23%|██████████████████▏                                                             | 68346/300000 [18:54<59:11, 65.24it/s]
epoch 68300  training loss: 11.720780372619629
epoch 68300  clean testing loss: 50.47109603881836
epoch 68400  training loss: 11.192540168762207


 23%|██████████████████▍                                                             | 69123/300000 [19:07<58:23, 65.91it/s]
epoch 68500  training loss: 11.444853782653809
epoch 68500  clean testing loss: 50.26941680908203
epoch 68600  training loss: 11.191398620605469
epoch 68600  clean testing loss: 50.53662872314453
epoch 68700  training loss: 10.780633926391602
epoch 68700  clean testing loss: 50.895484924316406
epoch 68800  training loss: 10.932908058166504
epoch 68800  clean testing loss: 51.01305389404297
epoch 68900  training loss: 10.90330982208252
epoch 68900  clean testing loss: 50.707767486572266
epoch 69000  training loss: 10.540559768676758
epoch 69000  clean testing loss: 50.571861267089844
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise5.00e+01_invop1 ...
epoch 69100  training loss: 11.146048545837402

 23%|██████████████████▍                                                             | 69256/300000 [19:09<58:00, 66.30it/s]
epoch 69200  training loss: 10.766561508178711
epoch 69200  clean testing loss: 50.5576286315918
epoch 69300  training loss: 10.943547248840332
epoch 69300  clean testing loss: 50.84524154663086
epoch 69400  training loss: 10.759356498718262
epoch 69400  clean testing loss: 51.5370979309082
epoch 69500  training loss: 11.432297706604004

 23%|██████████████████▌                                                             | 69494/300000 [19:13<58:00, 66.23it/s]
epoch 69600  training loss: 10.579909324645996

 23%|██████████████████▌                                                             | 69627/300000 [19:15<57:38, 66.62it/s]
epoch 69700  training loss: 10.634403228759766

 23%|██████████████████▌                                                             | 69760/300000 [19:17<57:36, 66.62it/s]
epoch 69800  training loss: 11.579085350036621
epoch 69800  clean testing loss: 52.8369255065918
epoch 69900  training loss: 11.268424034118652

 23%|██████████████████▋                                                             | 69893/300000 [19:19<58:01, 66.10it/s]
epoch 70000  training loss: 10.498334884643555
epoch 70000  clean testing loss: 52.21503448486328


 23%|██████████████████▏                                                           | 70110/300000 [19:31<1:05:46, 58.25it/s]
epoch 70100  training loss: 10.531420707702637
epoch 70100  clean testing loss: 52.74386215209961
epoch 70200  training loss: 10.567658424377441

 23%|██████████████████▋                                                             | 70243/300000 [19:33<57:43, 66.35it/s]
epoch 70300  training loss: 11.187344551086426

 23%|██████████████████▊                                                             | 70376/300000 [19:35<58:32, 65.38it/s]
epoch 70400  training loss: 10.928543090820312

 24%|██████████████████▊                                                             | 70509/300000 [19:37<58:02, 65.91it/s]
epoch 70500  training loss: 11.024904251098633
epoch 70500  clean testing loss: 52.279300689697266
epoch 70600  training loss: 11.316353797912598

 24%|██████████████████▊                                                             | 70642/300000 [19:39<57:21, 66.65it/s]
epoch 70700  training loss: 10.1832857131958

 24%|██████████████████▊                                                             | 70775/300000 [19:41<57:24, 66.54it/s]
epoch 70800  training loss: 11.008946418762207

 24%|██████████████████▉                                                             | 70908/300000 [19:43<57:52, 65.97it/s]
epoch 70900  training loss: 10.663654327392578
epoch 70900  clean testing loss: 51.70189666748047
epoch 71000  training loss: 10.71532917022705
epoch 71000  clean testing loss: 52.24862289428711

 24%|██████████████████▉                                                             | 71050/300000 [19:45<47:48, 79.80it/s]
epoch 71100  training loss: 11.137953758239746

 24%|███████████████████                                                             | 71273/300000 [19:48<57:49, 65.93it/s]
epoch 71200  training loss: 11.066884994506836
epoch 71200  clean testing loss: 51.654510498046875
epoch 71300  training loss: 10.648828506469727

 24%|███████████████████                                                             | 71406/300000 [19:50<57:32, 66.20it/s]
epoch 71400  training loss: 9.823010444641113

 24%|███████████████████                                                             | 71539/300000 [19:52<58:03, 65.59it/s]
epoch 71500  training loss: 10.9114408493042
epoch 71500  clean testing loss: 51.95655059814453
epoch 71600  training loss: 11.093244552612305
epoch 71600  clean testing loss: 51.82875061035156
epoch 71700  training loss: 11.102831840515137


 24%|███████████████████▏                                                            | 71903/300000 [19:58<57:13, 66.43it/s]
epoch 71800  training loss: 11.430469512939453
epoch 71800  clean testing loss: 52.711875915527344
epoch 71900  training loss: 11.34997272491455
epoch 71900  clean testing loss: 52.34919738769531
epoch 72000  training loss: 10.534689903259277
epoch 72000  clean testing loss: 52.11565017700195

 24%|███████████████████▏                                                            | 72036/300000 [20:00<57:05, 66.55it/s]
epoch 72100  training loss: 11.108882904052734

 24%|███████████████████▏                                                            | 72169/300000 [20:02<56:54, 66.73it/s]
epoch 72200  training loss: 11.287412643432617
epoch 72200  clean testing loss: 52.10951232910156
epoch 72300  training loss: 10.932389259338379

 24%|███████████████████▎                                                            | 72302/300000 [20:04<57:10, 66.38it/s]
epoch 72400  training loss: 10.760420799255371

 24%|███████████████████▎                                                            | 72435/300000 [20:06<57:03, 66.48it/s]
epoch 72500  training loss: 10.379271507263184

 24%|███████████████████▎                                                            | 72568/300000 [20:08<56:59, 66.52it/s]
epoch 72600  training loss: 11.029918670654297
epoch 72600  clean testing loss: 53.125450134277344
epoch 72700  training loss: 10.845178604125977

 24%|███████████████████▍                                                            | 72701/300000 [20:10<56:52, 66.61it/s]
epoch 72800  training loss: 10.800182342529297

 24%|███████████████████▍                                                            | 72834/300000 [20:12<57:54, 65.38it/s]
epoch 72900  training loss: 10.557003021240234

 24%|███████████████████▍                                                            | 72967/300000 [20:14<56:50, 66.58it/s]
epoch 73000  training loss: 11.523208618164062
epoch 73000  clean testing loss: 53.56999588012695
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise5.00e+01_invop1 ...
epoch 73100  training loss: 10.809828758239746

 24%|███████████████████▍                                                            | 73100/300000 [20:16<57:36, 65.64it/s]
epoch 73200  training loss: 11.123250961303711

 24%|███████████████████▌                                                            | 73247/300000 [20:18<45:56, 82.26it/s]
epoch 73300  training loss: 11.807683944702148
epoch 73300  clean testing loss: 53.149993896484375
epoch 73400  training loss: 10.448843955993652


 25%|███████████████████▊                                                            | 74115/300000 [20:29<57:46, 65.16it/s]
epoch 73500  training loss: 11.307232856750488
epoch 73500  clean testing loss: 52.36082458496094
epoch 73600  training loss: 10.68148136138916
epoch 73600  clean testing loss: 52.666603088378906
epoch 73700  training loss: 11.518254280090332
epoch 73700  clean testing loss: 52.442996978759766
epoch 73800  training loss: 11.586697578430176
epoch 73800  clean testing loss: 52.26270294189453
epoch 73900  training loss: 11.349100112915039
epoch 73900  clean testing loss: 52.91182327270508
epoch 74000  training loss: 11.244389533996582
epoch 74000  clean testing loss: 52.23970413208008
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise5.00e+01_invop1 ...
epoch 74100  training loss: 11.62169075012207
epoch 74100  clean testing loss: 53.28327941894531
epoch 74200  training loss: 11.136096000671387

 25%|███████████████████▊                                                            | 74248/300000 [20:31<57:49, 65.07it/s]
epoch 74300  training loss: 12.275358200073242

 25%|███████████████████▊                                                            | 74381/300000 [20:33<56:36, 66.42it/s]
epoch 74400  training loss: 10.5180082321167
epoch 74400  clean testing loss: 52.77818298339844
epoch 74500  training loss: 11.119093894958496

 25%|███████████████████▊                                                            | 74514/300000 [20:35<56:23, 66.65it/s]
epoch 74600  training loss: 11.457338333129883

 25%|███████████████████▉                                                            | 74647/300000 [20:37<56:32, 66.43it/s]
epoch 74700  training loss: 10.858148574829102

 25%|███████████████████▉                                                            | 74780/300000 [20:39<56:16, 66.71it/s]
epoch 74800  training loss: 11.26718807220459
epoch 74800  clean testing loss: 53.293251037597656
epoch 74900  training loss: 11.348358154296875


 25%|████████████████████                                                            | 75109/300000 [20:44<57:00, 65.74it/s]
epoch 75000  training loss: 10.323162078857422
epoch 75000  clean testing loss: 52.74408721923828
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise5.00e+01_invop1 ...
epoch 75100  training loss: 11.399819374084473

 25%|████████████████████                                                            | 75242/300000 [20:46<56:48, 65.93it/s]
epoch 75200  training loss: 11.239431381225586

 25%|████████████████████                                                            | 75368/300000 [20:48<57:02, 65.64it/s]
epoch 75300  training loss: 10.826705932617188

 25%|████████████████████▏                                                           | 75501/300000 [20:50<56:51, 65.81it/s]
epoch 75400  training loss: 11.301369667053223
epoch 75400  clean testing loss: 52.507171630859375
epoch 75500  training loss: 10.824191093444824

 25%|████████████████████▏                                                           | 75634/300000 [20:52<56:23, 66.32it/s]
epoch 75600  training loss: 11.80159854888916

 25%|████████████████████▏                                                           | 75767/300000 [20:54<57:14, 65.29it/s]
epoch 75700  training loss: 10.929122924804688

 25%|████████████████████▏                                                           | 75900/300000 [20:56<55:52, 66.84it/s]
epoch 75800  training loss: 10.655279159545898
epoch 75800  clean testing loss: 52.34881591796875
epoch 75900  training loss: 11.335532188415527

 25%|████████████████████▎                                                           | 76033/300000 [20:58<56:07, 66.52it/s]
epoch 76000  training loss: 10.504424095153809
epoch 76000  clean testing loss: 52.262149810791016

 25%|████████████████████▎                                                           | 76166/300000 [21:00<56:02, 66.57it/s]
epoch 76100  training loss: 10.735240936279297

 25%|████████████████████▎                                                           | 76299/300000 [21:02<56:01, 66.56it/s]
epoch 76200  training loss: 12.091968536376953
epoch 76200  clean testing loss: 53.32619094848633
epoch 76300  training loss: 10.87438678741455

 25%|████████████████████▍                                                           | 76432/300000 [21:04<56:34, 65.86it/s]
epoch 76400  training loss: 11.506725311279297

 26%|████████████████████▍                                                           | 76565/300000 [21:07<56:01, 66.47it/s]
epoch 76500  training loss: 10.708131790161133
epoch 76500  clean testing loss: 52.088172912597656
epoch 76600  training loss: 10.80970287322998
epoch 76600  clean testing loss: 52.49734115600586
epoch 76700  training loss: 11.008607864379883
epoch 76700  clean testing loss: 52.966556549072266
epoch 76800  training loss: 11.625990867614746
epoch 76800  clean testing loss: 52.90937805175781
epoch 76900  training loss: 11.760760307312012
epoch 76900  clean testing loss: 52.559993743896484
epoch 77000  training loss: 10.617429733276367
epoch 77000  clean testing loss: 52.170677185058594
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise5.00e+01_invop1 ...
epoch 77100  training loss: 10.705496788024902

 26%|████████████████████▌                                                           | 77104/300000 [21:16<57:31, 64.57it/s]
epoch 77200  training loss: 10.424725532531738

 26%|████████████████████▌                                                           | 77237/300000 [21:18<56:04, 66.21it/s]
epoch 77300  training loss: 10.71545696258545

 26%|████████████████████▋                                                           | 77370/300000 [21:20<55:32, 66.82it/s]
epoch 77400  training loss: 10.478795051574707

 26%|████████████████████▋                                                           | 77503/300000 [21:22<55:27, 66.86it/s]
epoch 77500  training loss: 10.36506462097168
epoch 77500  clean testing loss: 52.25310516357422
epoch 77600  training loss: 10.263955116271973

 26%|████████████████████▋                                                           | 77636/300000 [21:24<55:19, 66.98it/s]
epoch 77700  training loss: 10.480358123779297
epoch 77700  clean testing loss: 52.70747756958008
epoch 77800  training loss: 10.442258834838867
epoch 77800  clean testing loss: 52.38747787475586
epoch 77900  training loss: 10.372194290161133
epoch 77900  clean testing loss: 53.029197692871094
epoch 78000  training loss: 11.005086898803711
epoch 78000  clean testing loss: 53.26808547973633
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise5.00e+01_invop1 ...
epoch 78100  training loss: 10.371758460998535

 26%|████████████████████▎                                                         | 78098/300000 [21:34<1:00:11, 61.44it/s]
epoch 78200  training loss: 10.242463111877441

 26%|████████████████████▊                                                           | 78231/300000 [21:36<55:35, 66.49it/s]
epoch 78300  training loss: 10.946121215820312

 26%|████████████████████▉                                                           | 78364/300000 [21:38<56:00, 65.96it/s]
epoch 78400  training loss: 11.017338752746582
epoch 78400  clean testing loss: 53.233314514160156
epoch 78500  training loss: 10.972990989685059

 26%|████████████████████▉                                                           | 78497/300000 [21:40<55:46, 66.18it/s]
epoch 78600  training loss: 11.457390785217285

 26%|████████████████████▉                                                           | 78630/300000 [21:42<55:22, 66.63it/s]
epoch 78700  training loss: 11.655900955200195

 26%|█████████████████████                                                           | 78763/300000 [21:44<55:25, 66.53it/s]
epoch 78800  training loss: 12.021883010864258
epoch 78800  clean testing loss: 53.146846771240234
epoch 78900  training loss: 11.614136695861816

 26%|█████████████████████                                                           | 78896/300000 [21:46<55:13, 66.72it/s]
epoch 79000  training loss: 11.886425971984863
epoch 79000  clean testing loss: 53.3521728515625

 26%|█████████████████████                                                           | 79029/300000 [21:48<55:38, 66.20it/s]
epoch 79100  training loss: 11.345393180847168

 26%|█████████████████████                                                           | 79162/300000 [21:50<55:11, 66.68it/s]
epoch 79200  training loss: 10.749788284301758
epoch 79200  clean testing loss: 53.69486999511719
epoch 79300  training loss: 11.461305618286133

 26%|█████████████████████▏                                                          | 79295/300000 [21:52<55:14, 66.60it/s]
epoch 79400  training loss: 11.764043807983398
epoch 79400  clean testing loss: 53.7750129699707
epoch 79500  training loss: 11.51034164428711
epoch 79500  clean testing loss: 53.98140335083008
epoch 79600  training loss: 11.70514965057373
epoch 79600  clean testing loss: 53.662322998046875
epoch 79700  training loss: 10.347926139831543
epoch 79700  clean testing loss: 53.03719711303711
epoch 79800  training loss: 11.520354270935059
epoch 79800  clean testing loss: 53.545040130615234
epoch 79900  training loss: 11.486083030700684

 27%|█████████████████████▎                                                          | 79960/300000 [22:02<55:03, 66.62it/s]
epoch 80000  training loss: 11.268671989440918
epoch 80000  clean testing loss: 53.01976776123047

 27%|█████████████████████▎                                                          | 80093/300000 [22:04<55:03, 66.57it/s]
epoch 80100  training loss: 11.203811645507812
epoch 80100  clean testing loss: 53.126731872558594
epoch 80200  training loss: 11.274552345275879

 27%|█████████████████████▍                                                          | 80226/300000 [22:06<55:39, 65.81it/s]
epoch 80300  training loss: 11.571221351623535

 27%|█████████████████████▍                                                          | 80359/300000 [22:08<55:26, 66.02it/s]
epoch 80400  training loss: 11.142585754394531


 27%|█████████████████████▋                                                          | 81115/300000 [22:22<56:22, 64.71it/s]
epoch 80500  training loss: 10.82106876373291
epoch 80500  clean testing loss: 54.79533004760742
epoch 80600  training loss: 11.793432235717773
epoch 80600  clean testing loss: 54.3397216796875
epoch 80700  training loss: 11.604299545288086
epoch 80700  clean testing loss: 54.34814453125
epoch 80800  training loss: 11.190659523010254
epoch 80800  clean testing loss: 54.289634704589844
epoch 80900  training loss: 12.061079978942871
epoch 80900  clean testing loss: 53.65513610839844
epoch 81000  training loss: 10.736166954040527
epoch 81000  clean testing loss: 54.553775787353516
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise5.00e+01_invop1 ...
epoch 81100  training loss: 11.41386604309082

 27%|█████████████████████▋                                                          | 81248/300000 [22:24<55:05, 66.17it/s]
epoch 81200  training loss: 11.007824897766113

 27%|█████████████████████▋                                                          | 81451/300000 [22:27<54:40, 66.62it/s]
epoch 81300  training loss: 11.080016136169434
epoch 81300  clean testing loss: 54.36954879760742
epoch 81400  training loss: 12.395893096923828

 27%|█████████████████████▊                                                          | 81584/300000 [22:29<54:04, 67.33it/s]
epoch 81500  training loss: 11.675756454467773

 27%|█████████████████████▊                                                          | 81717/300000 [22:31<55:12, 65.90it/s]
epoch 81600  training loss: 10.73618221282959
epoch 81600  clean testing loss: 54.10475158691406
epoch 81700  training loss: 10.998363494873047

 27%|█████████████████████▊                                                          | 81850/300000 [22:33<55:13, 65.84it/s]
epoch 81800  training loss: 11.065755844116211

 27%|█████████████████████▊                                                          | 81983/300000 [22:35<54:25, 66.76it/s]
epoch 81900  training loss: 11.068329811096191

 27%|█████████████████████▉                                                          | 82116/300000 [22:37<54:50, 66.22it/s]
epoch 82000  training loss: 12.00735092163086
epoch 82000  clean testing loss: 54.45450210571289
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise5.00e+01_invop1 ...
epoch 82100  training loss: 10.905169486999512

 27%|█████████████████████▉                                                          | 82249/300000 [22:39<54:49, 66.19it/s]
epoch 82200  training loss: 11.846986770629883

 28%|██████████████████████                                                          | 82816/300000 [22:48<55:00, 65.81it/s]
epoch 82300  training loss: 11.449488639831543
epoch 82300  clean testing loss: 54.73387145996094
epoch 82400  training loss: 11.64545726776123
epoch 82400  clean testing loss: 54.57107925415039
epoch 82500  training loss: 11.63623046875
epoch 82500  clean testing loss: 54.92387008666992
epoch 82600  training loss: 11.11164665222168
epoch 82600  clean testing loss: 54.94805908203125
epoch 82700  training loss: 11.243823051452637

 28%|██████████████████████                                                          | 82949/300000 [22:50<53:55, 67.09it/s]
epoch 82800  training loss: 11.52866268157959
epoch 82800  clean testing loss: 55.09142303466797
epoch 82900  training loss: 11.495954513549805

 28%|██████████████████████▏                                                         | 83082/300000 [22:52<53:55, 67.05it/s]
epoch 83000  training loss: 10.283927917480469
epoch 83000  clean testing loss: 55.16423797607422

 28%|██████████████████████▏                                                         | 83215/300000 [22:54<54:26, 66.36it/s]
epoch 83100  training loss: 10.795534133911133

 28%|██████████████████████▏                                                         | 83348/300000 [22:56<54:16, 66.53it/s]
epoch 83200  training loss: 10.540419578552246
epoch 83200  clean testing loss: 54.950721740722656
epoch 83300  training loss: 10.452483177185059

 28%|██████████████████████▎                                                         | 83481/300000 [22:58<54:20, 66.41it/s]
epoch 83400  training loss: 11.466124534606934

 28%|██████████████████████▎                                                         | 83614/300000 [23:00<54:42, 65.93it/s]
epoch 83500  training loss: 11.200479507446289

 28%|██████████████████████▎                                                         | 83747/300000 [23:02<54:08, 66.56it/s]
epoch 83600  training loss: 10.459957122802734
epoch 83600  clean testing loss: 55.51014709472656
epoch 83700  training loss: 11.618817329406738

 28%|██████████████████████▎                                                         | 83880/300000 [23:04<53:30, 67.31it/s]
epoch 83800  training loss: 11.350682258605957

 28%|██████████████████████▍                                                         | 84013/300000 [23:06<54:49, 65.65it/s]
epoch 83900  training loss: 10.49510383605957

 28%|██████████████████████▍                                                         | 84146/300000 [23:08<53:59, 66.64it/s]
epoch 84000  training loss: 11.511720657348633
epoch 84000  clean testing loss: 55.63833236694336
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise5.00e+01_invop1 ...
epoch 84100  training loss: 10.984268188476562

 28%|██████████████████████▍                                                         | 84279/300000 [23:10<55:13, 65.09it/s]
epoch 84200  training loss: 11.716980934143066

 28%|██████████████████████▌                                                         | 84405/300000 [23:12<54:15, 66.22it/s]
epoch 84300  training loss: 11.043530464172363

 28%|██████████████████████▌                                                         | 84538/300000 [23:14<54:25, 65.99it/s]
epoch 84400  training loss: 10.665681838989258
epoch 84400  clean testing loss: 55.19744873046875
epoch 84500  training loss: 11.53739070892334

 28%|██████████████████████▌                                                         | 84671/300000 [23:16<54:06, 66.33it/s]
epoch 84600  training loss: 10.206293106079102

 28%|██████████████████████▌                                                         | 84839/300000 [23:18<54:30, 65.79it/s]
epoch 84700  training loss: 11.585901260375977
epoch 84700  clean testing loss: 54.86790466308594
epoch 84800  training loss: 10.848254203796387

 28%|██████████████████████▋                                                         | 84972/300000 [23:20<54:39, 65.57it/s]
epoch 84900  training loss: 11.302836418151855

 28%|██████████████████████▋                                                         | 85105/300000 [23:22<54:35, 65.61it/s]
epoch 85000  training loss: 11.57541275024414
epoch 85000  clean testing loss: 54.76042556762695
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise5.00e+01_invop1 ...
epoch 85100  training loss: 10.35009479522705

 28%|██████████████████████▋                                                         | 85238/300000 [23:24<53:49, 66.50it/s]
epoch 85200  training loss: 10.715215682983398
epoch 85200  clean testing loss: 55.695377349853516
epoch 85300  training loss: 11.267068862915039

 28%|██████████████████████▊                                                         | 85371/300000 [23:26<53:49, 66.46it/s]
epoch 85400  training loss: 11.659687042236328
epoch 85400  clean testing loss: 55.229827880859375
epoch 85500  training loss: 11.55330753326416
epoch 85500  clean testing loss: 55.11030197143555
epoch 85600  training loss: 11.500597953796387
epoch 85600  clean testing loss: 55.19550323486328
epoch 85700  training loss: 11.111705780029297
epoch 85700  clean testing loss: 55.0235481262207
epoch 85800  training loss: 11.50590705871582
epoch 85800  clean testing loss: 55.19724655151367
epoch 85900  training loss: 11.255049705505371
epoch 85900  clean testing loss: 55.575706481933594
epoch 86000  training loss: 10.471466064453125
epoch 86000  clean testing loss: 54.81612777709961


 29%|██████████████████████▉                                                         | 86197/300000 [23:39<54:09, 65.79it/s]
epoch 86100  training loss: 10.890178680419922
epoch 86100  clean testing loss: 54.91013717651367
epoch 86200  training loss: 11.29322338104248

 29%|███████████████████████                                                         | 86330/300000 [23:41<53:54, 66.05it/s]
epoch 86300  training loss: 11.295618057250977
epoch 86300  clean testing loss: 54.454227447509766
epoch 86400  training loss: 11.658838272094727

 29%|███████████████████████                                                         | 86463/300000 [23:43<53:05, 67.04it/s]
epoch 86500  training loss: 11.364912986755371
epoch 86500  clean testing loss: 54.671180725097656
epoch 86600  training loss: 10.837125778198242

 29%|███████████████████████                                                         | 86596/300000 [23:45<53:08, 66.94it/s]
epoch 86700  training loss: 12.2427978515625

 29%|███████████████████████▏                                                        | 86729/300000 [23:47<53:44, 66.13it/s]
epoch 86800  training loss: 12.248946189880371

 29%|███████████████████████▏                                                        | 86862/300000 [23:49<55:43, 63.75it/s]
epoch 86900  training loss: 11.611223220825195
epoch 86900  clean testing loss: 54.912784576416016
epoch 87000  training loss: 10.945748329162598
epoch 87000  clean testing loss: 54.721744537353516


 29%|██████████████████████▋                                                       | 87051/300000 [23:55<1:32:31, 38.36it/s]
epoch 87100  training loss: 11.716242790222168

 29%|███████████████████████▏                                                        | 87184/300000 [23:57<53:28, 66.33it/s]
epoch 87200  training loss: 11.813424110412598
epoch 87200  clean testing loss: 54.74814224243164
epoch 87300  training loss: 10.817523002624512

 29%|███████████████████████▎                                                        | 87450/300000 [24:01<53:04, 66.74it/s]
epoch 87400  training loss: 12.171384811401367

 29%|███████████████████████▎                                                        | 87583/300000 [24:03<53:15, 66.48it/s]
epoch 87500  training loss: 11.733908653259277

 29%|███████████████████████▍                                                        | 87716/300000 [24:05<53:18, 66.37it/s]
epoch 87600  training loss: 11.998902320861816
epoch 87600  clean testing loss: 54.97372055053711
epoch 87700  training loss: 11.202201843261719
epoch 87700  clean testing loss: 54.91677474975586
epoch 87800  training loss: 11.052162170410156

 29%|███████████████████████▍                                                        | 87849/300000 [24:07<52:57, 66.77it/s]
epoch 87900  training loss: 11.978153228759766

 29%|███████████████████████▍                                                        | 87982/300000 [24:09<53:30, 66.03it/s]
epoch 88000  training loss: 11.648880958557129
epoch 88000  clean testing loss: 54.9043083190918

 29%|███████████████████████▍                                                        | 88115/300000 [24:12<53:51, 65.58it/s]
epoch 88100  training loss: 11.296975135803223

 29%|███████████████████████▌                                                        | 88255/300000 [24:14<53:07, 66.44it/s]
epoch 88200  training loss: 12.762394905090332

 29%|███████████████████████▌                                                        | 88381/300000 [24:16<53:11, 66.31it/s]
epoch 88300  training loss: 12.640305519104004
epoch 88300  clean testing loss: 55.63945007324219
epoch 88400  training loss: 11.227270126342773

 30%|███████████████████████▌                                                        | 88514/300000 [24:18<53:05, 66.39it/s]
epoch 88500  training loss: 11.396986961364746

 30%|███████████████████████▋                                                        | 88654/300000 [24:20<53:34, 65.75it/s]
epoch 88600  training loss: 10.801677703857422
epoch 88600  clean testing loss: 55.88510513305664
epoch 88700  training loss: 11.463438034057617

 30%|███████████████████████▊                                                        | 89116/300000 [24:28<53:20, 65.89it/s]
epoch 88800  training loss: 11.34994888305664
epoch 88800  clean testing loss: 55.766075134277344
epoch 88900  training loss: 11.19076156616211
epoch 88900  clean testing loss: 55.859676361083984
epoch 89000  training loss: 11.184295654296875
epoch 89000  clean testing loss: 55.82441711425781
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise5.00e+01_invop1 ...
epoch 89100  training loss: 11.687968254089355

 30%|███████████████████████▊                                                        | 89249/300000 [24:30<52:57, 66.33it/s]
epoch 89200  training loss: 12.116000175476074

 30%|███████████████████████▊                                                        | 89382/300000 [24:32<52:44, 66.56it/s]
epoch 89300  training loss: 12.60472583770752

 30%|███████████████████████▊                                                        | 89515/300000 [24:34<52:43, 66.54it/s]
epoch 89400  training loss: 12.42033863067627
epoch 89400  clean testing loss: 55.88743209838867
epoch 89500  training loss: 11.135454177856445
epoch 89500  clean testing loss: 55.60685348510742
epoch 89600  training loss: 10.758576393127441
epoch 89600  clean testing loss: 55.50856399536133
epoch 89700  training loss: 11.174251556396484
epoch 89700  clean testing loss: 55.604774475097656
epoch 89800  training loss: 11.813236236572266
epoch 89800  clean testing loss: 55.553253173828125
epoch 89900  training loss: 11.250970840454102
epoch 89900  clean testing loss: 55.7208251953125
epoch 90000  training loss: 10.53783130645752
epoch 90000  clean testing loss: 55.359291076660156


 30%|████████████████████████                                                        | 90208/300000 [24:44<53:11, 65.74it/s]
epoch 90100  training loss: 11.672176361083984
epoch 90100  clean testing loss: 55.460811614990234
epoch 90200  training loss: 11.262137413024902

 30%|████████████████████████                                                        | 90341/300000 [24:46<51:58, 67.22it/s]
epoch 90300  training loss: 10.75761604309082

 30%|████████████████████████▏                                                       | 90474/300000 [24:48<53:41, 65.03it/s]
epoch 90400  training loss: 11.383978843688965

 30%|████████████████████████▏                                                       | 90607/300000 [24:50<52:50, 66.05it/s]
epoch 90500  training loss: 11.874156951904297
epoch 90500  clean testing loss: 55.5845832824707
epoch 90600  training loss: 11.081578254699707

 30%|████████████████████████▏                                                       | 90705/300000 [24:52<52:22, 66.61it/s]
epoch 90700  training loss: 10.45549488067627

 30%|████████████████████████▎                                                       | 90943/300000 [24:55<52:23, 66.50it/s]
epoch 90800  training loss: 10.766164779663086
epoch 90800  clean testing loss: 55.48630905151367
epoch 90900  training loss: 11.10876178741455

 30%|████████████████████████▎                                                       | 91076/300000 [24:57<52:13, 66.68it/s]
epoch 91000  training loss: 11.929506301879883
epoch 91000  clean testing loss: 55.47305679321289
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise5.00e+01_invop1 ...
epoch 91100  training loss: 11.993061065673828
epoch 91100  clean testing loss: 55.663204193115234
epoch 91200  training loss: 11.627333641052246

 30%|████████████████████████▎                                                       | 91209/300000 [24:59<52:45, 65.96it/s]
epoch 91300  training loss: 12.444852828979492


 30%|████████████████████████▍                                                       | 91475/300000 [25:03<52:37, 66.05it/s]
epoch 91400  training loss: 12.482786178588867

 31%|████████████████████████▍                                                       | 91608/300000 [25:05<53:10, 65.32it/s]
epoch 91500  training loss: 10.96226978302002
epoch 91500  clean testing loss: 55.27324295043945
epoch 91600  training loss: 10.96242618560791
epoch 91600  clean testing loss: 55.24925231933594
epoch 91700  training loss: 10.960139274597168

 31%|████████████████████████▍                                                       | 91741/300000 [25:07<51:48, 67.00it/s]
epoch 91800  training loss: 12.153219223022461

 31%|████████████████████████▍                                                       | 91874/300000 [25:09<51:57, 66.77it/s]
epoch 91900  training loss: 11.803972244262695
epoch 91900  clean testing loss: 55.506229400634766
epoch 92000  training loss: 11.07454776763916
epoch 92000  clean testing loss: 55.12976837158203


 31%|███████████████████████▉                                                      | 92028/300000 [25:13<2:19:04, 24.92it/s]
epoch 92100  training loss: 11.002847671508789

 31%|████████████████████████▌                                                       | 92161/300000 [25:15<52:47, 65.61it/s]
epoch 92200  training loss: 11.682753562927246

 31%|████████████████████████▌                                                       | 92294/300000 [25:17<52:09, 66.37it/s]
epoch 92300  training loss: 12.304692268371582
epoch 92300  clean testing loss: 55.842796325683594
epoch 92400  training loss: 11.293277740478516

 31%|████████████████████████▋                                                       | 92420/300000 [25:19<52:50, 65.47it/s]
epoch 92500  training loss: 10.909534454345703

 31%|████████████████████████▋                                                       | 92553/300000 [25:21<52:20, 66.05it/s]
epoch 92600  training loss: 11.248004913330078

 31%|████████████████████████▋                                                       | 92686/300000 [25:23<52:06, 66.31it/s]
epoch 92700  training loss: 10.719125747680664
epoch 92700  clean testing loss: 55.28413391113281
epoch 92800  training loss: 10.62828254699707

 31%|████████████████████████▊                                                       | 92819/300000 [25:25<52:05, 66.29it/s]
epoch 92900  training loss: 10.53350830078125

 31%|████████████████████████▊                                                       | 92952/300000 [25:27<52:16, 66.00it/s]
epoch 93000  training loss: 11.91950798034668
epoch 93000  clean testing loss: 55.29849624633789

 31%|████████████████████████▊                                                       | 92994/300000 [25:28<51:57, 66.39it/s]
epoch 93100  training loss: 10.975711822509766

 31%|████████████████████████▊                                                       | 93099/300000 [25:31<53:28, 64.49it/s]
epoch 93200  training loss: 11.395471572875977

 31%|████████████████████████▊                                                       | 93232/300000 [25:33<52:06, 66.14it/s]
epoch 93300  training loss: 11.569384574890137


 31%|████████████████████████▉                                                       | 93666/300000 [25:39<52:11, 65.88it/s]
epoch 93400  training loss: 11.118391990661621
epoch 93400  clean testing loss: 55.243038177490234
epoch 93500  training loss: 10.925272941589355
epoch 93500  clean testing loss: 55.34161376953125
epoch 93600  training loss: 11.262357711791992
epoch 93600  clean testing loss: 55.17983627319336
epoch 93700  training loss: 11.603816032409668


 31%|█████████████████████████                                                       | 93932/300000 [25:43<52:33, 65.35it/s]
epoch 93800  training loss: 12.263708114624023
epoch 93800  clean testing loss: 55.58464431762695
epoch 93900  training loss: 10.247652053833008
epoch 93900  clean testing loss: 55.0550422668457
epoch 94000  training loss: 11.927107810974121
epoch 94000  clean testing loss: 55.30757141113281

 31%|█████████████████████████                                                       | 93995/300000 [25:44<51:13, 67.02it/s]
epoch 94100  training loss: 10.92662239074707

 31%|█████████████████████████                                                       | 94114/300000 [25:50<53:44, 63.85it/s]
epoch 94200  training loss: 11.520500183105469

 31%|█████████████████████████▏                                                      | 94247/300000 [25:52<51:14, 66.91it/s]
epoch 94300  training loss: 11.381656646728516


 32%|█████████████████████████▎                                                      | 94947/300000 [26:02<51:12, 66.74it/s]
epoch 94400  training loss: 11.659229278564453
epoch 94400  clean testing loss: 55.081424713134766
epoch 94500  training loss: 11.157770156860352
epoch 94500  clean testing loss: 55.17406463623047
epoch 94600  training loss: 12.334756851196289
epoch 94600  clean testing loss: 55.068668365478516
epoch 94700  training loss: 11.352361679077148
epoch 94700  clean testing loss: 54.921573638916016
epoch 94800  training loss: 11.731167793273926
epoch 94800  clean testing loss: 55.21505355834961
epoch 94900  training loss: 11.567010879516602

 32%|█████████████████████████▎                                                      | 95080/300000 [26:05<51:24, 66.43it/s]
epoch 95000  training loss: 11.021642684936523
epoch 95000  clean testing loss: 55.14866256713867

 32%|█████████████████████████▍                                                      | 95213/300000 [26:06<51:06, 66.78it/s]
epoch 95100  training loss: 12.239605903625488
epoch 95100  clean testing loss: 55.38748550415039
epoch 95200  training loss: 10.83333683013916

 32%|█████████████████████████▍                                                      | 95346/300000 [26:08<51:41, 65.99it/s]
epoch 95300  training loss: 11.142287254333496

 32%|█████████████████████████▍                                                      | 95451/300000 [26:10<50:50, 67.05it/s]
epoch 95400  training loss: 11.942547798156738

 32%|█████████████████████████▍                                                      | 95584/300000 [26:12<50:54, 66.91it/s]
epoch 95500  training loss: 11.151344299316406
epoch 95500  clean testing loss: 55.00694274902344
epoch 95600  training loss: 12.224513053894043

 32%|█████████████████████████▌                                                      | 95717/300000 [26:14<51:23, 66.25it/s]
epoch 95700  training loss: 11.646025657653809

 32%|█████████████████████████▌                                                      | 95850/300000 [26:16<51:34, 65.98it/s]
epoch 95800  training loss: 11.937236785888672

 32%|█████████████████████████▌                                                      | 95983/300000 [26:18<50:49, 66.89it/s]
epoch 95900  training loss: 11.151143074035645
epoch 95900  clean testing loss: 55.13927459716797
epoch 96000  training loss: 12.32661247253418
epoch 96000  clean testing loss: 55.26884460449219

 32%|█████████████████████████▋                                                      | 96116/300000 [26:20<51:30, 65.98it/s]
epoch 96100  training loss: 11.173771858215332

 32%|█████████████████████████▋                                                      | 96249/300000 [26:22<50:47, 66.86it/s]
epoch 96200  training loss: 10.868233680725098

 32%|█████████████████████████▋                                                      | 96382/300000 [26:24<50:30, 67.19it/s]
epoch 96300  training loss: 11.116385459899902
epoch 96300  clean testing loss: 55.0081672668457
epoch 96400  training loss: 11.306073188781738

 32%|█████████████████████████▋                                                      | 96515/300000 [26:26<51:05, 66.37it/s]
epoch 96500  training loss: 11.56167984008789

 32%|█████████████████████████▊                                                      | 96648/300000 [26:28<51:04, 66.36it/s]
epoch 96600  training loss: 12.40995979309082

 32%|█████████████████████████▊                                                      | 96781/300000 [26:30<51:02, 66.35it/s]
epoch 96700  training loss: 12.16210651397705

 32%|█████████████████████████▊                                                      | 96914/300000 [26:32<51:16, 66.00it/s]
epoch 96800  training loss: 12.349922180175781
epoch 96800  clean testing loss: 55.27802658081055
epoch 96900  training loss: 12.027194023132324

 32%|█████████████████████████▉                                                      | 97047/300000 [26:34<51:13, 66.03it/s]
epoch 97000  training loss: 11.415684700012207
epoch 97000  clean testing loss: 55.17287063598633

 32%|█████████████████████████▉                                                      | 97180/300000 [26:36<50:29, 66.95it/s]
epoch 97100  training loss: 12.06320858001709

 32%|█████████████████████████▉                                                      | 97313/300000 [26:38<51:02, 66.18it/s]
epoch 97200  training loss: 12.005258560180664
epoch 97200  clean testing loss: 55.335845947265625
epoch 97300  training loss: 11.549631118774414

 32%|█████████████████████████▉                                                      | 97446/300000 [26:40<50:53, 66.33it/s]
epoch 97400  training loss: 11.06011962890625

 33%|██████████████████████████                                                      | 97579/300000 [26:42<50:29, 66.82it/s]
epoch 97500  training loss: 11.243444442749023

 33%|██████████████████████████                                                      | 97712/300000 [26:44<51:18, 65.71it/s]
epoch 97600  training loss: 11.708876609802246
epoch 97600  clean testing loss: 55.40749740600586
epoch 97700  training loss: 11.064596176147461

 33%|██████████████████████████                                                      | 97845/300000 [26:46<50:48, 66.31it/s]
epoch 97800  training loss: 11.47264289855957

 33%|██████████████████████████▏                                                     | 97978/300000 [26:48<50:36, 66.54it/s]
epoch 97900  training loss: 11.828994750976562

 33%|██████████████████████████▏                                                     | 98111/300000 [26:50<50:36, 66.49it/s]
epoch 98000  training loss: 11.279784202575684
epoch 98000  clean testing loss: 55.622314453125
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise5.00e+01_invop1 ...
epoch 98100  training loss: 11.089717864990234

 33%|██████████████████████████▏                                                     | 98244/300000 [26:52<50:40, 66.36it/s]
epoch 98200  training loss: 11.599186897277832

 33%|██████████████████████████▏                                                     | 98377/300000 [26:54<50:51, 66.08it/s]
epoch 98300  training loss: 11.459562301635742

 33%|██████████████████████████▎                                                     | 98510/300000 [26:56<49:56, 67.23it/s]
epoch 98400  training loss: 11.701811790466309
epoch 98400  clean testing loss: 55.52932357788086
epoch 98500  training loss: 12.119726181030273

 33%|██████████████████████████▎                                                     | 98643/300000 [26:58<50:25, 66.56it/s]
epoch 98600  training loss: 11.619790077209473

 33%|██████████████████████████▎                                                     | 98776/300000 [27:00<50:07, 66.92it/s]
epoch 98700  training loss: 12.520862579345703
epoch 98700  clean testing loss: 55.46502685546875
epoch 98800  training loss: 11.876065254211426
epoch 98800  clean testing loss: 55.284122467041016
epoch 98900  training loss: 12.392725944519043
epoch 98900  clean testing loss: 55.3682746887207
epoch 99000  training loss: 12.678216934204102
epoch 99000  clean testing loss: 55.34003448486328

 33%|██████████████████████████▍                                                     | 99077/300000 [27:05<50:11, 66.72it/s]
epoch 99100  training loss: 11.935354232788086
epoch 99100  clean testing loss: 55.42317581176758
epoch 99200  training loss: 11.324824333190918

 33%|██████████████████████████▍                                                     | 99210/300000 [27:07<50:47, 65.88it/s]
epoch 99300  training loss: 12.068554878234863

 33%|██████████████████████████▍                                                     | 99343/300000 [27:09<50:54, 65.69it/s]
epoch 99400  training loss: 12.248163223266602

 33%|██████████████████████████▌                                                     | 99476/300000 [27:11<50:11, 66.59it/s]
epoch 99500  training loss: 11.641610145568848
epoch 99500  clean testing loss: 55.542320251464844
epoch 99600  training loss: 12.084389686584473

 33%|██████████████████████████▌                                                     | 99609/300000 [27:13<50:08, 66.62it/s]
epoch 99700  training loss: 11.818802833557129

 33%|██████████████████████████▌                                                     | 99742/300000 [27:15<50:32, 66.04it/s]
epoch 99800  training loss: 11.46301555633545


 33%|██████████████████████████▋                                                     | 99973/300000 [27:18<50:32, 65.97it/s]
epoch 99900  training loss: 11.625701904296875
epoch 99900  clean testing loss: 55.507110595703125
epoch 100000  training loss: 11.981142044067383
epoch 100000  clean testing loss: 55.42762756347656


 33%|██████████████████████████▍                                                    | 100225/300000 [27:42<51:08, 65.10it/s]
epoch 100100  training loss: 11.652633666992188
epoch 100100  clean testing loss: 55.744964599609375
epoch 100200  training loss: 11.736627578735352

 33%|██████████████████████████▍                                                    | 100358/300000 [27:44<50:16, 66.18it/s]
epoch 100300  training loss: 11.64560317993164

 33%|██████████████████████████▍                                                    | 100491/300000 [27:46<50:16, 66.13it/s]
epoch 100400  training loss: 11.525552749633789

 34%|██████████████████████████▍                                                    | 100624/300000 [27:48<53:34, 62.03it/s]
epoch 100500  training loss: 11.695064544677734
epoch 100500  clean testing loss: 55.46120834350586
epoch 100600  training loss: 12.571803092956543

 34%|██████████████████████████▌                                                    | 100750/300000 [27:50<53:15, 62.36it/s]
epoch 100700  training loss: 11.478582382202148

 34%|██████████████████████████▌                                                    | 100883/300000 [27:52<49:28, 67.07it/s]
epoch 100800  training loss: 11.53292465209961

 34%|██████████████████████████▌                                                    | 100995/300000 [27:54<50:27, 65.73it/s]
epoch 100900  training loss: 10.922632217407227
epoch 100900  clean testing loss: 55.537174224853516
epoch 101000  training loss: 11.258302688598633
epoch 101000  clean testing loss: 55.59577941894531

 34%|██████████████████████████▌                                                    | 101107/300000 [28:01<54:28, 60.85it/s]
epoch 101100  training loss: 11.63890266418457

 34%|██████████████████████████▋                                                    | 101233/300000 [28:03<51:09, 64.76it/s]
epoch 101200  training loss: 11.671686172485352

 34%|██████████████████████████▋                                                    | 101366/300000 [28:05<50:12, 65.95it/s]
epoch 101300  training loss: 11.351046562194824

 34%|██████████████████████████▋                                                    | 101492/300000 [28:07<54:09, 61.09it/s]
epoch 101400  training loss: 10.331976890563965

 34%|██████████████████████████▊                                                    | 101626/300000 [28:09<49:20, 67.01it/s]
epoch 101500  training loss: 11.12343692779541
epoch 101500  clean testing loss: 55.771766662597656
epoch 101600  training loss: 11.43826961517334

 34%|██████████████████████████▊                                                    | 101759/300000 [28:11<49:35, 66.62it/s]
epoch 101700  training loss: 11.364665031433105

 34%|██████████████████████████▊                                                    | 101892/300000 [28:13<49:09, 67.17it/s]
epoch 101800  training loss: 11.0023832321167

 34%|██████████████████████████▊                                                    | 102025/300000 [28:15<50:28, 65.38it/s]
epoch 101900  training loss: 11.191184043884277
epoch 101900  clean testing loss: 55.78099822998047
epoch 102000  training loss: 11.581559181213379
epoch 102000  clean testing loss: 55.726165771484375

 34%|██████████████████████████▉                                                    | 102151/300000 [28:17<49:50, 66.17it/s]
epoch 102100  training loss: 11.383225440979004

 34%|██████████████████████████▉                                                    | 102284/300000 [28:19<50:39, 65.05it/s]
epoch 102200  training loss: 10.680534362792969

 34%|██████████████████████████▉                                                    | 102417/300000 [28:21<49:54, 65.99it/s]
epoch 102300  training loss: 10.727204322814941
epoch 102300  clean testing loss: 55.90170669555664
epoch 102400  training loss: 10.259238243103027

 34%|███████████████████████████                                                    | 102550/300000 [28:23<49:29, 66.48it/s]
epoch 102500  training loss: 11.670893669128418
epoch 102500  clean testing loss: 55.96112060546875
epoch 102600  training loss: 11.676301002502441
epoch 102600  clean testing loss: 55.99421310424805
epoch 102700  training loss: 11.45278263092041

 34%|███████████████████████████                                                    | 102718/300000 [28:25<49:44, 66.11it/s]
epoch 102800  training loss: 11.073396682739258


 34%|███████████████████████████                                                    | 102984/300000 [28:29<50:01, 65.64it/s]
epoch 102900  training loss: 10.837865829467773

 34%|███████████████████████████▏                                                   | 103118/300000 [28:33<49:55, 65.72it/s]
epoch 103000  training loss: 12.086674690246582
epoch 103000  clean testing loss: 55.96146774291992
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise5.00e+01_invop1 ...
epoch 103100  training loss: 11.182201385498047

 34%|███████████████████████████▏                                                   | 103251/300000 [28:35<49:08, 66.72it/s]
epoch 103200  training loss: 11.751911163330078

 34%|███████████████████████████▏                                                   | 103384/300000 [28:37<49:36, 66.05it/s]
epoch 103300  training loss: 11.41114330291748

 35%|███████████████████████████▎                                                   | 103517/300000 [28:39<50:01, 65.45it/s]
epoch 103400  training loss: 11.32895565032959
epoch 103400  clean testing loss: 55.62238693237305
epoch 103500  training loss: 11.736607551574707

 35%|███████████████████████████▎                                                   | 103650/300000 [28:41<49:18, 66.37it/s]
epoch 103600  training loss: 11.050406455993652

 35%|███████████████████████████▎                                                   | 103783/300000 [28:43<49:11, 66.49it/s]
epoch 103700  training loss: 11.472152709960938

 35%|███████████████████████████▎                                                   | 103916/300000 [28:45<49:38, 65.83it/s]
epoch 103800  training loss: 11.076423645019531
epoch 103800  clean testing loss: 56.03070831298828
epoch 103900  training loss: 11.805912971496582

 35%|███████████████████████████▍                                                   | 104000/300000 [28:46<49:49, 65.57it/s]
epoch 104000  training loss: 11.156729698181152
epoch 104000  clean testing loss: 55.882057189941406


 35%|███████████████████████████▍                                                   | 104210/300000 [28:57<49:15, 66.26it/s]
epoch 104100  training loss: 10.525613784790039
epoch 104100  clean testing loss: 55.39198684692383
epoch 104200  training loss: 11.062602043151855

 35%|███████████████████████████▍                                                   | 104343/300000 [28:59<49:14, 66.22it/s]
epoch 104300  training loss: 11.531478881835938

 35%|███████████████████████████▌                                                   | 104476/300000 [29:01<49:13, 66.19it/s]
epoch 104400  training loss: 11.8329439163208

 35%|███████████████████████████▌                                                   | 104609/300000 [29:03<48:41, 66.87it/s]
epoch 104500  training loss: 11.57752513885498
epoch 104500  clean testing loss: 55.56654739379883
epoch 104600  training loss: 10.961960792541504

 35%|███████████████████████████▌                                                   | 104742/300000 [29:05<49:27, 65.80it/s]
epoch 104700  training loss: 11.622397422790527

 35%|███████████████████████████▌                                                   | 104875/300000 [29:07<48:33, 66.98it/s]
epoch 104800  training loss: 11.508888244628906

 35%|███████████████████████████▋                                                   | 105008/300000 [29:09<49:37, 65.50it/s]
epoch 104900  training loss: 11.502180099487305

 35%|███████████████████████████▋                                                   | 105160/300000 [29:11<42:49, 75.81it/s]
epoch 105000  training loss: 11.607030868530273
epoch 105000  clean testing loss: 55.63459396362305
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise5.00e+01_invop1 ...
epoch 105100  training loss: 11.640045166015625

 35%|███████████████████████████▋                                                   | 105312/300000 [29:13<42:41, 76.00it/s]
epoch 105200  training loss: 11.77668571472168

 35%|███████████████████████████▊                                                   | 105442/300000 [29:15<49:25, 65.61it/s]
epoch 105300  training loss: 12.780766487121582
epoch 105300  clean testing loss: 55.479087829589844
epoch 105400  training loss: 11.474925994873047

 35%|███████████████████████████▊                                                   | 105575/300000 [29:17<49:08, 65.93it/s]
epoch 105500  training loss: 10.986259460449219

 35%|███████████████████████████▊                                                   | 105708/300000 [29:19<48:40, 66.54it/s]
epoch 105600  training loss: 11.249210357666016

 35%|███████████████████████████▊                                                   | 105841/300000 [29:21<48:41, 66.47it/s]
epoch 105700  training loss: 12.198249816894531
epoch 105700  clean testing loss: 55.664649963378906
epoch 105800  training loss: 12.08078384399414

 35%|███████████████████████████▉                                                   | 105974/300000 [29:23<48:28, 66.71it/s]
epoch 105900  training loss: 11.61052417755127

 35%|███████████████████████████▉                                                   | 106051/300000 [29:25<59:34, 54.25it/s]
epoch 106000  training loss: 11.1080961227417
epoch 106000  clean testing loss: 55.37392044067383

 35%|███████████████████████████▉                                                   | 106184/300000 [29:27<48:43, 66.30it/s]
epoch 106100  training loss: 11.593473434448242

 35%|███████████████████████████▉                                                   | 106198/300000 [29:27<49:13, 65.63it/s]
epoch 106200  training loss: 10.84792709350586
epoch 106200  clean testing loss: 55.32783889770508
epoch 106300  training loss: 11.722129821777344
epoch 106300  clean testing loss: 55.28816604614258
epoch 106400  training loss: 12.120613098144531
epoch 106400  clean testing loss: 55.36971664428711
epoch 106500  training loss: 12.014691352844238
epoch 106500  clean testing loss: 55.587493896484375
epoch 106600  training loss: 11.40750789642334
epoch 106600  clean testing loss: 55.24659729003906
epoch 106700  training loss: 12.380989074707031
epoch 106700  clean testing loss: 55.46627426147461
epoch 106800  training loss: 12.29868221282959

 36%|██████████████████████████▎                                               | 106862/300000 [46:50<1206:21:54, 22.49s/it]
epoch 106900  training loss: 12.492236137390137

 36%|███████████████████████████▍                                                 | 106996/300000 [46:52<2:08:50, 24.97it/s]
epoch 107000  training loss: 11.6622896194458
epoch 107000  clean testing loss: 55.58396911621094
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise5.00e+01_invop1 ...
epoch 107100  training loss: 11.03537368774414

 36%|████████████████████████████▏                                                  | 107129/300000 [46:54<48:05, 66.85it/s]
epoch 107200  training loss: 11.214906692504883

 36%|████████████████████████████▏                                                  | 107262/300000 [46:56<48:44, 65.90it/s]
epoch 107300  training loss: 11.44971752166748

 36%|████████████████████████████▎                                                  | 107397/300000 [46:58<46:57, 68.35it/s]
epoch 107400  training loss: 10.908496856689453
epoch 107400  clean testing loss: 55.52128982543945
epoch 107500  training loss: 11.499357223510742

 36%|████████████████████████████▎                                                  | 107523/300000 [47:00<49:18, 65.06it/s]
epoch 107600  training loss: 11.107759475708008

 36%|████████████████████████████▎                                                  | 107656/300000 [47:02<50:42, 63.21it/s]
epoch 107700  training loss: 11.736808776855469

 36%|████████████████████████████▍                                                  | 107789/300000 [47:04<48:47, 65.66it/s]
epoch 107800  training loss: 11.421289443969727
epoch 107800  clean testing loss: 55.661930084228516
epoch 107900  training loss: 12.376562118530273

 36%|████████████████████████████▍                                                  | 107922/300000 [47:06<48:34, 65.90it/s]
epoch 108000  training loss: 10.80751895904541
epoch 108000  clean testing loss: 55.41584014892578

 36%|████████████████████████████▍                                                  | 107999/300000 [47:07<48:23, 66.13it/s]
epoch 108100  training loss: 11.452444076538086


 36%|████████████████████████████▌                                                  | 108259/300000 [47:14<48:44, 65.55it/s]
epoch 108200  training loss: 11.255943298339844
epoch 108200  clean testing loss: 55.390193939208984
epoch 108300  training loss: 11.445056915283203


 36%|████████████████████████████▌                                                  | 108518/300000 [47:18<48:26, 65.87it/s]
epoch 108400  training loss: 11.945535659790039
epoch 108400  clean testing loss: 55.19204330444336
epoch 108500  training loss: 11.994898796081543
epoch 108500  clean testing loss: 55.27822494506836
epoch 108600  training loss: 11.46477222442627

 36%|████████████████████████████▌                                                  | 108651/300000 [47:20<47:29, 67.16it/s]
epoch 108700  training loss: 11.21710205078125

 36%|████████████████████████████▋                                                  | 108784/300000 [47:22<48:04, 66.30it/s]
epoch 108800  training loss: 11.314342498779297
epoch 108800  clean testing loss: 55.361507415771484
epoch 108900  training loss: 11.35654354095459

 36%|████████████████████████████▋                                                  | 108917/300000 [47:24<48:26, 65.75it/s]
epoch 109000  training loss: 10.765666961669922
epoch 109000  clean testing loss: 55.42601776123047

 36%|████████████████████████████▋                                                  | 109050/300000 [47:26<48:46, 65.25it/s]
epoch 109100  training loss: 11.013028144836426


 36%|████████████████████████████▊                                                  | 109316/300000 [47:30<47:22, 67.08it/s]
epoch 109200  training loss: 11.703824996948242
epoch 109200  clean testing loss: 55.49321365356445
epoch 109300  training loss: 11.393311500549316

 36%|████████████████████████████▊                                                  | 109449/300000 [47:32<47:31, 66.82it/s]
epoch 109400  training loss: 11.540216445922852

 37%|████████████████████████████▊                                                  | 109575/300000 [47:34<48:27, 65.50it/s]
epoch 109500  training loss: 11.421473503112793

 37%|████████████████████████████▉                                                  | 109708/300000 [47:36<48:01, 66.03it/s]
epoch 109600  training loss: 11.353259086608887
epoch 109600  clean testing loss: 55.6496467590332
epoch 109700  training loss: 11.459023475646973
epoch 109700  clean testing loss: 55.6113166809082
epoch 109800  training loss: 11.898295402526855


 37%|████████████████████████████▉                                                  | 110108/300000 [47:42<47:48, 66.20it/s]
epoch 109900  training loss: 11.331504821777344
epoch 109900  clean testing loss: 55.18388748168945
epoch 110000  training loss: 11.338611602783203
epoch 110000  clean testing loss: 55.2050666809082

 37%|█████████████████████████████                                                  | 110241/300000 [47:44<47:48, 66.16it/s]
epoch 110100  training loss: 11.622281074523926
epoch 110100  clean testing loss: 55.41581344604492
epoch 110200  training loss: 12.574797630310059

 37%|█████████████████████████████                                                  | 110374/300000 [47:46<47:24, 66.66it/s]
epoch 110300  training loss: 11.577451705932617


 37%|█████████████████████████████▏                                                 | 110738/300000 [47:52<47:27, 66.46it/s]
epoch 110400  training loss: 10.674242973327637
epoch 110400  clean testing loss: 55.35249328613281
epoch 110500  training loss: 11.879416465759277
epoch 110500  clean testing loss: 55.58177947998047
epoch 110600  training loss: 11.54544734954834
epoch 110600  clean testing loss: 55.360782623291016
epoch 110700  training loss: 11.831629753112793

 37%|█████████████████████████████▏                                                 | 110871/300000 [47:54<47:21, 66.56it/s]
epoch 110800  training loss: 11.309748649597168

 37%|█████████████████████████████▏                                                 | 110997/300000 [47:56<47:55, 65.73it/s]
epoch 110900  training loss: 11.477180480957031
epoch 110900  clean testing loss: 55.40034866333008
epoch 111000  training loss: 11.612199783325195
epoch 111000  clean testing loss: 55.210533142089844


 37%|█████████████████████████████▎                                                 | 111165/300000 [48:06<47:33, 66.17it/s]
epoch 111100  training loss: 11.410112380981445

 37%|█████████████████████████████▎                                                 | 111298/300000 [48:08<47:07, 66.73it/s]
epoch 111200  training loss: 11.555022239685059
epoch 111200  clean testing loss: 55.37975311279297
epoch 111300  training loss: 11.33755111694336

 37%|█████████████████████████████▎                                                 | 111431/300000 [48:10<47:44, 65.83it/s]
epoch 111400  training loss: 12.092625617980957

 37%|█████████████████████████████▍                                                 | 111564/300000 [48:12<48:48, 64.35it/s]
epoch 111500  training loss: 11.418270111083984

 37%|█████████████████████████████▍                                                 | 111697/300000 [48:14<47:47, 65.66it/s]
epoch 111600  training loss: 11.725217819213867
epoch 111600  clean testing loss: 55.314998626708984
epoch 111700  training loss: 11.047079086303711

 37%|█████████████████████████████▍                                                 | 111823/300000 [48:16<47:23, 66.17it/s]
epoch 111800  training loss: 12.213932037353516

 37%|█████████████████████████████▍                                                 | 111956/300000 [48:18<47:27, 66.04it/s]
epoch 111900  training loss: 11.153658866882324

 37%|█████████████████████████████▌                                                 | 112082/300000 [48:20<47:38, 65.73it/s]
epoch 112000  training loss: 11.237578392028809
epoch 112000  clean testing loss: 55.252071380615234

 37%|█████████████████████████████▌                                                 | 112215/300000 [48:22<47:46, 65.52it/s]
epoch 112100  training loss: 11.866822242736816
epoch 112100  clean testing loss: 55.311798095703125
epoch 112200  training loss: 11.561098098754883

 37%|█████████████████████████████▌                                                 | 112348/300000 [48:24<48:04, 65.04it/s]
epoch 112300  training loss: 11.953113555908203

 37%|█████████████████████████████▌                                                 | 112481/300000 [48:26<47:30, 65.79it/s]
epoch 112400  training loss: 11.525908470153809

 38%|█████████████████████████████▋                                                 | 112614/300000 [48:28<47:03, 66.37it/s]
epoch 112500  training loss: 11.35550594329834
epoch 112500  clean testing loss: 55.40729904174805
epoch 112600  training loss: 11.350293159484863

 38%|█████████████████████████████▋                                                 | 112747/300000 [48:30<47:16, 66.01it/s]
epoch 112700  training loss: 11.758151054382324

 38%|█████████████████████████████▋                                                 | 112880/300000 [48:32<47:11, 66.09it/s]
epoch 112800  training loss: 11.431000709533691

 38%|█████████████████████████████▊                                                 | 112999/300000 [48:34<46:49, 66.55it/s]
epoch 112900  training loss: 11.431979179382324
epoch 112900  clean testing loss: 55.53666305541992
epoch 113000  training loss: 12.036252975463867
epoch 113000  clean testing loss: 55.30086135864258

 38%|█████████████████████████████▊                                                 | 113118/300000 [48:37<48:04, 64.79it/s]
epoch 113100  training loss: 11.714533805847168

 38%|█████████████████████████████▊                                                 | 113251/300000 [48:39<46:52, 66.40it/s]
epoch 113200  training loss: 11.016641616821289

 38%|█████████████████████████████▊                                                 | 113384/300000 [48:41<46:52, 66.36it/s]
epoch 113300  training loss: 11.068204879760742
epoch 113300  clean testing loss: 55.47275161743164
epoch 113400  training loss: 10.88884162902832
epoch 113400  clean testing loss: 55.40995407104492
epoch 113500  training loss: 12.153853416442871
epoch 113500  clean testing loss: 55.394840240478516
epoch 113600  training loss: 12.190291404724121
epoch 113600  clean testing loss: 55.50505065917969
epoch 113700  training loss: 11.61367130279541
epoch 113700  clean testing loss: 55.55293273925781
epoch 113800  training loss: 12.054572105407715
epoch 113800  clean testing loss: 55.494625091552734
epoch 113900  training loss: 11.60141658782959

 38%|█████████████████████████████▉                                                 | 113916/300000 [48:49<47:24, 65.41it/s]
epoch 114000  training loss: 11.360875129699707
epoch 114000  clean testing loss: 55.337284088134766

 38%|██████████████████████████████                                                 | 114049/300000 [48:51<46:52, 66.12it/s]
epoch 114100  training loss: 11.182450294494629

 38%|██████████████████████████████                                                 | 114182/300000 [48:53<46:32, 66.53it/s]
epoch 114200  training loss: 11.158379554748535
epoch 114200  clean testing loss: 55.697227478027344
epoch 114300  training loss: 11.546894073486328

 38%|██████████████████████████████                                                 | 114315/300000 [48:55<46:31, 66.51it/s]
epoch 114400  training loss: 11.402915954589844

 38%|██████████████████████████████▏                                                | 114448/300000 [48:57<46:28, 66.54it/s]
epoch 114500  training loss: 11.420882225036621

 38%|██████████████████████████████▏                                                | 114581/300000 [48:59<46:26, 66.55it/s]
epoch 114600  training loss: 11.529800415039062
epoch 114600  clean testing loss: 55.38702392578125
epoch 114700  training loss: 11.691378593444824

 38%|██████████████████████████████▏                                                | 114714/300000 [49:02<46:49, 65.96it/s]
epoch 114800  training loss: 12.015103340148926

 38%|██████████████████████████████▏                                                | 114847/300000 [49:04<46:53, 65.82it/s]
epoch 114900  training loss: 11.624141693115234

 38%|██████████████████████████████▎                                                | 114980/300000 [49:06<47:28, 64.96it/s]
epoch 115000  training loss: 11.452377319335938
epoch 115000  clean testing loss: 55.69744873046875
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise5.00e+01_invop1 ...
epoch 115100  training loss: 11.270694732666016

 38%|██████████████████████████████▎                                                | 115106/300000 [49:07<46:40, 66.02it/s]
epoch 115200  training loss: 11.601832389831543

 38%|██████████████████████████████▎                                                | 115239/300000 [49:09<47:23, 64.97it/s]
epoch 115300  training loss: 11.013811111450195

 38%|██████████████████████████████▍                                                | 115372/300000 [49:11<46:02, 66.84it/s]
epoch 115400  training loss: 11.787229537963867
epoch 115400  clean testing loss: 55.43115997314453
epoch 115500  training loss: 11.514490127563477

 39%|██████████████████████████████▍                                                | 115505/300000 [49:13<46:45, 65.77it/s]
epoch 115600  training loss: 11.452047348022461

 39%|██████████████████████████████▍                                                | 115638/300000 [49:16<45:49, 67.05it/s]
epoch 115700  training loss: 12.01055908203125

 39%|██████████████████████████████▍                                                | 115771/300000 [49:18<46:38, 65.82it/s]
epoch 115800  training loss: 11.706018447875977
epoch 115800  clean testing loss: 55.56428527832031
epoch 115900  training loss: 11.447102546691895

 39%|██████████████████████████████▌                                                | 115904/300000 [49:20<46:41, 65.70it/s]
epoch 116000  training loss: 11.837064743041992
epoch 116000  clean testing loss: 55.4642219543457

 39%|██████████████████████████████▌                                                | 116037/300000 [49:22<47:01, 65.20it/s]
epoch 116100  training loss: 11.72750473022461

 39%|██████████████████████████████▌                                                | 116170/300000 [49:24<46:13, 66.28it/s]
epoch 116200  training loss: 11.449467658996582

 39%|██████████████████████████████▋                                                | 116303/300000 [49:26<46:20, 66.06it/s]
epoch 116300  training loss: 12.213289260864258
epoch 116300  clean testing loss: 55.57696533203125
epoch 116400  training loss: 12.03874397277832

 39%|██████████████████████████████▋                                                | 116436/300000 [49:28<46:10, 66.25it/s]
epoch 116500  training loss: 11.74492073059082

 39%|██████████████████████████████▋                                                | 116569/300000 [49:30<45:57, 66.52it/s]
epoch 116600  training loss: 12.224813461303711

 39%|██████████████████████████████▋                                                | 116702/300000 [49:32<45:34, 67.03it/s]
epoch 116700  training loss: 11.777945518493652
epoch 116700  clean testing loss: 55.54838562011719
epoch 116800  training loss: 11.692479133605957

 39%|██████████████████████████████▊                                                | 116835/300000 [49:34<46:11, 66.09it/s]
epoch 116900  training loss: 11.677088737487793

 39%|██████████████████████████████▊                                                | 116968/300000 [49:36<45:35, 66.90it/s]
epoch 117000  training loss: 11.689413070678711
epoch 117000  clean testing loss: 55.5647087097168

 39%|██████████████████████████████▊                                                | 116996/300000 [49:36<46:21, 65.80it/s]
epoch 117100  training loss: 11.893389701843262

 39%|██████████████████████████████▊                                                | 117101/300000 [49:43<50:34, 60.27it/s]
epoch 117200  training loss: 11.560598373413086

 39%|██████████████████████████████▊                                                | 117234/300000 [49:45<45:45, 66.58it/s]
epoch 117300  training loss: 11.531892776489258

 39%|██████████████████████████████▉                                                | 117367/300000 [49:47<46:28, 65.49it/s]
epoch 117400  training loss: 11.564787864685059
epoch 117400  clean testing loss: 55.846275329589844
epoch 117500  training loss: 11.91691780090332

 39%|██████████████████████████████▉                                                | 117500/300000 [49:49<46:38, 65.22it/s]
epoch 117600  training loss: 11.980413436889648

 39%|██████████████████████████████▉                                                | 117633/300000 [49:51<45:50, 66.31it/s]
epoch 117700  training loss: 12.112990379333496

 39%|███████████████████████████████                                                | 117766/300000 [49:53<45:45, 66.37it/s]
epoch 117800  training loss: 11.346421241760254
epoch 117800  clean testing loss: 55.732818603515625
epoch 117900  training loss: 11.864900588989258

 39%|███████████████████████████████                                                | 117892/300000 [49:55<46:14, 65.63it/s]
epoch 118000  training loss: 11.843470573425293
epoch 118000  clean testing loss: 55.79011535644531
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise5.00e+01_invop1 ...
epoch 118100  training loss: 11.741311073303223

 39%|███████████████████████████████                                                | 118116/300000 [50:05<48:40, 62.27it/s]
epoch 118200  training loss: 11.801337242126465

 39%|███████████████████████████████▏                                               | 118249/300000 [50:07<46:05, 65.73it/s]
epoch 118300  training loss: 12.002362251281738

 39%|███████████████████████████████▏                                               | 118382/300000 [50:09<45:21, 66.74it/s]
epoch 118400  training loss: 11.937350273132324
epoch 118400  clean testing loss: 55.70066833496094
epoch 118500  training loss: 11.717844009399414

 40%|███████████████████████████████▏                                               | 118515/300000 [50:11<45:36, 66.33it/s]
epoch 118600  training loss: 12.437488555908203

 40%|███████████████████████████████▏                                               | 118648/300000 [50:13<45:42, 66.13it/s]
epoch 118700  training loss: 12.374608039855957

 40%|███████████████████████████████▎                                               | 118781/300000 [50:15<45:36, 66.23it/s]
epoch 118800  training loss: 11.216261863708496
epoch 118800  clean testing loss: 55.841644287109375
epoch 118900  training loss: 11.762258529663086

 40%|███████████████████████████████▎                                               | 118914/300000 [50:17<45:18, 66.62it/s]
epoch 119000  training loss: 12.019968032836914
epoch 119000  clean testing loss: 55.728633880615234

 40%|███████████████████████████████▎                                               | 119110/300000 [50:29<50:39, 59.52it/s]
epoch 119100  training loss: 12.040462493896484

 40%|███████████████████████████████▍                                               | 119243/300000 [50:31<45:18, 66.48it/s]
epoch 119200  training loss: 11.222957611083984
epoch 119200  clean testing loss: 55.817134857177734
epoch 119300  training loss: 11.60492992401123

 40%|███████████████████████████████▍                                               | 119376/300000 [50:33<45:47, 65.75it/s]
epoch 119400  training loss: 11.806604385375977
epoch 119400  clean testing loss: 55.74406051635742
epoch 119500  training loss: 11.636039733886719


 40%|███████████████████████████████▌                                               | 119712/300000 [50:38<45:22, 66.23it/s]
epoch 119600  training loss: 11.960192680358887
epoch 119600  clean testing loss: 55.63572311401367
epoch 119700  training loss: 12.146470069885254

 40%|███████████████████████████████▌                                               | 119845/300000 [50:40<44:49, 66.98it/s]
epoch 119800  training loss: 12.156771659851074

 40%|███████████████████████████████▌                                               | 119978/300000 [50:42<45:27, 66.01it/s]
epoch 119900  training loss: 12.294095993041992

 40%|███████████████████████████████▋                                               | 120111/300000 [50:44<45:21, 66.10it/s]
epoch 120000  training loss: 11.534197807312012
epoch 120000  clean testing loss: 55.56696319580078
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise5.00e+01_invop1 ...
epoch 120100  training loss: 11.474685668945312

 40%|███████████████████████████████▋                                               | 120244/300000 [50:46<45:11, 66.28it/s]
epoch 120200  training loss: 11.334973335266113

 40%|███████████████████████████████▋                                               | 120377/300000 [50:48<44:55, 66.64it/s]
epoch 120300  training loss: 11.843958854675293

 40%|███████████████████████████████▋                                               | 120503/300000 [50:50<45:11, 66.20it/s]
epoch 120400  training loss: 11.420722007751465
epoch 120400  clean testing loss: 55.664154052734375
epoch 120500  training loss: 11.885071754455566

 40%|███████████████████████████████▊                                               | 120643/300000 [50:52<44:43, 66.84it/s]
epoch 120600  training loss: 12.13979721069336

 40%|███████████████████████████████▊                                               | 120776/300000 [50:54<45:18, 65.92it/s]
epoch 120700  training loss: 11.930330276489258

 40%|███████████████████████████████▊                                               | 120902/300000 [50:56<45:36, 65.46it/s]
epoch 120800  training loss: 11.957941055297852
epoch 120800  clean testing loss: 55.68861770629883
epoch 120900  training loss: 11.992037773132324

 40%|███████████████████████████████▊                                               | 121035/300000 [50:58<45:02, 66.21it/s]
epoch 121000  training loss: 12.256183624267578
epoch 121000  clean testing loss: 55.826316833496094

 40%|███████████████████████████████▉                                               | 121168/300000 [51:00<44:59, 66.24it/s]
epoch 121100  training loss: 11.429744720458984

 40%|███████████████████████████████▉                                               | 121308/300000 [51:02<44:44, 66.56it/s]
epoch 121200  training loss: 12.037945747375488
epoch 121200  clean testing loss: 55.72922134399414
epoch 121300  training loss: 11.838040351867676

 41%|████████████████████████████████                                               | 121637/300000 [51:07<45:35, 65.20it/s]
epoch 121400  training loss: 11.935225486755371
epoch 121400  clean testing loss: 55.72745132446289
epoch 121500  training loss: 12.02197551727295
epoch 121500  clean testing loss: 55.82389831542969
epoch 121600  training loss: 12.277076721191406

 41%|████████████████████████████████                                               | 121770/300000 [51:09<45:04, 65.89it/s]
epoch 121700  training loss: 12.062541961669922

 41%|████████████████████████████████                                               | 121868/300000 [51:10<44:25, 66.83it/s]
epoch 121800  training loss: 11.63115119934082

 41%|████████████████████████████████▏                                              | 122064/300000 [51:13<44:48, 66.20it/s]
epoch 121900  training loss: 11.90877628326416
epoch 121900  clean testing loss: 55.710025787353516
epoch 122000  training loss: 11.889988899230957
epoch 122000  clean testing loss: 55.36948013305664

 41%|████████████████████████████████▏                                              | 122197/300000 [51:15<44:27, 66.66it/s]
epoch 122100  training loss: 12.372764587402344
epoch 122100  clean testing loss: 55.72011947631836
epoch 122200  training loss: 12.321756362915039

 41%|████████████████████████████████▏                                              | 122330/300000 [51:17<44:52, 65.98it/s]
epoch 122300  training loss: 11.454729080200195

 41%|████████████████████████████████▏                                              | 122463/300000 [51:19<44:21, 66.72it/s]
epoch 122400  training loss: 11.272527694702148

 41%|████████████████████████████████▎                                              | 122596/300000 [51:21<44:48, 65.99it/s]
epoch 122500  training loss: 12.046282768249512
epoch 122500  clean testing loss: 55.72317886352539
epoch 122600  training loss: 12.02214241027832

 41%|████████████████████████████████▎                                              | 122729/300000 [51:23<44:37, 66.22it/s]
epoch 122700  training loss: 12.139444351196289

 41%|████████████████████████████████▎                                              | 122862/300000 [51:25<44:52, 65.78it/s]
epoch 122800  training loss: 11.43278980255127

 41%|████████████████████████████████▍                                              | 123093/300000 [51:35<52:10, 56.52it/s]
epoch 122900  training loss: 11.606858253479004
epoch 122900  clean testing loss: 55.6551399230957
epoch 123000  training loss: 12.166279792785645
epoch 123000  clean testing loss: 55.77729797363281
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise5.00e+01_invop1 ...
epoch 123100  training loss: 11.48999309539795

 41%|████████████████████████████████▍                                              | 123226/300000 [51:37<44:42, 65.89it/s]
epoch 123200  training loss: 11.822248458862305

 41%|████████████████████████████████▍                                              | 123359/300000 [51:39<44:28, 66.20it/s]
epoch 123300  training loss: 12.283758163452148

 41%|████████████████████████████████▌                                              | 123492/300000 [51:41<44:16, 66.44it/s]
epoch 123400  training loss: 11.967979431152344
epoch 123400  clean testing loss: 55.651123046875
epoch 123500  training loss: 11.92116641998291

 41%|████████████████████████████████▌                                              | 123625/300000 [51:43<44:34, 65.94it/s]
epoch 123600  training loss: 13.08305549621582

 41%|████████████████████████████████▌                                              | 123758/300000 [51:45<44:14, 66.40it/s]
epoch 123700  training loss: 12.279104232788086

 41%|████████████████████████████████▌                                              | 123891/300000 [51:47<45:14, 64.87it/s]
epoch 123800  training loss: 12.201518058776855

 41%|████████████████████████████████▋                                              | 124024/300000 [51:49<44:22, 66.09it/s]
epoch 123900  training loss: 11.758986473083496
epoch 123900  clean testing loss: 55.51483154296875
epoch 124000  training loss: 12.997147560119629
epoch 124000  clean testing loss: 55.81343078613281

 41%|████████████████████████████████▋                                              | 124150/300000 [51:51<44:16, 66.19it/s]
epoch 124100  training loss: 12.657486915588379

 41%|████████████████████████████████▋                                              | 124283/300000 [51:53<44:13, 66.22it/s]
epoch 124200  training loss: 11.487506866455078

 42%|████████████████████████████████▊                                              | 124682/300000 [51:59<44:14, 66.06it/s]
epoch 124300  training loss: 11.256287574768066
epoch 124300  clean testing loss: 55.557430267333984
epoch 124400  training loss: 11.971223831176758
epoch 124400  clean testing loss: 55.70344161987305
epoch 124500  training loss: 12.013758659362793
epoch 124500  clean testing loss: 55.52046203613281
epoch 124600  training loss: 11.70177173614502

 42%|████████████████████████████████▊                                              | 124815/300000 [52:01<43:33, 67.04it/s]
epoch 124700  training loss: 12.250893592834473
epoch 124700  clean testing loss: 55.651859283447266
epoch 124800  training loss: 12.105237007141113

 42%|████████████████████████████████▉                                              | 124948/300000 [52:03<43:50, 66.56it/s]
epoch 124900  training loss: 11.702125549316406

 42%|████████████████████████████████▉                                              | 125081/300000 [52:05<43:53, 66.41it/s]
epoch 125000  training loss: 11.92866325378418
epoch 125000  clean testing loss: 55.707149505615234

 42%|████████████████████████████████▉                                              | 125214/300000 [52:07<44:11, 65.92it/s]
epoch 125100  training loss: 11.895628929138184
epoch 125100  clean testing loss: 55.606143951416016
epoch 125200  training loss: 12.079582214355469

 42%|█████████████████████████████████                                              | 125347/300000 [52:09<44:12, 65.86it/s]
epoch 125300  training loss: 12.318589210510254

 42%|█████████████████████████████████                                              | 125480/300000 [52:11<43:34, 66.76it/s]
epoch 125400  training loss: 11.436500549316406

 42%|█████████████████████████████████                                              | 125614/300000 [52:13<43:35, 66.66it/s]
epoch 125500  training loss: 11.779960632324219
epoch 125500  clean testing loss: 55.528900146484375
epoch 125600  training loss: 12.326508522033691

 42%|█████████████████████████████████▏                                             | 126013/300000 [52:19<43:38, 66.44it/s]
epoch 125700  training loss: 11.970582962036133
epoch 125700  clean testing loss: 55.473777770996094
epoch 125800  training loss: 12.041513442993164
epoch 125800  clean testing loss: 55.49778747558594
epoch 125900  training loss: 12.000289916992188
epoch 125900  clean testing loss: 55.453800201416016
epoch 126000  training loss: 11.733067512512207
epoch 126000  clean testing loss: 55.556800842285156
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise5.00e+01_invop1 ...
epoch 126100  training loss: 11.772991180419922

 42%|█████████████████████████████████▏                                             | 126163/300000 [52:21<42:00, 68.97it/s]
epoch 126200  training loss: 11.8178071975708


 42%|█████████████████████████████████▎                                             | 126422/300000 [52:25<43:45, 66.12it/s]
epoch 126300  training loss: 12.45266342163086
epoch 126300  clean testing loss: 55.61071014404297
epoch 126400  training loss: 12.170122146606445

 42%|█████████████████████████████████▎                                             | 126555/300000 [52:27<43:11, 66.94it/s]
epoch 126500  training loss: 12.2253999710083

 42%|█████████████████████████████████▎                                             | 126688/300000 [52:29<43:41, 66.10it/s]
epoch 126600  training loss: 11.92652416229248

 42%|█████████████████████████████████▍                                             | 126821/300000 [52:31<43:30, 66.34it/s]
epoch 126700  training loss: 12.740668296813965
epoch 126700  clean testing loss: 55.603519439697266
epoch 126800  training loss: 12.714405059814453

 42%|█████████████████████████████████▍                                             | 126954/300000 [52:33<43:28, 66.34it/s]
epoch 126900  training loss: 12.2894926071167

 42%|█████████████████████████████████▍                                             | 127108/300000 [52:40<46:03, 62.56it/s]
epoch 127000  training loss: 12.085091590881348
epoch 127000  clean testing loss: 55.53129196166992
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise5.00e+01_invop1 ...
epoch 127100  training loss: 12.197670936584473

 42%|█████████████████████████████████▌                                             | 127241/300000 [52:42<43:57, 65.49it/s]
epoch 127200  training loss: 12.001191139221191

 42%|█████████████████████████████████▌                                             | 127374/300000 [52:44<43:27, 66.19it/s]
epoch 127300  training loss: 12.195347785949707

 43%|█████████████████████████████████▌                                             | 127507/300000 [52:46<43:44, 65.73it/s]
epoch 127400  training loss: 11.80876636505127
epoch 127400  clean testing loss: 55.479488372802734
epoch 127500  training loss: 11.198437690734863

 43%|█████████████████████████████████▌                                             | 127640/300000 [52:48<43:06, 66.65it/s]
epoch 127600  training loss: 11.133232116699219

 43%|█████████████████████████████████▋                                             | 127773/300000 [52:50<43:21, 66.21it/s]
epoch 127700  training loss: 11.496670722961426

 43%|█████████████████████████████████▋                                             | 127906/300000 [52:52<44:05, 65.05it/s]
epoch 127800  training loss: 11.965017318725586
epoch 127800  clean testing loss: 55.552024841308594
epoch 127900  training loss: 12.137155532836914

 43%|█████████████████████████████████▋                                             | 128032/300000 [52:54<43:44, 65.52it/s]
epoch 128000  training loss: 11.87693977355957
epoch 128000  clean testing loss: 55.43821716308594

 43%|█████████████████████████████████▊                                             | 128165/300000 [52:56<43:24, 65.97it/s]
epoch 128100  training loss: 12.006160736083984

 43%|█████████████████████████████████▊                                             | 128298/300000 [52:58<43:14, 66.17it/s]
epoch 128200  training loss: 12.105344772338867
epoch 128200  clean testing loss: 55.52228927612305
epoch 128300  training loss: 12.116986274719238
epoch 128300  clean testing loss: 55.41805648803711
epoch 128400  training loss: 12.29240894317627
epoch 128400  clean testing loss: 55.54306411743164
epoch 128500  training loss: 11.912281036376953

 43%|█████████████████████████████████▊                                             | 128564/300000 [53:02<42:39, 66.98it/s]
epoch 128600  training loss: 12.391656875610352

 43%|█████████████████████████████████▉                                             | 128697/300000 [53:04<43:28, 65.68it/s]
epoch 128700  training loss: 12.287824630737305
epoch 128700  clean testing loss: 55.492191314697266
epoch 128800  training loss: 12.28069019317627

 43%|█████████████████████████████████▉                                             | 128830/300000 [53:06<43:06, 66.17it/s]
epoch 128900  training loss: 11.605175971984863

 43%|█████████████████████████████████▉                                             | 128963/300000 [53:08<43:03, 66.19it/s]
epoch 129000  training loss: 11.84135913848877
epoch 129000  clean testing loss: 55.47300338745117

 43%|█████████████████████████████████▉                                             | 129096/300000 [53:10<43:10, 65.98it/s]
epoch 129100  training loss: 11.744979858398438
epoch 129100  clean testing loss: 55.3572998046875
epoch 129200  training loss: 11.18655776977539

 43%|██████████████████████████████████                                             | 129229/300000 [53:12<43:05, 66.04it/s]
epoch 129300  training loss: 11.267473220825195

 43%|██████████████████████████████████                                             | 129362/300000 [53:14<42:43, 66.58it/s]
epoch 129400  training loss: 11.691908836364746

 43%|██████████████████████████████████                                             | 129495/300000 [53:16<43:00, 66.08it/s]
epoch 129500  training loss: 12.021787643432617
epoch 129500  clean testing loss: 55.48280715942383
epoch 129600  training loss: 11.794121742248535

 43%|██████████████████████████████████▏                                            | 129628/300000 [53:18<42:52, 66.24it/s]
epoch 129700  training loss: 12.206012725830078


 43%|██████████████████████████████████▎                                            | 130111/300000 [53:29<45:23, 62.37it/s]
epoch 129800  training loss: 11.733757972717285
epoch 129800  clean testing loss: 55.57231903076172
epoch 129900  training loss: 11.59760856628418
epoch 129900  clean testing loss: 55.42583465576172
epoch 130000  training loss: 11.335877418518066
epoch 130000  clean testing loss: 55.33358383178711
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise5.00e+01_invop1 ...
epoch 130100  training loss: 11.60584831237793
epoch 130100  clean testing loss: 55.52293395996094
epoch 130200  training loss: 11.57774829864502


 43%|██████████████████████████████████▎                                            | 130377/300000 [53:33<42:44, 66.15it/s]
epoch 130300  training loss: 11.489666938781738

 44%|██████████████████████████████████▎                                            | 130510/300000 [53:35<42:46, 66.04it/s]
epoch 130400  training loss: 11.336309432983398
epoch 130400  clean testing loss: 55.308738708496094
epoch 130500  training loss: 11.207085609436035

 44%|██████████████████████████████████▍                                            | 130643/300000 [53:37<42:53, 65.80it/s]
epoch 130600  training loss: 11.328289985656738

 44%|██████████████████████████████████▍                                            | 130776/300000 [53:39<42:37, 66.16it/s]
epoch 130700  training loss: 11.147356033325195

 44%|██████████████████████████████████▌                                            | 131042/300000 [53:44<42:33, 66.17it/s]
epoch 130800  training loss: 11.260977745056152
epoch 130800  clean testing loss: 55.24040985107422
epoch 130900  training loss: 11.524603843688965
epoch 130900  clean testing loss: 55.24201583862305
epoch 131000  training loss: 12.593170166015625
epoch 131000  clean testing loss: 55.46452713012695
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise5.00e+01_invop1 ...
epoch 131100  training loss: 11.585182189941406

 44%|██████████████████████████████████▌                                            | 131175/300000 [53:46<42:16, 66.57it/s]
epoch 131200  training loss: 11.621734619140625
epoch 131200  clean testing loss: 55.34441375732422
epoch 131300  training loss: 11.990429878234863

 44%|██████████████████████████████████▌                                            | 131308/300000 [53:48<42:23, 66.31it/s]
epoch 131400  training loss: 11.915058135986328

 44%|██████████████████████████████████▌                                            | 131441/300000 [53:50<42:26, 66.19it/s]
epoch 131500  training loss: 11.784867286682129

 44%|██████████████████████████████████▋                                            | 131574/300000 [53:52<42:19, 66.33it/s]
epoch 131600  training loss: 12.118807792663574
epoch 131600  clean testing loss: 55.22808074951172
epoch 131700  training loss: 12.313470840454102

 44%|██████████████████████████████████▋                                            | 131700/300000 [53:53<42:24, 66.13it/s]
epoch 131800  training loss: 11.262136459350586

 44%|██████████████████████████████████▋                                            | 131840/300000 [53:56<42:09, 66.49it/s]
epoch 131900  training loss: 12.1441011428833

 44%|██████████████████████████████████▊                                            | 131973/300000 [53:58<42:05, 66.53it/s]
epoch 132000  training loss: 12.317102432250977
epoch 132000  clean testing loss: 55.45685577392578
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise5.00e+01_invop1 ...
epoch 132100  training loss: 12.283548355102539

 44%|██████████████████████████████████▊                                            | 132099/300000 [53:59<42:48, 65.38it/s]
epoch 132200  training loss: 11.99404525756836


 44%|██████████████████████████████████▊                                            | 132365/300000 [54:03<41:54, 66.67it/s]
epoch 132300  training loss: 12.10913372039795

 44%|██████████████████████████████████▉                                            | 132498/300000 [54:05<42:27, 65.76it/s]
epoch 132400  training loss: 12.137717247009277
epoch 132400  clean testing loss: 55.29936599731445
epoch 132500  training loss: 12.046297073364258

 44%|██████████████████████████████████▉                                            | 132631/300000 [54:08<42:05, 66.26it/s]
epoch 132600  training loss: 12.103010177612305
epoch 132600  clean testing loss: 55.397830963134766
epoch 132700  training loss: 12.089881896972656
epoch 132700  clean testing loss: 55.363555908203125
epoch 132800  training loss: 12.174736976623535
epoch 132800  clean testing loss: 55.41401290893555
epoch 132900  training loss: 12.449380874633789
epoch 132900  clean testing loss: 55.35719680786133
epoch 133000  training loss: 12.698623657226562
epoch 133000  clean testing loss: 55.38169860839844

 44%|███████████████████████████████████                                            | 133030/300000 [54:14<42:22, 65.66it/s]
epoch 133100  training loss: 11.932760238647461

 44%|███████████████████████████████████                                            | 133163/300000 [54:16<41:52, 66.41it/s]
epoch 133200  training loss: 11.736937522888184

 44%|███████████████████████████████████                                            | 133296/300000 [54:18<42:03, 66.05it/s]
epoch 133300  training loss: 12.402328491210938
epoch 133300  clean testing loss: 55.304229736328125
epoch 133400  training loss: 12.106014251708984
epoch 133400  clean testing loss: 55.42304229736328
epoch 133500  training loss: 12.237188339233398
epoch 133500  clean testing loss: 55.380680084228516
epoch 133600  training loss: 12.137638092041016
epoch 133600  clean testing loss: 55.240150451660156
epoch 133700  training loss: 12.250088691711426
epoch 133700  clean testing loss: 55.3841667175293
epoch 133800  training loss: 11.645190238952637
epoch 133800  clean testing loss: 55.51362609863281
epoch 133900  training loss: 12.123568534851074

 45%|███████████████████████████████████▎                                           | 133898/300000 [54:27<41:13, 67.14it/s]
epoch 134000  training loss: 11.748273849487305
epoch 134000  clean testing loss: 55.27738571166992

 45%|███████████████████████████████████▎                                           | 134031/300000 [54:29<41:51, 66.09it/s]
epoch 134100  training loss: 11.833337783813477

 45%|███████████████████████████████████▎                                           | 134164/300000 [54:31<41:46, 66.17it/s]
epoch 134200  training loss: 11.848517417907715
epoch 134200  clean testing loss: 55.50822830200195
epoch 134300  training loss: 12.198990821838379

 45%|███████████████████████████████████▎                                           | 134297/300000 [54:33<41:39, 66.29it/s]
epoch 134400  training loss: 11.581483840942383

 45%|███████████████████████████████████▍                                           | 134430/300000 [54:35<41:21, 66.72it/s]
epoch 134500  training loss: 11.439534187316895

 45%|███████████████████████████████████▍                                           | 134556/300000 [54:37<41:37, 66.24it/s]
epoch 134600  training loss: 11.892260551452637
epoch 134600  clean testing loss: 55.4349479675293
epoch 134700  training loss: 12.240388870239258

 45%|███████████████████████████████████▍                                           | 134689/300000 [54:39<41:40, 66.10it/s]
epoch 134800  training loss: 11.517648696899414

 45%|███████████████████████████████████▌                                           | 134822/300000 [54:41<42:01, 65.51it/s]
epoch 134900  training loss: 11.770941734313965

 45%|███████████████████████████████████▌                                           | 134955/300000 [54:43<42:04, 65.38it/s]
epoch 135000  training loss: 11.801831245422363
epoch 135000  clean testing loss: 55.47037887573242
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise5.00e+01_invop1 ...
epoch 135100  training loss: 11.540992736816406

 45%|███████████████████████████████████▌                                           | 135088/300000 [54:45<41:39, 65.97it/s]
epoch 135200  training loss: 11.886787414550781

 45%|███████████████████████████████████▌                                           | 135221/300000 [54:47<41:46, 65.74it/s]
epoch 135300  training loss: 11.427093505859375

 45%|███████████████████████████████████▋                                           | 135354/300000 [54:49<41:31, 66.09it/s]
epoch 135400  training loss: 11.299976348876953

 45%|███████████████████████████████████▋                                           | 135480/300000 [54:51<41:23, 66.26it/s]
epoch 135500  training loss: 11.516097068786621
epoch 135500  clean testing loss: 55.4257926940918
epoch 135600  training loss: 11.645885467529297

 45%|███████████████████████████████████▋                                           | 135613/300000 [54:53<41:58, 65.27it/s]
epoch 135700  training loss: 11.649263381958008

 45%|███████████████████████████████████▋                                           | 135746/300000 [54:55<41:43, 65.60it/s]
epoch 135800  training loss: 11.321582794189453

 45%|███████████████████████████████████▊                                           | 135879/300000 [54:57<41:28, 65.96it/s]
epoch 135900  training loss: 11.915120124816895
epoch 135900  clean testing loss: 55.4514045715332
epoch 136000  training loss: 11.615640640258789
epoch 136000  clean testing loss: 55.45014190673828

 45%|███████████████████████████████████▊                                           | 136012/300000 [54:59<41:55, 65.18it/s]
epoch 136100  training loss: 11.801131248474121
epoch 136100  clean testing loss: 55.43039321899414
epoch 136200  training loss: 11.596426010131836
epoch 136200  clean testing loss: 55.445125579833984
epoch 136300  training loss: 11.882564544677734
epoch 136300  clean testing loss: 55.30820846557617
epoch 136400  training loss: 11.576650619506836
epoch 136400  clean testing loss: 55.48423767089844
epoch 136500  training loss: 12.814569473266602
epoch 136500  clean testing loss: 55.28416061401367
epoch 136600  training loss: 11.685965538024902
epoch 136600  clean testing loss: 55.48862075805664
epoch 136700  training loss: 11.785260200500488
epoch 136700  clean testing loss: 55.45295333862305
epoch 136800  training loss: 11.853719711303711
epoch 136800  clean testing loss: 55.30219650268555
epoch 136900  training loss: 11.813918113708496
epoch 136900  clean testing loss: 55.29265213012695
epoch 137000  training loss: 11.849623680114746
epoch 137000  clean testing loss: 55.41238784790039
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise5.00e+01_invop1 ...
epoch 137100  training loss: 12.424824714660645

 46%|████████████████████████████████████                                           | 137116/300000 [55:22<42:27, 63.94it/s]
epoch 137200  training loss: 11.869227409362793

 46%|████████████████████████████████████▏                                          | 137268/300000 [55:24<35:17, 76.86it/s]
epoch 137300  training loss: 12.22514533996582
epoch 137300  clean testing loss: 55.28940963745117
epoch 137400  training loss: 12.028346061706543

 46%|████████████████████████████████████▏                                          | 137428/300000 [55:26<35:25, 76.47it/s]
epoch 137500  training loss: 12.317965507507324
epoch 137500  clean testing loss: 55.35515594482422
epoch 137600  training loss: 12.3966703414917
epoch 137600  clean testing loss: 55.36552429199219
epoch 137700  training loss: 12.279352188110352
epoch 137700  clean testing loss: 55.36320114135742
epoch 137800  training loss: 11.891436576843262
epoch 137800  clean testing loss: 55.43960952758789
epoch 137900  training loss: 12.273192405700684
epoch 137900  clean testing loss: 55.43524169921875
epoch 138000  training loss: 11.732748985290527
epoch 138000  clean testing loss: 55.39107131958008
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise5.00e+01_invop1 ...
epoch 138100  training loss: 11.960089683532715


 46%|████████████████████████████████████▍                                          | 138292/300000 [55:38<35:12, 76.55it/s]
epoch 138200  training loss: 12.562190055847168
epoch 138200  clean testing loss: 55.34889221191406
epoch 138300  training loss: 11.669337272644043

 46%|████████████████████████████████████▍                                          | 138444/300000 [55:40<35:19, 76.22it/s]
epoch 138400  training loss: 11.593240737915039

 46%|████████████████████████████████████▍                                          | 138596/300000 [55:42<35:03, 76.73it/s]
epoch 138500  training loss: 11.459842681884766
epoch 138500  clean testing loss: 55.3294792175293
epoch 138600  training loss: 12.408012390136719

 46%|████████████████████████████████████▌                                          | 138756/300000 [55:44<35:15, 76.22it/s]
epoch 138700  training loss: 11.966080665588379

 46%|████████████████████████████████████▌                                          | 138908/300000 [55:46<34:51, 77.01it/s]
epoch 138800  training loss: 12.45577621459961
epoch 138800  clean testing loss: 55.29439926147461
epoch 138900  training loss: 11.929191589355469
epoch 138900  clean testing loss: 55.28908920288086
epoch 139000  training loss: 12.429232597351074
epoch 139000  clean testing loss: 55.289669036865234
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise5.00e+01_invop1 ...
epoch 139100  training loss: 12.122262001037598

 46%|████████████████████████████████████▋                                          | 139116/300000 [55:52<37:33, 71.40it/s]
epoch 139200  training loss: 11.841197967529297

 46%|████████████████████████████████████▋                                          | 139268/300000 [55:54<34:56, 76.68it/s]
epoch 139300  training loss: 12.018864631652832
epoch 139300  clean testing loss: 55.271324157714844
epoch 139400  training loss: 11.648694038391113

 46%|████████████████████████████████████▋                                          | 139420/300000 [55:56<35:03, 76.36it/s]
epoch 139500  training loss: 11.318826675415039

 47%|████████████████████████████████████▊                                          | 139572/300000 [55:58<35:05, 76.21it/s]
epoch 139600  training loss: 11.830498695373535
epoch 139600  clean testing loss: 55.3767204284668
epoch 139700  training loss: 11.61764907836914

 47%|████████████████████████████████████▊                                          | 139732/300000 [56:00<34:51, 76.62it/s]
epoch 139800  training loss: 11.895248413085938

 47%|████████████████████████████████████▊                                          | 139884/300000 [56:02<34:51, 76.56it/s]
epoch 139900  training loss: 12.303386688232422
epoch 139900  clean testing loss: 55.35908889770508
epoch 140000  training loss: 12.269819259643555
epoch 140000  clean testing loss: 55.33386993408203

 47%|████████████████████████████████████▉                                          | 140036/300000 [56:04<35:01, 76.12it/s]
epoch 140100  training loss: 11.77894115447998

 47%|████████████████████████████████████▉                                          | 140188/300000 [56:06<34:50, 76.46it/s]
epoch 140200  training loss: 12.263097763061523
epoch 140200  clean testing loss: 55.28173828125
epoch 140300  training loss: 11.852665901184082

 47%|████████████████████████████████████▉                                          | 140340/300000 [56:08<35:01, 75.96it/s]
epoch 140400  training loss: 11.867076873779297

 47%|████████████████████████████████████▉                                          | 140492/300000 [56:10<35:06, 75.72it/s]
epoch 140500  training loss: 11.91985034942627
epoch 140500  clean testing loss: 55.278926849365234
epoch 140600  training loss: 11.680194854736328

 47%|█████████████████████████████████████                                          | 140644/300000 [56:12<34:53, 76.13it/s]
epoch 140700  training loss: 12.080610275268555
epoch 140700  clean testing loss: 55.33356857299805
epoch 140800  training loss: 12.31830883026123

 47%|█████████████████████████████████████                                          | 140796/300000 [56:14<34:54, 76.03it/s]
epoch 140900  training loss: 11.768418312072754

 47%|█████████████████████████████████████                                          | 140948/300000 [56:16<34:55, 75.90it/s]
epoch 141000  training loss: 12.120209693908691
epoch 141000  clean testing loss: 55.30472946166992
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise5.00e+01_invop1 ...
epoch 141100  training loss: 12.26583480834961

 47%|█████████████████████████████████████▏                                         | 141116/300000 [56:30<42:52, 61.77it/s]
epoch 141200  training loss: 12.064696311950684

 47%|█████████████████████████████████████▏                                         | 141268/300000 [56:32<34:39, 76.35it/s]
epoch 141300  training loss: 12.032045364379883
epoch 141300  clean testing loss: 55.336891174316406
epoch 141400  training loss: 12.52281379699707

 47%|█████████████████████████████████████▏                                         | 141420/300000 [56:34<34:41, 76.20it/s]
epoch 141500  training loss: 12.72597599029541

 47%|█████████████████████████████████████▎                                         | 141572/300000 [56:36<34:21, 76.87it/s]
epoch 141600  training loss: 12.058920860290527
epoch 141600  clean testing loss: 55.30875778198242
epoch 141700  training loss: 12.276148796081543

 47%|█████████████████████████████████████▎                                         | 141724/300000 [56:38<34:45, 75.91it/s]
epoch 141800  training loss: 12.333605766296387
epoch 141800  clean testing loss: 55.290340423583984
epoch 141900  training loss: 12.388218879699707
epoch 141900  clean testing loss: 55.26081848144531
epoch 142000  training loss: 12.070030212402344
epoch 142000  clean testing loss: 55.26514434814453
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise5.00e+01_invop1 ...
epoch 142100  training loss: 12.473885536193848

 47%|█████████████████████████████████████▍                                         | 142132/300000 [56:51<37:58, 69.28it/s]
epoch 142200  training loss: 12.171971321105957

 47%|█████████████████████████████████████▍                                         | 142284/300000 [56:53<34:11, 76.90it/s]
epoch 142300  training loss: 12.028780937194824
epoch 142300  clean testing loss: 55.30275344848633
epoch 142400  training loss: 12.502004623413086

 47%|█████████████████████████████████████▌                                         | 142436/300000 [56:55<34:24, 76.34it/s]
epoch 142500  training loss: 12.204303741455078

 48%|█████████████████████████████████████▌                                         | 142588/300000 [56:57<34:09, 76.80it/s]
epoch 142600  training loss: 11.96679973602295
epoch 142600  clean testing loss: 55.3089485168457
epoch 142700  training loss: 11.967235565185547

 48%|█████████████████████████████████████▌                                         | 142740/300000 [56:59<34:26, 76.11it/s]
epoch 142800  training loss: 12.251688957214355

 48%|█████████████████████████████████████▋                                         | 142900/300000 [57:01<34:12, 76.53it/s]
epoch 142900  training loss: 12.269922256469727
epoch 142900  clean testing loss: 55.25949478149414
epoch 143000  training loss: 12.641358375549316
epoch 143000  clean testing loss: 55.30318832397461

 48%|█████████████████████████████████████▋                                         | 143052/300000 [57:03<34:32, 75.72it/s]
epoch 143100  training loss: 11.985888481140137

 48%|█████████████████████████████████████▋                                         | 143204/300000 [57:05<34:22, 76.01it/s]
epoch 143200  training loss: 12.401717185974121
epoch 143200  clean testing loss: 55.28886032104492
epoch 143300  training loss: 12.793560981750488

 48%|█████████████████████████████████████▊                                         | 143356/300000 [57:07<34:08, 76.46it/s]
epoch 143400  training loss: 12.203034400939941
epoch 143400  clean testing loss: 55.27684020996094
epoch 143500  training loss: 12.514670372009277

 48%|█████████████████████████████████████▊                                         | 143508/300000 [57:09<34:27, 75.68it/s]
epoch 143600  training loss: 12.14199447631836

 48%|█████████████████████████████████████▊                                         | 143660/300000 [57:11<34:27, 75.62it/s]
epoch 143700  training loss: 12.058500289916992
epoch 143700  clean testing loss: 55.330936431884766
epoch 143800  training loss: 12.364651679992676

 48%|█████████████████████████████████████▊                                         | 143812/300000 [57:13<34:13, 76.07it/s]
epoch 143900  training loss: 11.875184059143066

 48%|█████████████████████████████████████▉                                         | 143964/300000 [57:15<34:01, 76.42it/s]
epoch 144000  training loss: 12.104769706726074
epoch 144000  clean testing loss: 55.25132751464844
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise5.00e+01_invop1 ...
epoch 144100  training loss: 12.159722328186035

 48%|█████████████████████████████████████▉                                         | 144116/300000 [57:17<34:14, 75.87it/s]
epoch 144200  training loss: 12.131917953491211

 48%|█████████████████████████████████████▉                                         | 144268/300000 [57:19<34:08, 76.01it/s]
epoch 144300  training loss: 11.644274711608887
epoch 144300  clean testing loss: 55.26384735107422
epoch 144400  training loss: 12.601841926574707

 48%|██████████████████████████████████████                                         | 144420/300000 [57:21<34:05, 76.04it/s]
epoch 144500  training loss: 12.034591674804688

 48%|██████████████████████████████████████                                         | 144572/300000 [57:23<34:26, 75.20it/s]
epoch 144600  training loss: 12.079147338867188
epoch 144600  clean testing loss: 55.34963607788086
epoch 144700  training loss: 12.11166763305664

 48%|██████████████████████████████████████                                         | 144727/300000 [57:25<31:18, 82.65it/s]
epoch 144800  training loss: 12.514315605163574

 48%|██████████████████████████████████████▏                                        | 144898/300000 [57:27<31:06, 83.08it/s]
epoch 144900  training loss: 11.761438369750977
epoch 144900  clean testing loss: 55.31388473510742
epoch 145000  training loss: 12.385682106018066
epoch 145000  clean testing loss: 55.26221466064453

 48%|██████████████████████████████████████▏                                        | 145060/300000 [57:29<31:02, 83.21it/s]
epoch 145100  training loss: 11.875482559204102
epoch 145100  clean testing loss: 55.28573989868164
epoch 145200  training loss: 11.82575798034668

 48%|██████████████████████████████████████▏                                        | 145231/300000 [57:31<30:56, 83.36it/s]
epoch 145300  training loss: 12.007312774658203

 48%|██████████████████████████████████████▎                                        | 145393/300000 [57:33<30:51, 83.48it/s]
epoch 145400  training loss: 12.12246322631836
epoch 145400  clean testing loss: 55.280208587646484
epoch 145500  training loss: 11.396516799926758

 49%|██████████████████████████████████████▎                                        | 145564/300000 [57:35<30:53, 83.32it/s]
epoch 145600  training loss: 11.927834510803223
epoch 145600  clean testing loss: 55.28713607788086
epoch 145700  training loss: 11.663756370544434
epoch 145700  clean testing loss: 55.315277099609375
epoch 145800  training loss: 11.687131881713867
epoch 145800  clean testing loss: 55.291900634765625
epoch 145900  training loss: 11.682059288024902
epoch 145900  clean testing loss: 55.2956657409668
epoch 146000  training loss: 12.022041320800781
epoch 146000  clean testing loss: 55.311805725097656
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise5.00e+01_invop1 ...
epoch 146100  training loss: 11.947453498840332

 49%|██████████████████████████████████████▍                                        | 146131/300000 [57:43<31:00, 82.69it/s]
epoch 146200  training loss: 12.240431785583496
epoch 146200  clean testing loss: 55.28343963623047
epoch 146300  training loss: 12.11795425415039

 49%|██████████████████████████████████████▌                                        | 146302/300000 [57:45<30:34, 83.76it/s]
epoch 146400  training loss: 12.213518142700195

 49%|██████████████████████████████████████▌                                        | 146464/300000 [57:47<30:31, 83.83it/s]
epoch 146500  training loss: 12.45692253112793
epoch 146500  clean testing loss: 55.29361343383789
epoch 146600  training loss: 12.211686134338379

 49%|██████████████████████████████████████▌                                        | 146635/300000 [57:49<30:30, 83.78it/s]
epoch 146700  training loss: 12.082805633544922
epoch 146700  clean testing loss: 55.2841796875
epoch 146800  training loss: 11.661906242370605

 49%|██████████████████████████████████████▋                                        | 146806/300000 [57:51<30:27, 83.82it/s]
epoch 146900  training loss: 11.501422882080078
epoch 146900  clean testing loss: 55.25846481323242
epoch 147000  training loss: 11.647004127502441
epoch 147000  clean testing loss: 55.26025390625



 49%|██████████████████████████████████████▋                                        | 147138/300000 [58:09<36:19, 70.13it/s]
epoch 147100  training loss: 11.861830711364746

 49%|██████████████████████████████████████▊                                        | 147300/300000 [58:11<30:23, 83.75it/s]
epoch 147200  training loss: 11.720035552978516
epoch 147200  clean testing loss: 55.25336837768555
epoch 147300  training loss: 11.611368179321289

 49%|██████████████████████████████████████▊                                        | 147471/300000 [58:13<30:15, 84.00it/s]
epoch 147400  training loss: 11.682716369628906
epoch 147400  clean testing loss: 55.27903747558594
epoch 147500  training loss: 12.196274757385254
epoch 147500  clean testing loss: 55.25984573364258
epoch 147600  training loss: 11.708367347717285

 49%|██████████████████████████████████████▉                                        | 147642/300000 [58:15<30:19, 83.73it/s]
epoch 147700  training loss: 11.871907234191895
epoch 147700  clean testing loss: 55.27434539794922
epoch 147800  training loss: 11.970080375671387

 49%|██████████████████████████████████████▉                                        | 147804/300000 [58:17<30:27, 83.29it/s]
epoch 147900  training loss: 12.161120414733887
epoch 147900  clean testing loss: 55.31586837768555
epoch 148000  training loss: 12.120064735412598
epoch 148000  clean testing loss: 55.3143310546875



 49%|███████████████████████████████████████                                        | 148136/300000 [58:26<32:19, 78.31it/s]
epoch 148100  training loss: 12.405915260314941
epoch 148100  clean testing loss: 55.300086975097656
epoch 148200  training loss: 12.045409202575684

 49%|███████████████████████████████████████                                        | 148307/300000 [58:28<30:12, 83.71it/s]
epoch 148300  training loss: 11.823713302612305
epoch 148300  clean testing loss: 55.2878303527832
epoch 148400  training loss: 12.348654747009277

 49%|███████████████████████████████████████                                        | 148478/300000 [58:30<30:03, 84.02it/s]
epoch 148500  training loss: 12.218111991882324

 50%|███████████████████████████████████████▏                                       | 148649/300000 [58:32<29:58, 84.16it/s]
epoch 148600  training loss: 12.14157485961914
epoch 148600  clean testing loss: 55.2452392578125
epoch 148700  training loss: 12.144838333129883

 50%|███████████████████████████████████████▏                                       | 148811/300000 [58:34<30:11, 83.44it/s]
epoch 148800  training loss: 12.073426246643066
epoch 148800  clean testing loss: 55.23466110229492
epoch 148900  training loss: 11.69571304321289

 50%|███████████████████████████████████████▏                                       | 148982/300000 [58:36<30:02, 83.77it/s]
epoch 149000  training loss: 11.913973808288574
epoch 149000  clean testing loss: 55.23213577270508

 50%|███████████████████████████████████████▎                                       | 149144/300000 [58:38<29:58, 83.86it/s]
epoch 149100  training loss: 12.234989166259766
epoch 149100  clean testing loss: 55.23899459838867
epoch 149200  training loss: 11.896658897399902

 50%|███████████████████████████████████████▎                                       | 149315/300000 [58:40<30:05, 83.47it/s]
epoch 149300  training loss: 11.80523681640625
epoch 149300  clean testing loss: 55.381282806396484
epoch 149400  training loss: 11.574356079101562

 50%|███████████████████████████████████████▎                                       | 149486/300000 [58:42<29:53, 83.90it/s]
epoch 149500  training loss: 12.085882186889648

 50%|███████████████████████████████████████▍                                       | 149648/300000 [58:44<30:00, 83.52it/s]
epoch 149600  training loss: 11.7340669631958
epoch 149600  clean testing loss: 55.37736129760742
epoch 149700  training loss: 11.951940536499023

 50%|███████████████████████████████████████▍                                       | 149819/300000 [58:46<30:01, 83.36it/s]
epoch 149800  training loss: 11.888069152832031
epoch 149800  clean testing loss: 55.38423538208008
epoch 149900  training loss: 11.744797706604004

 50%|███████████████████████████████████████▍                                       | 149981/300000 [58:48<29:45, 84.04it/s]
epoch 150000  training loss: 12.55002212524414
epoch 150000  clean testing loss: 55.367950439453125

 50%|███████████████████████████████████████▌                                       | 150152/300000 [58:50<29:41, 84.09it/s]
epoch 150100  training loss: 12.465784072875977
epoch 150100  clean testing loss: 55.38093948364258
epoch 150200  training loss: 12.116662979125977

 50%|███████████████████████████████████████▌                                       | 150323/300000 [58:52<29:50, 83.58it/s]
epoch 150300  training loss: 12.486377716064453
epoch 150300  clean testing loss: 55.36724853515625
epoch 150400  training loss: 12.1710786819458

 50%|███████████████████████████████████████▋                                       | 150485/300000 [58:54<29:52, 83.42it/s]
epoch 150500  training loss: 11.85145092010498

 50%|███████████████████████████████████████▋                                       | 150656/300000 [58:56<29:37, 84.00it/s]
epoch 150600  training loss: 12.181129455566406
epoch 150600  clean testing loss: 55.416046142578125
epoch 150700  training loss: 12.011181831359863

 50%|███████████████████████████████████████▋                                       | 150827/300000 [58:58<29:35, 84.01it/s]
epoch 150800  training loss: 11.874466896057129
epoch 150800  clean testing loss: 55.40340042114258
epoch 150900  training loss: 12.121259689331055

 50%|███████████████████████████████████████▊                                       | 150989/300000 [59:00<29:35, 83.93it/s]
epoch 151000  training loss: 12.48888874053955
epoch 151000  clean testing loss: 55.37470626831055

 50%|███████████████████████████████████████▊                                       | 151151/300000 [59:03<29:43, 83.45it/s]
epoch 151100  training loss: 12.51517105102539
epoch 151100  clean testing loss: 55.39400863647461
epoch 151200  training loss: 12.303889274597168

 50%|███████████████████████████████████████▊                                       | 151313/300000 [59:04<29:46, 83.21it/s]
epoch 151300  training loss: 11.979849815368652

 50%|███████████████████████████████████████▉                                       | 151484/300000 [59:07<29:38, 83.48it/s]
epoch 151400  training loss: 11.958589553833008
epoch 151400  clean testing loss: 55.401546478271484
epoch 151500  training loss: 11.920035362243652

 51%|███████████████████████████████████████▉                                       | 151772/300000 [59:10<29:29, 83.75it/s]
epoch 151600  training loss: 11.881390571594238
epoch 151600  clean testing loss: 55.40081787109375
epoch 151700  training loss: 11.982473373413086

 51%|████████████████████████████████████████                                       | 151943/300000 [59:12<29:31, 83.56it/s]
epoch 151800  training loss: 12.124637603759766
epoch 151800  clean testing loss: 55.401611328125
epoch 151900  training loss: 12.229873657226562

 51%|████████████████████████████████████████                                       | 152114/300000 [59:14<29:29, 83.59it/s]
epoch 152000  training loss: 12.214669227600098
epoch 152000  clean testing loss: 55.37455749511719
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise5.00e+01_invop1 ...
epoch 152100  training loss: 12.125595092773438

 51%|████████████████████████████████████████                                       | 152276/300000 [59:16<29:20, 83.92it/s]
epoch 152200  training loss: 12.387815475463867

 51%|████████████████████████████████████████▏                                      | 152447/300000 [59:18<29:13, 84.16it/s]
epoch 152300  training loss: 12.317473411560059
epoch 152300  clean testing loss: 55.4020881652832
epoch 152400  training loss: 12.300118446350098

 51%|████████████████████████████████████████▎                                      | 152870/300000 [59:23<29:21, 83.51it/s]
epoch 152500  training loss: 12.248676300048828
epoch 152500  clean testing loss: 55.39824295043945
epoch 152600  training loss: 12.243976593017578
epoch 152600  clean testing loss: 55.40483474731445
epoch 152700  training loss: 12.43161392211914
epoch 152700  clean testing loss: 55.402198791503906
epoch 152800  training loss: 12.566494941711426

 51%|████████████████████████████████████████▎                                      | 153032/300000 [59:25<29:25, 83.23it/s]
epoch 152900  training loss: 12.263352394104004
epoch 152900  clean testing loss: 55.4069938659668
epoch 153000  training loss: 11.960631370544434
epoch 153000  clean testing loss: 55.40814208984375

 51%|████████████████████████████████████████▎                                      | 153203/300000 [59:27<29:22, 83.28it/s]
epoch 153100  training loss: 11.981793403625488
epoch 153100  clean testing loss: 55.42495346069336
epoch 153200  training loss: 11.827765464782715

 51%|████████████████████████████████████████▍                                      | 153365/300000 [59:29<29:14, 83.60it/s]
epoch 153300  training loss: 12.241236686706543

 51%|████████████████████████████████████████▍                                      | 153536/300000 [59:31<29:15, 83.45it/s]
epoch 153400  training loss: 11.577632904052734
epoch 153400  clean testing loss: 55.387176513671875
epoch 153500  training loss: 12.143381118774414

 51%|████████████████████████████████████████▍                                      | 153707/300000 [59:33<29:12, 83.47it/s]
epoch 153600  training loss: 11.649443626403809
epoch 153600  clean testing loss: 55.390899658203125
epoch 153700  training loss: 11.912094116210938

 51%|████████████████████████████████████████▌                                      | 153869/300000 [59:35<29:09, 83.55it/s]
epoch 153800  training loss: 12.28866958618164

 51%|████████████████████████████████████████▌                                      | 154040/300000 [59:37<29:11, 83.34it/s]
epoch 153900  training loss: 12.427284240722656
epoch 153900  clean testing loss: 55.402339935302734
epoch 154000  training loss: 12.0236177444458
epoch 154000  clean testing loss: 55.374916076660156

 51%|████████████████████████████████████████▌                                      | 154202/300000 [59:39<29:07, 83.43it/s]
epoch 154100  training loss: 12.228286743164062
epoch 154100  clean testing loss: 55.401588439941406
epoch 154200  training loss: 12.357211112976074

 51%|████████████████████████████████████████▋                                      | 154373/300000 [59:41<29:03, 83.52it/s]
epoch 154300  training loss: 11.693258285522461

 52%|████████████████████████████████████████▋                                      | 154544/300000 [59:43<28:59, 83.61it/s]
epoch 154400  training loss: 11.784562110900879
epoch 154400  clean testing loss: 55.410911560058594
epoch 154500  training loss: 11.793387413024902

 52%|████████████████████████████████████████▋                                      | 154706/300000 [59:45<29:04, 83.28it/s]
epoch 154600  training loss: 11.781240463256836
epoch 154600  clean testing loss: 55.43130111694336
epoch 154700  training loss: 11.956113815307617

 52%|████████████████████████████████████████▊                                      | 154877/300000 [59:47<28:57, 83.54it/s]
epoch 154800  training loss: 12.029773712158203

 52%|████████████████████████████████████████▊                                      | 155039/300000 [59:49<28:58, 83.40it/s]
epoch 154900  training loss: 12.323612213134766
epoch 154900  clean testing loss: 55.420738220214844
epoch 155000  training loss: 12.33590316772461
epoch 155000  clean testing loss: 55.42232894897461

 52%|████████████████████████████████████████▊                                      | 155210/300000 [59:51<28:49, 83.72it/s]
epoch 155100  training loss: 12.154889106750488
epoch 155100  clean testing loss: 55.39276885986328
epoch 155200  training loss: 12.180896759033203

 52%|████████████████████████████████████████▉                                      | 155381/300000 [59:53<28:51, 83.53it/s]
epoch 155300  training loss: 12.220561027526855

 52%|████████████████████████████████████████▉                                      | 155543/300000 [59:55<28:49, 83.52it/s]
epoch 155400  training loss: 12.725963592529297
epoch 155400  clean testing loss: 55.39530944824219
epoch 155500  training loss: 12.04377555847168

 52%|█████████████████████████████████████████                                      | 155714/300000 [59:57<28:44, 83.65it/s]
epoch 155600  training loss: 12.353986740112305
epoch 155600  clean testing loss: 55.40582275390625
epoch 155700  training loss: 11.868450164794922
epoch 155700  clean testing loss: 55.39657211303711
epoch 155800  training loss: 11.534268379211426
epoch 155800  clean testing loss: 55.39252471923828
Validation loss variation < 1e-6, trained to interpolation, stop

 52%|█████████████████████████████████████████                                      | 155800/300000 [59:58<55:30, 43.29it/s]