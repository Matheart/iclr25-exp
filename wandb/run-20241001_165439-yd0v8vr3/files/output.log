
  0%|                                                                                | 95/100000 [00:01<23:31, 70.75it/s]
epoch 0  training loss: 40.66864776611328
epoch 0  clean testing loss: 48.85091781616211
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size10_noise1.00e-01_invop1 ...
epoch 100  training loss: 0.40798765420913696

  0%|▏                                                                              | 239/100000 [00:03<23:02, 72.18it/s]
epoch 200  training loss: 0.0007066367543302476

  0%|▎                                                                              | 383/100000 [00:05<23:25, 70.89it/s]
epoch 300  training loss: 0.00011019178054993972

  1%|▍                                                                              | 527/100000 [00:07<23:33, 70.38it/s]
epoch 400  training loss: 6.147703288661432e-07
epoch 400  clean testing loss: 2.83744740486145
epoch 500  training loss: 2.3535940840702096e-07

  1%|▍                                                                              | 599/100000 [00:08<23:27, 70.64it/s]
epoch 600  training loss: 8.961016106923125e-08

  1%|▊                                                                             | 1071/100000 [00:19<38:45, 42.54it/s]
epoch 700  training loss: 0.00014111991913523525
epoch 700  clean testing loss: 2.788830518722534
epoch 800  training loss: 2.9455887329277175e-07
epoch 800  clean testing loss: 2.823451519012451
epoch 900  training loss: 5.321284788806224e-06
epoch 900  clean testing loss: 2.8079166412353516
epoch 1000  training loss: 3.170706258970313e-05
epoch 1000  clean testing loss: 2.7938199043273926

  1%|▉                                                                             | 1215/100000 [00:21<22:32, 73.02it/s]
epoch 1100  training loss: 0.000427080609370023
epoch 1100  clean testing loss: 2.8459417819976807
epoch 1200  training loss: 6.39119525658316e-06

  1%|█                                                                             | 1359/100000 [00:23<23:18, 70.52it/s]
epoch 1300  training loss: 2.920666338468436e-05

  1%|█▏                                                                            | 1496/100000 [00:25<25:23, 64.65it/s]
epoch 1400  training loss: 5.4015786190575454e-06
epoch 1400  clean testing loss: 2.847015380859375
epoch 1500  training loss: 1.8297940187039785e-05

  2%|█▎                                                                            | 1622/100000 [00:27<26:10, 62.64it/s]
epoch 1600  training loss: 0.00011792425357270986

  2%|█▎                                                                            | 1755/100000 [00:29<25:33, 64.06it/s]
epoch 1700  training loss: 1.946652446349617e-05

  2%|█▍                                                                            | 1881/100000 [00:31<25:15, 64.74it/s]
epoch 1800  training loss: 1.8255786926602013e-05

  2%|█▌                                                                            | 2007/100000 [00:33<26:36, 61.36it/s]
epoch 1900  training loss: 4.0826522308634594e-05
epoch 1900  clean testing loss: 2.909468650817871
epoch 2000  training loss: 0.0001980573870241642
epoch 2000  clean testing loss: 2.8831355571746826

  2%|█▋                                                                            | 2140/100000 [00:35<25:27, 64.08it/s]
epoch 2100  training loss: 1.5367329979198985e-05

  2%|█▊                                                                            | 2266/100000 [00:37<25:21, 64.22it/s]
epoch 2200  training loss: 6.78132419125177e-05

  2%|█▊                                                                            | 2399/100000 [00:39<25:26, 63.95it/s]
epoch 2300  training loss: 4.302152319723973e-06
epoch 2300  clean testing loss: 2.846046209335327
epoch 2400  training loss: 0.003290311200544238

  3%|█▉                                                                            | 2525/100000 [00:41<25:12, 64.43it/s]
epoch 2500  training loss: 2.9268217986100353e-05

  3%|██                                                                            | 2658/100000 [00:43<25:11, 64.41it/s]
epoch 2600  training loss: 3.347418669363833e-06

  3%|██                                                                            | 2700/100000 [00:44<25:14, 64.26it/s]
epoch 2700  training loss: 0.0002668650995474309

  3%|██▍                                                                           | 3100/100000 [00:54<27:02, 59.74it/s]
epoch 2800  training loss: 1.4702787893838831e-06
epoch 2800  clean testing loss: 2.801060199737549
epoch 2900  training loss: 6.453433343267534e-06
epoch 2900  clean testing loss: 2.8094425201416016
epoch 3000  training loss: 8.926240298023913e-07
epoch 3000  clean testing loss: 2.797348976135254
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size10_noise1.00e-01_invop1 ...
epoch 3100  training loss: 1.6415713616879657e-05
epoch 3100  clean testing loss: 2.785783529281616
epoch 3200  training loss: 2.877788574551232e-06
epoch 3200  clean testing loss: 2.757352113723755
epoch 3300  training loss: 0.00040142968646250665

  3%|██▋                                                                           | 3366/100000 [00:58<24:52, 64.75it/s]
epoch 3400  training loss: 1.7481587519796449e-06

  3%|██▋                                                                           | 3492/100000 [01:00<24:55, 64.52it/s]
epoch 3500  training loss: 5.6297700723462185e-08


  4%|███▏                                                                          | 4080/100000 [01:09<24:52, 64.28it/s]
epoch 3600  training loss: 2.9338355034269625e-06
epoch 3600  clean testing loss: 2.7974178791046143
epoch 3700  training loss: 4.967921086063143e-06
epoch 3700  clean testing loss: 2.7986056804656982
epoch 3800  training loss: 8.291581252706237e-07
epoch 3800  clean testing loss: 2.8046822547912598
epoch 3900  training loss: 2.3230557417264208e-05
epoch 3900  clean testing loss: 2.8018391132354736
epoch 4000  training loss: 2.8985818971705157e-06
epoch 4000  clean testing loss: 2.8042640686035156
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size10_noise1.00e-01_invop1 ...
epoch 4100  training loss: 2.5819135771598667e-06
epoch 4100  clean testing loss: 2.7995479106903076
epoch 4200  training loss: 4.383908162708394e-06


  5%|███▌                                                                          | 4598/100000 [01:17<24:41, 64.42it/s]
epoch 4300  training loss: 1.1675482483042288e-06
epoch 4300  clean testing loss: 2.7988288402557373
epoch 4400  training loss: 3.701566674862988e-05
epoch 4400  clean testing loss: 2.7947235107421875
epoch 4500  training loss: 2.3785066787240794e-06
epoch 4500  clean testing loss: 2.8004541397094727
epoch 4600  training loss: 3.332596918426134e-07

  5%|███▉                                                                          | 5074/100000 [01:29<34:16, 46.16it/s]
epoch 4700  training loss: 3.547278993210057e-06
epoch 4700  clean testing loss: 2.808957099914551
epoch 4800  training loss: 0.002560859778895974
epoch 4800  clean testing loss: 2.6891252994537354
epoch 4900  training loss: 1.819315258444476e-07
epoch 4900  clean testing loss: 2.7051589488983154
epoch 5000  training loss: 5.2456329058259143e-08
epoch 5000  clean testing loss: 2.703275680541992
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size10_noise1.00e-01_invop1 ...
epoch 5100  training loss: 3.1240168027579784e-05

  5%|████                                                                          | 5200/100000 [01:31<24:45, 63.80it/s]
epoch 5200  training loss: 0.010738227516412735

  5%|████▏                                                                         | 5298/100000 [01:33<24:26, 64.58it/s]
epoch 5300  training loss: 5.873143436474493e-06

  6%|████▍                                                                         | 5718/100000 [01:39<24:42, 63.59it/s]
epoch 5400  training loss: 2.89683366183624e-09
epoch 5400  clean testing loss: 2.5876948833465576
epoch 5500  training loss: 4.728803570941409e-09
epoch 5500  clean testing loss: 2.5999019145965576
epoch 5600  training loss: 3.313950571737223e-07
epoch 5600  clean testing loss: 2.5978591442108154
epoch 5700  training loss: 1.5138441256112856e-07

  6%|████▌                                                                         | 5844/100000 [01:41<24:19, 64.49it/s]
epoch 5800  training loss: 1.7188091078423895e-05
epoch 5800  clean testing loss: 2.5895793437957764
epoch 5900  training loss: 1.7782266468202579e-06

  6%|████▋                                                                         | 5977/100000 [01:43<24:19, 64.43it/s]
epoch 6000  training loss: 9.222943481290713e-05
epoch 6000  clean testing loss: 2.547034978866577

  6%|████▊                                                                         | 6103/100000 [01:45<24:34, 63.67it/s]
epoch 6100  training loss: 2.3038471681502415e-06

  6%|████▊                                                                         | 6194/100000 [01:47<24:04, 64.92it/s]
epoch 6200  training loss: 1.1178763998032082e-05
epoch 6200  clean testing loss: 2.5671017169952393
epoch 6300  training loss: 2.402242387233855e-07
epoch 6300  clean testing loss: 2.565145492553711
epoch 6400  training loss: 1.4183332496031653e-05
epoch 6400  clean testing loss: 2.578622817993164
epoch 6500  training loss: 1.2207871122882352e-06
epoch 6500  clean testing loss: 2.575165271759033
epoch 6600  training loss: 0.00011952144996030256
epoch 6600  clean testing loss: 2.587601661682129
epoch 6700  training loss: 1.0204937098023947e-05


  7%|█████▎                                                                        | 6852/100000 [01:57<24:01, 64.62it/s]
epoch 6800  training loss: 0.00014299142640084028

  7%|█████▍                                                                        | 6943/100000 [01:58<23:58, 64.70it/s]
epoch 6900  training loss: 4.680987785832258e-06

  7%|█████▎                                                                      | 7020/100000 [02:01<1:08:23, 22.66it/s]
epoch 7000  training loss: 0.00015022815205156803
epoch 7000  clean testing loss: 2.5277040004730225

  7%|█████▌                                                                        | 7146/100000 [02:03<23:55, 64.70it/s]
epoch 7100  training loss: 7.172265213739593e-06

  7%|█████▋                                                                        | 7279/100000 [02:05<24:02, 64.27it/s]
epoch 7200  training loss: 0.0002179883886128664
epoch 7200  clean testing loss: 2.5451416969299316
epoch 7300  training loss: 7.529091817559674e-06


  8%|█████▉                                                                        | 7601/100000 [02:10<23:44, 64.87it/s]
epoch 7400  training loss: 0.0004492161388043314
epoch 7400  clean testing loss: 2.54461932182312
epoch 7500  training loss: 0.002575185615569353

  8%|██████                                                                        | 7734/100000 [02:12<23:52, 64.42it/s]
epoch 7600  training loss: 0.0003155516751576215
epoch 7600  clean testing loss: 2.517648935317993
epoch 7700  training loss: 0.00015668080595787615

  8%|██████▏                                                                       | 7860/100000 [02:14<23:53, 64.29it/s]
epoch 7800  training loss: 0.0004761544114444405

  8%|██████▏                                                                       | 7895/100000 [02:14<23:49, 64.42it/s]
epoch 7900  training loss: 0.001573239336721599
epoch 7900  clean testing loss: 2.6403656005859375
epoch 8000  training loss: 6.102676343289204e-05
epoch 8000  clean testing loss: 2.4806408882141113

  8%|██████                                                                      | 8014/100000 [02:18<1:41:06, 15.16it/s]
epoch 8100  training loss: 5.934525324846618e-05

  8%|██████▎                                                                       | 8140/100000 [02:20<23:54, 64.02it/s]
epoch 8200  training loss: 4.5943819714011624e-05

  8%|██████▍                                                                       | 8273/100000 [02:22<23:53, 63.98it/s]
epoch 8300  training loss: 0.00021258620836306363

  8%|██████▌                                                                       | 8399/100000 [02:24<23:47, 64.16it/s]
epoch 8400  training loss: 0.0001641167764319107
epoch 8400  clean testing loss: 2.5852160453796387
epoch 8500  training loss: 8.874345076037571e-05

  9%|██████▋                                                                       | 8532/100000 [02:26<23:31, 64.78it/s]
epoch 8600  training loss: 3.860859123960836e-06

  9%|██████▊                                                                       | 8658/100000 [02:28<23:29, 64.79it/s]
epoch 8700  training loss: 2.9136868761270307e-05

  9%|██████▊                                                                       | 8791/100000 [02:30<23:37, 64.36it/s]
epoch 8800  training loss: 4.15797476307489e-05
epoch 8800  clean testing loss: 2.5983381271362305
epoch 8900  training loss: 0.00018545605416875333

  9%|██████▉                                                                       | 8917/100000 [02:32<23:31, 64.53it/s]
epoch 9000  training loss: 0.00027800549287348986
epoch 9000  clean testing loss: 2.598696231842041

  9%|███████                                                                       | 9050/100000 [02:34<23:30, 64.47it/s]
epoch 9100  training loss: 6.62787351757288e-06

  9%|███████▏                                                                      | 9176/100000 [02:36<23:16, 65.03it/s]
epoch 9200  training loss: 3.344814103911631e-05

  9%|███████▏                                                                      | 9197/100000 [02:36<23:24, 64.66it/s]
epoch 9300  training loss: 5.1718416216317564e-06
epoch 9300  clean testing loss: 2.6022088527679443
epoch 9400  training loss: 1.3040890735283028e-05
epoch 9400  clean testing loss: 2.6049139499664307
epoch 9500  training loss: 7.8791181294946e-06


 10%|███████▌                                                                      | 9631/100000 [02:43<23:14, 64.81it/s]
epoch 9600  training loss: 3.780094630201347e-05

 10%|███████▊                                                                      | 9953/100000 [02:48<23:15, 64.52it/s]
epoch 9700  training loss: 2.8295018637436442e-05
epoch 9700  clean testing loss: 2.6131179332733154
epoch 9800  training loss: 1.939735193445813e-05
epoch 9800  clean testing loss: 2.600151300430298
epoch 9900  training loss: 6.938431033631787e-05

 10%|███████▊                                                                     | 10086/100000 [02:50<23:12, 64.58it/s]
epoch 10000  training loss: 3.4772434446495026e-05
epoch 10000  clean testing loss: 2.5830559730529785

 10%|███████▊                                                                     | 10212/100000 [02:52<23:22, 64.03it/s]
epoch 10100  training loss: 7.32636108295992e-05
epoch 10100  clean testing loss: 2.576564073562622
epoch 10200  training loss: 2.2402030026569264e-06

 10%|███████▉                                                                     | 10345/100000 [02:54<23:20, 64.01it/s]
epoch 10300  training loss: 7.359748451563064e-06

 10%|████████                                                                     | 10394/100000 [02:55<23:09, 64.47it/s]
epoch 10400  training loss: 1.6227677406277508e-06
epoch 10400  clean testing loss: 2.572390079498291
epoch 10500  training loss: 8.045756840147078e-05
epoch 10500  clean testing loss: 2.6148669719696045
epoch 10600  training loss: 3.856035982607864e-05
epoch 10600  clean testing loss: 2.5772502422332764
epoch 10700  training loss: 3.212957381037995e-05
epoch 10700  clean testing loss: 2.590616226196289
epoch 10800  training loss: 0.0006336948135867715
epoch 10800  clean testing loss: 2.5504086017608643
epoch 10900  training loss: 5.094614971312694e-05
epoch 10900  clean testing loss: 2.5850188732147217
epoch 11000  training loss: 0.00031774997478350997
epoch 11000  clean testing loss: 2.569509267807007

 11%|████████▌                                                                    | 11073/100000 [03:06<24:14, 61.14it/s]
epoch 11100  training loss: 2.016091229961603e-06

 11%|████████▋                                                                    | 11206/100000 [03:08<22:59, 64.38it/s]
epoch 11200  training loss: 8.826120028970763e-05
epoch 11200  clean testing loss: 2.5640950202941895
epoch 11300  training loss: 1.8894264940172434e-05


 11%|████████▊                                                                    | 11465/100000 [03:12<22:54, 64.43it/s]
epoch 11400  training loss: 0.00038342582411132753
epoch 11400  clean testing loss: 2.5814075469970703
epoch 11500  training loss: 6.226416189747397e-06

 12%|████████▉                                                                    | 11591/100000 [03:14<23:00, 64.03it/s]
epoch 11600  training loss: 0.0001171190888271667
epoch 11600  clean testing loss: 2.578320026397705
epoch 11700  training loss: 0.0001476966281188652

 12%|█████████                                                                    | 11696/100000 [03:16<23:25, 62.84it/s]
epoch 11800  training loss: 0.0011809993302449584
epoch 11800  clean testing loss: 2.56335186958313
epoch 11900  training loss: 0.0005967166507616639

 12%|█████████▏                                                                   | 11948/100000 [03:24<30:56, 47.43it/s]
epoch 12000  training loss: 4.960510977980448e-06
epoch 12000  clean testing loss: 2.531712055206299


 12%|█████████▍                                                                   | 12207/100000 [03:28<23:18, 62.76it/s]
epoch 12100  training loss: 0.00011506053124321625

 12%|█████████▍                                                                   | 12333/100000 [03:30<22:42, 64.34it/s]
epoch 12200  training loss: 3.54450021404773e-05
epoch 12200  clean testing loss: 2.536154270172119
epoch 12300  training loss: 0.00014402651868294924
epoch 12300  clean testing loss: 2.52079176902771
epoch 12400  training loss: 1.0984439541061874e-05

 12%|█████████▌                                                                   | 12466/100000 [03:32<22:31, 64.77it/s]
epoch 12500  training loss: 2.5198740331688896e-05

 13%|█████████▋                                                                   | 12599/100000 [03:34<22:14, 65.47it/s]
epoch 12600  training loss: 2.7987354769720696e-05
epoch 12600  clean testing loss: 2.510388135910034
epoch 12700  training loss: 7.333775283768773e-05


 13%|██████████                                                                   | 13082/100000 [03:42<22:23, 64.71it/s]
epoch 12800  training loss: 8.939290273701772e-05
epoch 12800  clean testing loss: 2.4609367847442627
epoch 12900  training loss: 1.350198272120906e-05
epoch 12900  clean testing loss: 2.494845390319824
epoch 13000  training loss: 0.0002948280598502606
epoch 13000  clean testing loss: 2.5095486640930176
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size10_noise1.00e-01_invop1 ...
epoch 13100  training loss: 0.0001540712983114645

 13%|██████████▏                                                                  | 13215/100000 [03:44<22:29, 64.30it/s]
epoch 13200  training loss: 1.9697876268764958e-05
epoch 13200  clean testing loss: 2.486269235610962
epoch 13300  training loss: 1.0251910680381116e-05


 13%|██████████▎                                                                  | 13474/100000 [03:48<22:19, 64.58it/s]
epoch 13400  training loss: 5.381618393585086e-06

 14%|██████████▍                                                                  | 13600/100000 [03:50<22:29, 64.04it/s]
epoch 13500  training loss: 0.00013473488797899336

 14%|██████████▌                                                                  | 13726/100000 [03:52<22:14, 64.65it/s]
epoch 13600  training loss: 6.421167927328497e-05
epoch 13600  clean testing loss: 2.4729957580566406
epoch 13700  training loss: 0.005277394782751799

 14%|██████████▋                                                                  | 13859/100000 [03:54<22:15, 64.51it/s]
epoch 13800  training loss: 2.4623004719614983e-06

 14%|██████████▊                                                                  | 13985/100000 [03:56<22:11, 64.61it/s]
epoch 13900  training loss: 2.40422309616406e-06

 14%|██████████▊                                                                  | 14111/100000 [03:58<22:44, 62.93it/s]
epoch 14000  training loss: 1.6932566495597712e-06
epoch 14000  clean testing loss: 2.4181039333343506

 14%|██████████▉                                                                  | 14209/100000 [03:59<22:20, 63.99it/s]
epoch 14100  training loss: 0.00010179359378525987
epoch 14100  clean testing loss: 2.4091780185699463
epoch 14200  training loss: 5.825660264235921e-05

 14%|███████████                                                                  | 14342/100000 [04:01<22:03, 64.74it/s]
epoch 14300  training loss: 6.772094820917118e-06

 14%|███████████▏                                                                 | 14468/100000 [04:03<21:59, 64.82it/s]
epoch 14400  training loss: 4.2113742892979644e-06

 15%|███████████▏                                                                 | 14601/100000 [04:05<22:11, 64.15it/s]
epoch 14500  training loss: 4.722172889159992e-05
epoch 14500  clean testing loss: 2.4347379207611084
epoch 14600  training loss: 7.032631401671097e-05

 15%|███████████▎                                                                 | 14727/100000 [04:07<22:06, 64.30it/s]
epoch 14700  training loss: 6.945197128516156e-06

 15%|███████████▍                                                                 | 14860/100000 [04:09<22:13, 63.83it/s]
epoch 14800  training loss: 9.867289008980151e-06

 15%|███████████▌                                                                 | 14986/100000 [04:11<21:59, 64.42it/s]
epoch 14900  training loss: 9.628365660319105e-05

 15%|███████████▋                                                                 | 15098/100000 [04:13<21:52, 64.67it/s]
epoch 15000  training loss: 9.521756874164566e-05
epoch 15000  clean testing loss: 2.4609248638153076
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size10_noise1.00e-01_invop1 ...
epoch 15100  training loss: 5.7420846133027226e-05

 15%|███████████▋                                                                 | 15245/100000 [04:15<22:16, 63.43it/s]
epoch 15200  training loss: 9.978839443647303e-07

 15%|███████████▊                                                                 | 15371/100000 [04:17<21:54, 64.36it/s]
epoch 15300  training loss: 9.94987476587994e-06

 15%|███████████▊                                                                 | 15399/100000 [04:18<21:54, 64.37it/s]
epoch 15400  training loss: 0.00047161057591438293

 16%|████████████▎                                                                | 16050/100000 [04:28<21:52, 63.98it/s]
epoch 15500  training loss: 1.4270264728111215e-05
epoch 15500  clean testing loss: 2.423952579498291
epoch 15600  training loss: 0.0006554450956173241
epoch 15600  clean testing loss: 2.4016239643096924
epoch 15700  training loss: 2.0455468074942473e-06
epoch 15700  clean testing loss: 2.4204773902893066
epoch 15800  training loss: 2.3086302462616004e-05
epoch 15800  clean testing loss: 2.4125239849090576
epoch 15900  training loss: 4.4033249650965445e-06
epoch 15900  clean testing loss: 2.4221818447113037
epoch 16000  training loss: 1.681128196651116e-06
epoch 16000  clean testing loss: 2.4247307777404785

 16%|████████████▍                                                                | 16176/100000 [04:30<21:33, 64.79it/s]
epoch 16100  training loss: 6.798631602578098e-06

 16%|████████████▌                                                                | 16309/100000 [04:32<21:31, 64.78it/s]
epoch 16200  training loss: 0.0005753400037065148

 16%|████████████▋                                                                | 16435/100000 [04:34<21:34, 64.54it/s]
epoch 16300  training loss: 1.4132259593679919e-06
epoch 16300  clean testing loss: 2.3584582805633545
epoch 16400  training loss: 0.0004974502953700721

 17%|████████████▊                                                                | 16568/100000 [04:36<21:27, 64.82it/s]
epoch 16500  training loss: 5.920860601804634e-08

 17%|████████████▊                                                                | 16666/100000 [04:37<21:32, 64.46it/s]
epoch 16600  training loss: 2.3350856281467713e-05

 17%|████████████▊                                                                | 16694/100000 [04:38<21:57, 63.23it/s]
epoch 16700  training loss: 3.883030512952246e-05

 17%|█████████████▏                                                               | 17079/100000 [04:44<21:36, 63.96it/s]
epoch 16800  training loss: 9.887610758596566e-06
epoch 16800  clean testing loss: 2.374347686767578
epoch 16900  training loss: 1.8873490262194537e-05
epoch 16900  clean testing loss: 2.382855176925659
epoch 17000  training loss: 9.128842975769658e-06
epoch 17000  clean testing loss: 2.3783376216888428

 17%|█████████████▏                                                               | 17100/100000 [04:44<21:40, 63.76it/s]
epoch 17100  training loss: 0.00011951180931646377

 17%|█████████████▍                                                               | 17373/100000 [04:48<21:18, 64.61it/s]
epoch 17200  training loss: 9.72383622865891e-06
epoch 17200  clean testing loss: 2.381854772567749
epoch 17300  training loss: 6.021833087288542e-06

 18%|█████████████▍                                                               | 17506/100000 [04:50<21:22, 64.30it/s]
epoch 17400  training loss: 2.3545192107121693e-06
epoch 17400  clean testing loss: 2.366454839706421
epoch 17500  training loss: 6.304398266365752e-05
epoch 17500  clean testing loss: 2.3415277004241943
epoch 17600  training loss: 1.9419258023845032e-05

 18%|█████████████▌                                                               | 17632/100000 [04:52<21:15, 64.58it/s]
epoch 17700  training loss: 1.469561811973108e-05

 18%|█████████████▋                                                               | 17765/100000 [04:54<21:17, 64.36it/s]
epoch 17800  training loss: 3.536073336363188e-06

 18%|█████████████▊                                                               | 17891/100000 [04:56<21:14, 64.45it/s]
epoch 17900  training loss: 7.53721542423591e-06
epoch 17900  clean testing loss: 2.4015958309173584
epoch 18000  training loss: 3.5811935958918184e-06
epoch 18000  clean testing loss: 2.397268056869507

 18%|█████████████▊                                                               | 18017/100000 [04:58<21:16, 64.25it/s]
epoch 18100  training loss: 3.585723243304528e-05

 18%|█████████████▉                                                               | 18150/100000 [05:00<21:14, 64.21it/s]
epoch 18200  training loss: 0.00010007063247030601

 18%|██████████████                                                               | 18276/100000 [05:02<21:08, 64.43it/s]
epoch 18300  training loss: 0.009289263747632504
epoch 18300  clean testing loss: 1.8753540515899658
epoch 18400  training loss: 1.3824242159898859e-05

 18%|██████████████▏                                                              | 18395/100000 [05:04<21:07, 64.40it/s]
epoch 18500  training loss: 0.00017110582848545164
epoch 18500  clean testing loss: 2.218925952911377
epoch 18600  training loss: 6.908920795467566e-07
epoch 18600  clean testing loss: 2.215754747390747
epoch 18700  training loss: 4.611279564414872e-06

 19%|██████████████▍                                                              | 18703/100000 [05:09<21:06, 64.21it/s]
epoch 18800  training loss: 1.1613352398853749e-05

 19%|██████████████▍                                                              | 18829/100000 [05:11<20:56, 64.62it/s]
epoch 18900  training loss: 0.00016495471936650574

 19%|██████████████▌                                                              | 18955/100000 [05:13<20:50, 64.82it/s]
epoch 19000  training loss: 2.0529820176307112e-05

 19%|██████████████▋                                                              | 18999/100000 [05:14<19:29, 69.29it/s]
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size10_noise1.00e-01_invop1 ...
epoch 19100  training loss: 5.881573997612577e-06

 19%|██████████████▋                                                              | 19110/100000 [05:17<21:28, 62.78it/s]
epoch 19200  training loss: 1.9590537704061717e-05

 19%|██████████████▊                                                              | 19243/100000 [05:19<20:52, 64.47it/s]
epoch 19300  training loss: 1.927452103700489e-06

 19%|██████████████▉                                                              | 19369/100000 [05:21<20:41, 64.93it/s]
epoch 19400  training loss: 2.032042721111793e-05
epoch 19400  clean testing loss: 2.2109711170196533
epoch 19500  training loss: 0.00010149364970857278

 19%|███████████████                                                              | 19495/100000 [05:23<20:50, 64.36it/s]
epoch 19600  training loss: 3.547019514371641e-05
epoch 19600  clean testing loss: 2.2096364498138428
epoch 19700  training loss: 5.628707526739163e-07
epoch 19700  clean testing loss: 2.190361738204956
epoch 19800  training loss: 1.588601116964128e-05
epoch 19800  clean testing loss: 2.1928515434265137
epoch 19900  training loss: 3.5636185202747583e-05
epoch 19900  clean testing loss: 2.1887340545654297
epoch 20000  training loss: 0.0002103860751958564
epoch 20000  clean testing loss: 2.1958131790161133
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size10_noise1.00e-01_invop1 ...
epoch 20100  training loss: 3.869663032673998e-06

 20%|███████████████▍                                                             | 20097/100000 [05:33<20:52, 63.77it/s]
epoch 20200  training loss: 1.4072027454403724e-07

 20%|███████████████▌                                                             | 20244/100000 [05:35<20:41, 64.22it/s]
epoch 20300  training loss: 9.726541611598805e-05

 20%|███████████████▋                                                             | 20370/100000 [05:37<20:36, 64.39it/s]
epoch 20400  training loss: 0.00011219782754778862
epoch 20400  clean testing loss: 2.260740280151367
epoch 20500  training loss: 2.7708099992196367e-07

 21%|███████████████▊                                                             | 20503/100000 [05:39<20:49, 63.62it/s]
epoch 20600  training loss: 3.126854508650467e-08

 21%|███████████████▊                                                             | 20594/100000 [05:40<20:25, 64.78it/s]
epoch 20700  training loss: 5.386600150814047e-06

 21%|███████████████▉                                                             | 20762/100000 [05:43<20:23, 64.75it/s]
epoch 20800  training loss: 1.9795565094682388e-05

 21%|████████████████                                                             | 20888/100000 [05:45<20:21, 64.78it/s]
epoch 20900  training loss: 3.554800059646368e-06
epoch 20900  clean testing loss: 2.1497390270233154
epoch 21000  training loss: 1.4333177205116954e-05

 21%|████████████████▏                                                            | 21000/100000 [05:47<20:15, 65.02it/s]
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size10_noise1.00e-01_invop1 ...
epoch 21100  training loss: 2.0511092770902906e-06

 21%|████████████████▎                                                            | 21133/100000 [05:51<20:36, 63.76it/s]
epoch 21200  training loss: 3.706638381117955e-05

 21%|████████████████▎                                                            | 21259/100000 [05:53<20:23, 64.38it/s]
epoch 21300  training loss: 0.00019774136308114976

 21%|████████████████▍                                                            | 21392/100000 [05:55<20:18, 64.49it/s]
epoch 21400  training loss: 3.7684181734221056e-05
epoch 21400  clean testing loss: 2.096364736557007
epoch 21500  training loss: 0.0002108301705447957

 22%|████████████████▌                                                            | 21518/100000 [05:57<20:16, 64.53it/s]
epoch 21600  training loss: 9.639088602853008e-06

 22%|████████████████▋                                                            | 21651/100000 [05:59<20:28, 63.78it/s]
epoch 21700  training loss: 6.396783192030853e-06

 22%|████████████████▊                                                            | 21777/100000 [06:01<20:01, 65.09it/s]
epoch 21800  training loss: 2.1998856027494185e-05
epoch 21800  clean testing loss: 2.1248347759246826
epoch 21900  training loss: 2.70922409981722e-05

 22%|████████████████▊                                                            | 21896/100000 [06:03<20:03, 64.91it/s]
epoch 22000  training loss: 2.0610611954907654e-06
epoch 22000  clean testing loss: 2.1284148693084717

 22%|█████████████████                                                            | 22092/100000 [06:07<20:48, 62.39it/s]
epoch 22100  training loss: 6.809741989854956e-06
epoch 22100  clean testing loss: 2.095344305038452
epoch 22200  training loss: 0.00012720188533421606

 22%|█████████████████                                                            | 22197/100000 [06:09<20:06, 64.48it/s]
epoch 22300  training loss: 1.065198921423871e-05

 22%|█████████████████▏                                                           | 22351/100000 [06:11<20:01, 64.64it/s]
epoch 22400  training loss: 0.0005596486735157669

 22%|█████████████████▎                                                           | 22477/100000 [06:13<20:05, 64.31it/s]
epoch 22500  training loss: 1.34117171910475e-05
epoch 22500  clean testing loss: 2.0560989379882812
epoch 22600  training loss: 3.0424538635998033e-05

 23%|█████████████████▍                                                           | 22603/100000 [06:15<20:01, 64.39it/s]
epoch 22700  training loss: 0.0003278174845036119

 23%|█████████████████▍                                                           | 22694/100000 [06:16<19:58, 64.48it/s]
epoch 22800  training loss: 6.576758460141718e-05

 23%|█████████████████▌                                                           | 22862/100000 [06:19<20:04, 64.06it/s]
epoch 22900  training loss: 0.0001080824076780118

 23%|█████████████████▋                                                           | 22995/100000 [06:21<19:47, 64.86it/s]
epoch 23000  training loss: 1.559531824568694e-06
epoch 23000  clean testing loss: 2.0256645679473877

 23%|█████████████████▋                                                           | 23030/100000 [06:23<39:19, 32.62it/s]
epoch 23100  training loss: 4.405641811899841e-05

 23%|█████████████████▊                                                           | 23156/100000 [06:25<19:56, 64.20it/s]
epoch 23200  training loss: 1.251893991138786e-05

 23%|█████████████████▉                                                           | 23289/100000 [06:27<19:50, 64.41it/s]
epoch 23300  training loss: 0.00011152961815241724
epoch 23300  clean testing loss: 2.0366458892822266
epoch 23400  training loss: 3.59983641828876e-05

 23%|██████████████████                                                           | 23415/100000 [06:29<19:49, 64.38it/s]
epoch 23500  training loss: 0.0003912897955160588

 23%|██████████████████                                                           | 23499/100000 [06:30<19:36, 65.02it/s]
epoch 23600  training loss: 0.00016663975839037448

 24%|██████████████████▏                                                          | 23674/100000 [06:33<19:47, 64.26it/s]
epoch 23700  training loss: 0.00021332362666726112

 24%|██████████████████▎                                                          | 23800/100000 [06:35<19:54, 63.79it/s]
epoch 23800  training loss: 0.00044064383837394416
epoch 23800  clean testing loss: 2.110985517501831
epoch 23900  training loss: 3.210134673281573e-05

 24%|██████████████████▍                                                          | 23933/100000 [06:37<20:17, 62.48it/s]
epoch 24000  training loss: 0.0006919920560903847
epoch 24000  clean testing loss: 2.0760481357574463

 24%|██████████████████▌                                                          | 24059/100000 [06:39<19:41, 64.28it/s]
epoch 24100  training loss: 5.9126132327946834e-06

 24%|██████████████████▋                                                          | 24192/100000 [06:41<19:28, 64.86it/s]
epoch 24200  training loss: 3.5446960282570217e-06
epoch 24200  clean testing loss: 2.1082923412323
epoch 24300  training loss: 3.301465540062054e-06

 24%|██████████████████▋                                                          | 24311/100000 [06:43<19:26, 64.87it/s]
epoch 24400  training loss: 1.1907714906556066e-05

 24%|██████████████████▊                                                          | 24451/100000 [06:45<19:31, 64.47it/s]
epoch 24500  training loss: 5.125545067130588e-05

 25%|██████████████████▉                                                          | 24577/100000 [06:47<19:25, 64.72it/s]
epoch 24600  training loss: 2.125734135915991e-05

 25%|███████████████████                                                          | 24696/100000 [06:49<19:36, 64.03it/s]
epoch 24700  training loss: 1.6023213902371936e-05
epoch 24700  clean testing loss: 2.097271680831909
epoch 24800  training loss: 4.24672725785058e-05

 25%|███████████████████                                                          | 24836/100000 [06:51<19:26, 64.45it/s]
epoch 24900  training loss: 2.191823796238168e-06

 25%|███████████████████▏                                                         | 24962/100000 [06:53<19:44, 63.33it/s]
epoch 25000  training loss: 0.00021401683625299484
epoch 25000  clean testing loss: 2.0776782035827637

 25%|███████████████████▎                                                         | 25095/100000 [06:55<19:20, 64.55it/s]
epoch 25100  training loss: 7.358102720900206e-06
epoch 25100  clean testing loss: 2.0491151809692383
epoch 25200  training loss: 3.6376579828356626e-06

 25%|███████████████████▍                                                         | 25200/100000 [06:57<19:15, 64.72it/s]
epoch 25300  training loss: 1.94453562585295e-07
epoch 25300  clean testing loss: 2.0583913326263428
epoch 25400  training loss: 4.364414252222559e-08

 25%|███████████████████▌                                                         | 25480/100000 [07:01<19:10, 64.76it/s]
epoch 25500  training loss: 1.6698429305961326e-07
epoch 25500  clean testing loss: 2.0584490299224854
epoch 25600  training loss: 1.0756944902823307e-05

 26%|███████████████████▋                                                         | 25613/100000 [07:03<19:14, 64.44it/s]
epoch 25700  training loss: 3.272755930083804e-05

 26%|███████████████████▊                                                         | 25739/100000 [07:05<19:04, 64.91it/s]
epoch 25800  training loss: 0.00013619163655675948

 26%|███████████████████▊                                                         | 25795/100000 [07:06<19:04, 64.83it/s]
epoch 25900  training loss: 1.380387971039454e-06


 26%|████████████████████                                                         | 26096/100000 [07:14<20:48, 59.18it/s]
epoch 26000  training loss: 5.029178851145844e-07
epoch 26000  clean testing loss: 1.9583704471588135
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size10_noise1.00e-01_invop1 ...
epoch 26100  training loss: 3.301375272712903e-06

 26%|████████████████████▏                                                        | 26229/100000 [07:16<19:07, 64.30it/s]
epoch 26200  training loss: 3.976856987719657e-06
epoch 26200  clean testing loss: 1.9422909021377563
epoch 26300  training loss: 5.389005309552886e-06


 26%|████████████████████▍                                                        | 26495/100000 [07:20<19:06, 64.11it/s]
epoch 26400  training loss: 5.664352556777885e-06
epoch 26400  clean testing loss: 1.9303169250488281
epoch 26500  training loss: 4.187275806089019e-07

 27%|████████████████████▌                                                        | 26628/100000 [07:22<19:13, 63.61it/s]
epoch 26600  training loss: 4.621772973223415e-07

 27%|████████████████████▌                                                        | 26754/100000 [07:24<18:58, 64.33it/s]
epoch 26700  training loss: 2.5266551801905734e-06
epoch 26700  clean testing loss: 1.9526176452636719
epoch 26800  training loss: 5.596826667897403e-05

 27%|████████████████████▋                                                        | 26887/100000 [07:26<18:48, 64.76it/s]
epoch 26900  training loss: 1.6951898942352273e-06

 27%|████████████████████▊                                                        | 27013/100000 [07:28<19:03, 63.83it/s]
epoch 27000  training loss: 3.885855767293833e-05
epoch 27000  clean testing loss: 1.7950527667999268

 27%|████████████████████▉                                                        | 27139/100000 [07:30<18:52, 64.31it/s]
epoch 27100  training loss: 6.792011117795482e-05

 27%|████████████████████▉                                                        | 27272/100000 [07:32<18:49, 64.40it/s]
epoch 27200  training loss: 1.61821208166657e-05
epoch 27200  clean testing loss: 1.7922331094741821
epoch 27300  training loss: 6.8214076236472465e-06

 27%|█████████████████████                                                        | 27398/100000 [07:34<18:45, 64.48it/s]
epoch 27400  training loss: 2.0668164779635845e-06

 27%|█████████████████████▏                                                       | 27496/100000 [07:36<18:36, 64.91it/s]
epoch 27500  training loss: 0.00019033554417546839
epoch 27500  clean testing loss: 1.7927473783493042
epoch 27600  training loss: 1.2425007298588753e-06
epoch 27600  clean testing loss: 1.7869471311569214
epoch 27700  training loss: 0.00014198216376826167

 28%|█████████████████████▍                                                       | 27790/100000 [07:40<18:34, 64.77it/s]
epoch 27800  training loss: 6.647761620115489e-05

 28%|█████████████████████▍                                                       | 27916/100000 [07:42<18:46, 63.98it/s]
epoch 27900  training loss: 3.7216693726804806e-06
epoch 27900  clean testing loss: 1.789715051651001
epoch 28000  training loss: 1.8706150513025932e-05
epoch 28000  clean testing loss: 1.6996936798095703

 28%|█████████████████████▌                                                       | 28042/100000 [07:44<18:39, 64.27it/s]
epoch 28100  training loss: 2.4118248802551534e-06

 28%|█████████████████████▋                                                       | 28175/100000 [07:46<18:24, 65.00it/s]
epoch 28200  training loss: 8.720073196855083e-07

 28%|█████████████████████▊                                                       | 28308/100000 [07:48<18:29, 64.64it/s]
epoch 28300  training loss: 9.327346197096631e-05
epoch 28300  clean testing loss: 1.6896270513534546
epoch 28400  training loss: 2.379436045885086e-05

 28%|█████████████████████▊                                                       | 28399/100000 [07:50<18:26, 64.73it/s]
epoch 28500  training loss: 1.6126708942465484e-05
epoch 28500  clean testing loss: 1.6667217016220093
epoch 28600  training loss: 4.483329973936634e-07

 29%|██████████████████████                                                       | 28630/100000 [07:53<18:36, 63.93it/s]
epoch 28700  training loss: 3.692332029459067e-05

 29%|██████████████████████▏                                                      | 28756/100000 [07:55<18:30, 64.18it/s]
epoch 28800  training loss: 2.902991127484711e-06
epoch 28800  clean testing loss: 1.675474762916565
epoch 28900  training loss: 1.3338659300643485e-05

 29%|██████████████████████▏                                                      | 28889/100000 [07:58<18:19, 64.70it/s]
epoch 29000  training loss: 1.1609909961407539e-05

 29%|██████████████████████▎                                                      | 28994/100000 [07:59<18:18, 64.64it/s]
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size10_noise1.00e-01_invop1 ...
epoch 29100  training loss: 0.034753598272800446

 29%|██████████████████████▍                                                      | 29113/100000 [08:01<18:17, 64.57it/s]
epoch 29200  training loss: 0.0007574504124931991

 29%|██████████████████████▌                                                      | 29246/100000 [08:04<18:12, 64.75it/s]
epoch 29300  training loss: 6.033832960383734e-06

 29%|██████████████████████▌                                                      | 29372/100000 [08:05<18:16, 64.43it/s]
epoch 29400  training loss: 3.4534437531874573e-07
epoch 29400  clean testing loss: 1.8862361907958984
epoch 29500  training loss: 6.391080997758536e-09

 30%|██████████████████████▋                                                      | 29505/100000 [08:08<18:17, 64.25it/s]
epoch 29600  training loss: 4.036942335972071e-09

 30%|██████████████████████▊                                                      | 29596/100000 [08:09<18:16, 64.24it/s]
epoch 29700  training loss: 3.473760390448888e-09
epoch 29700  clean testing loss: 1.893026351928711
epoch 29800  training loss: 6.561917689396068e-05
epoch 29800  clean testing loss: 1.8914995193481445
epoch 29900  training loss: 1.6973681340459734e-05
epoch 29900  clean testing loss: 1.8867361545562744
epoch 30000  training loss: 9.271816816180944e-05
epoch 30000  clean testing loss: 1.9053938388824463

 30%|███████████████████████▏                                                     | 30039/100000 [08:18<32:11, 36.22it/s]
epoch 30100  training loss: 1.4603729141526856e-05

 30%|███████████████████████▏                                                     | 30165/100000 [08:20<18:02, 64.49it/s]
epoch 30200  training loss: 1.8993012417922728e-05
epoch 30200  clean testing loss: 1.8771251440048218
epoch 30300  training loss: 3.878577444993425e-06

 30%|███████████████████████▎                                                     | 30298/100000 [08:22<17:56, 64.72it/s]
epoch 30400  training loss: 2.3373824660666287e-05

 30%|███████████████████████▍                                                     | 30396/100000 [08:23<17:54, 64.80it/s]
epoch 30500  training loss: 2.8626038329093717e-05
epoch 30500  clean testing loss: 1.8650343418121338
epoch 30600  training loss: 2.8441585527616553e-05
epoch 30600  clean testing loss: 1.8695974349975586
epoch 30700  training loss: 5.668797166435979e-05
epoch 30700  clean testing loss: 1.8670711517333984
epoch 30800  training loss: 8.440545934718102e-05

 31%|███████████████████████▋                                                     | 30816/100000 [08:30<17:56, 64.27it/s]
epoch 30900  training loss: 4.975830597686581e-05

 31%|███████████████████████▊                                                     | 30942/100000 [08:32<17:49, 64.60it/s]
epoch 31000  training loss: 4.429096952662803e-05

 31%|███████████████████████▊                                                     | 30998/100000 [08:32<17:41, 64.99it/s]
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size10_noise1.00e-01_invop1 ...
epoch 31100  training loss: 4.801300019607879e-05

 31%|███████████████████████▉                                                     | 31109/100000 [08:42<19:40, 58.37it/s]
epoch 31200  training loss: 9.231722651747987e-05

 31%|████████████████████████                                                     | 31172/100000 [08:43<17:51, 64.26it/s]
epoch 31300  training loss: 6.660530198132619e-05

 31%|████████████████████████▏                                                    | 31333/100000 [08:46<17:42, 64.64it/s]
epoch 31400  training loss: 6.71924226480769e-06
epoch 31400  clean testing loss: 1.8751499652862549
epoch 31500  training loss: 1.6745083485147916e-05

 32%|████████████████████████▎                                                    | 31564/100000 [08:49<17:34, 64.91it/s]
epoch 31600  training loss: 1.1032262591470499e-05

 32%|████████████████████████▍                                                    | 31690/100000 [08:51<17:30, 65.00it/s]
epoch 31700  training loss: 4.395858195493929e-05

 32%|████████████████████████▌                                                    | 31823/100000 [08:53<17:36, 64.50it/s]
epoch 31800  training loss: 1.8539876691647805e-05

 32%|████████████████████████▌                                                    | 31900/100000 [08:54<17:29, 64.87it/s]
epoch 31900  training loss: 3.3130004339909647e-06
epoch 31900  clean testing loss: 1.8544248342514038
epoch 32000  training loss: 4.249517132848268e-06
epoch 32000  clean testing loss: 1.844167947769165

 32%|████████████████████████▋                                                    | 32047/100000 [08:57<17:48, 63.61it/s]
epoch 32100  training loss: 5.5349431931972504e-05

 32%|████████████████████████▊                                                    | 32180/100000 [08:59<17:21, 65.09it/s]
epoch 32200  training loss: 1.0398985068604816e-05
epoch 32200  clean testing loss: 1.86081063747406
epoch 32300  training loss: 5.026409326092107e-06
epoch 32300  clean testing loss: 1.8546234369277954
epoch 32400  training loss: 6.858322194602806e-06
epoch 32400  clean testing loss: 1.8584102392196655
epoch 32500  training loss: 2.8205378839629702e-05


 33%|█████████████████████████▏                                                   | 32635/100000 [09:06<17:20, 64.71it/s]
epoch 32600  training loss: 8.964110020315275e-05

 33%|█████████████████████████▏                                                   | 32698/100000 [09:07<17:26, 64.29it/s]
epoch 32700  training loss: 1.4934331602489692e-06
epoch 32700  clean testing loss: 1.857653021812439
epoch 32800  training loss: 3.795939846895635e-06
epoch 32800  clean testing loss: 1.850733995437622
epoch 32900  training loss: 6.290739838732406e-05

 33%|█████████████████████████▍                                                   | 32985/100000 [09:11<17:10, 65.03it/s]
epoch 33000  training loss: 5.0449027185095474e-05
epoch 33000  clean testing loss: 1.8375332355499268
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size10_noise1.00e-01_invop1 ...
epoch 33100  training loss: 1.723819150356576e-05


 33%|█████████████████████████▌                                                   | 33244/100000 [09:15<17:10, 64.80it/s]
epoch 33200  training loss: 2.9157987228245474e-05

 33%|█████████████████████████▋                                                   | 33377/100000 [09:17<17:11, 64.61it/s]
epoch 33300  training loss: 1.1279872524028178e-05

 34%|█████████████████████████▊                                                   | 33503/100000 [09:19<17:13, 64.32it/s]
epoch 33400  training loss: 2.259886787214782e-05
epoch 33400  clean testing loss: 1.827659010887146
epoch 33500  training loss: 1.681591493252199e-05

 34%|█████████████████████████▉                                                   | 33636/100000 [09:21<17:07, 64.61it/s]
epoch 33600  training loss: 3.775960794882849e-05
epoch 33600  clean testing loss: 1.824933409690857
epoch 33700  training loss: 5.189891817281023e-05

 34%|█████████████████████████▉                                                   | 33762/100000 [09:23<17:00, 64.92it/s]
epoch 33800  training loss: 3.4750672057271004e-05

 34%|██████████████████████████                                                   | 33895/100000 [09:25<17:00, 64.80it/s]
epoch 33900  training loss: 2.8021366233588196e-05
epoch 33900  clean testing loss: 1.8397254943847656
epoch 34000  training loss: 1.418298688804498e-05
epoch 34000  clean testing loss: 1.839521050453186

 34%|██████████████████████████▏                                                  | 34001/100000 [09:27<32:27, 33.89it/s]
epoch 34100  training loss: 1.451717434974853e-05

 34%|██████████████████████████▎                                                  | 34098/100000 [09:29<17:23, 63.16it/s]
epoch 34200  training loss: 2.591185420897091e-06

 34%|██████████████████████████▍                                                  | 34259/100000 [09:31<16:54, 64.83it/s]
epoch 34300  training loss: 0.00017889018636196852

 34%|██████████████████████████▍                                                  | 34392/100000 [09:33<16:59, 64.33it/s]
epoch 34400  training loss: 5.866706032975344e-06
epoch 34400  clean testing loss: 1.8349905014038086
epoch 34500  training loss: 1.3893016330257524e-05

 35%|██████████████████████████▌                                                  | 34518/100000 [09:35<16:53, 64.60it/s]
epoch 34600  training loss: 6.714687515341211e-06

 35%|██████████████████████████▋                                                  | 34651/100000 [09:37<16:56, 64.26it/s]
epoch 34700  training loss: 0.0002287049574078992

 35%|██████████████████████████▊                                                  | 34770/100000 [09:39<16:41, 65.13it/s]
epoch 34800  training loss: 5.771350970462663e-06
epoch 34800  clean testing loss: 1.8203409910202026
epoch 34900  training loss: 3.1888453122519422e-06
epoch 34900  clean testing loss: 1.8195072412490845
epoch 35000  training loss: 2.3015868464426603e-06
epoch 35000  clean testing loss: 1.817299246788025

 35%|██████████████████████████▎                                                | 35029/100000 [09:49<1:24:50, 12.76it/s]
epoch 35100  training loss: 5.573577709583333e-06

 35%|███████████████████████████                                                  | 35162/100000 [09:51<16:47, 64.36it/s]
epoch 35200  training loss: 6.0399420362955425e-06

 35%|███████████████████████████▏                                                 | 35288/100000 [09:53<16:48, 64.15it/s]
epoch 35300  training loss: 3.5192454106436344e-06

 35%|███████████████████████████▏                                                 | 35295/100000 [09:53<16:43, 64.47it/s]
epoch 35400  training loss: 2.173380153180915e-06
epoch 35400  clean testing loss: 1.8231321573257446
epoch 35500  training loss: 3.030804691661615e-06
epoch 35500  clean testing loss: 1.8242861032485962
epoch 35600  training loss: 7.116133474482922e-06
epoch 35600  clean testing loss: 1.820264220237732
epoch 35700  training loss: 7.942687261675019e-06
epoch 35700  clean testing loss: 1.8228434324264526
epoch 35800  training loss: 0.0011184230679646134
epoch 35800  clean testing loss: 1.8556807041168213
epoch 35900  training loss: 1.2276981351533323e-06

 36%|███████████████████████████▋                                                 | 35897/100000 [10:03<16:28, 64.84it/s]
epoch 36000  training loss: 3.756138312382973e-06
epoch 36000  clean testing loss: 1.830527901649475

 36%|███████████████████████████▊                                                 | 36093/100000 [10:09<17:54, 59.49it/s]
epoch 36100  training loss: 0.00029732511029578745
epoch 36100  clean testing loss: 1.831055998802185
epoch 36200  training loss: 6.753781235602219e-06


 36%|████████████████████████████                                                 | 36485/100000 [10:15<16:18, 64.93it/s]
epoch 36300  training loss: 2.234837438663817e-06
epoch 36300  clean testing loss: 1.8359262943267822
epoch 36400  training loss: 2.7442488317319658e-06
epoch 36400  clean testing loss: 1.833128809928894
epoch 36500  training loss: 1.613019958313089e-05

 37%|████████████████████████████▏                                                | 36548/100000 [10:16<16:16, 64.99it/s]
epoch 36600  training loss: 7.68817335483618e-06
epoch 36600  clean testing loss: 1.8375288248062134
epoch 36700  training loss: 3.699512035382213e-06
epoch 36700  clean testing loss: 1.8374565839767456
epoch 36800  training loss: 9.187450814351905e-06


 37%|████████████████████████████▍                                                | 36898/100000 [10:21<16:17, 64.56it/s]
epoch 36900  training loss: 6.126062544353772e-06

 37%|████████████████████████████▌                                                | 37094/100000 [10:24<16:30, 63.48it/s]
epoch 37000  training loss: 8.79600611369824e-06
epoch 37000  clean testing loss: 1.8159878253936768
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size10_noise1.00e-01_invop1 ...
epoch 37100  training loss: 5.3834505706618074e-06

epoch 37100  clean testing loss: 1.8190730810165405
epoch 37200  training loss: 5.926874109718483e-06
epoch 37200  clean testing loss: 1.8141605854034424
epoch 37300  training loss: 6.083297648729058e-06
epoch 37300  clean testing loss: 1.8153698444366455
epoch 37400  training loss: 7.319724772969494e-06
epoch 37400  clean testing loss: 1.814095377922058
epoch 37500  training loss: 6.358438986353576e-05
epoch 37500  clean testing loss: 1.8142677545547485
epoch 37600  training loss: 1.1788306437665597e-05
epoch 37600  clean testing loss: 1.8372068405151367
epoch 37700  training loss: 2.5222050226147985e-06
epoch 37700  clean testing loss: 1.836436152458191
epoch 37800  training loss: 0.00019582365348469466
epoch 37800  clean testing loss: 1.8337020874023438
epoch 37900  training loss: 7.345873655140167e-06

 38%|█████████████████████████████▎                                               | 38039/100000 [10:39<15:58, 64.62it/s]
epoch 38000  training loss: 7.3586211328802165e-06
epoch 38000  clean testing loss: 1.8148895502090454

 38%|█████████████████████████████▍                                               | 38165/100000 [10:41<15:51, 64.99it/s]
epoch 38100  training loss: 4.778155016538221e-06

 38%|█████████████████████████████▍                                               | 38200/100000 [10:41<16:02, 64.19it/s]
epoch 38200  training loss: 5.776174020866165e-06
epoch 38200  clean testing loss: 1.8145198822021484
epoch 38300  training loss: 3.3197591164935147e-06
epoch 38300  clean testing loss: 1.8151711225509644
epoch 38400  training loss: 5.20950698046363e-06
epoch 38400  clean testing loss: 1.8138586282730103
epoch 38500  training loss: 0.00017991090135183185
epoch 38500  clean testing loss: 1.8115123510360718
epoch 38600  training loss: 1.1371133041393477e-05
epoch 38600  clean testing loss: 1.8158683776855469
epoch 38700  training loss: 9.70020755630685e-06

 39%|█████████████████████████████▊                                               | 38781/100000 [10:50<15:44, 64.82it/s]
epoch 38800  training loss: 5.834311195940245e-06
epoch 38800  clean testing loss: 1.8115028142929077
epoch 38900  training loss: 5.301307282934431e-06

 39%|█████████████████████████████▉                                               | 38914/100000 [10:53<15:52, 64.11it/s]
epoch 39000  training loss: 6.311016022664262e-06
epoch 39000  clean testing loss: 1.8179107904434204

 39%|██████████████████████████████                                               | 39040/100000 [10:54<15:41, 64.75it/s]
epoch 39100  training loss: 9.337147639598697e-05

 39%|██████████████████████████████                                               | 39096/100000 [10:55<15:38, 64.88it/s]
epoch 39200  training loss: 3.378287829036708e-06

 39%|██████████████████████████████▎                                              | 39299/100000 [10:58<15:38, 64.66it/s]
epoch 39300  training loss: 4.091119535587495e-06
epoch 39300  clean testing loss: 1.8151601552963257
epoch 39400  training loss: 5.9297461120877415e-05

 39%|██████████████████████████████▎                                              | 39397/100000 [11:00<15:31, 65.08it/s]
epoch 39500  training loss: 0.000112885121779982
epoch 39500  clean testing loss: 1.8134359121322632
epoch 39600  training loss: 8.237495057983324e-05

 40%|██████████████████████████████▌                                              | 39691/100000 [11:05<15:32, 64.69it/s]
epoch 39700  training loss: 8.420970516453963e-06
epoch 39700  clean testing loss: 1.8234922885894775
epoch 39800  training loss: 6.7007872530666646e-06

 40%|██████████████████████████████▋                                              | 39796/100000 [11:06<15:27, 64.94it/s]
epoch 39900  training loss: 3.4634285839274526e-06
epoch 39900  clean testing loss: 1.8247817754745483
epoch 40000  training loss: 6.254681647988036e-05
epoch 40000  clean testing loss: 1.8217188119888306

 40%|██████████████████████████████▊                                              | 40076/100000 [11:11<15:28, 64.52it/s]
epoch 40100  training loss: 5.025996415497502e-06

 40%|██████████████████████████████▉                                              | 40202/100000 [11:13<15:29, 64.30it/s]
epoch 40200  training loss: 4.485927547648316e-06
epoch 40200  clean testing loss: 1.820756196975708
epoch 40300  training loss: 0.0007754726684652269

 40%|███████████████████████████████                                              | 40300/100000 [11:14<15:23, 64.62it/s]
epoch 40400  training loss: 6.50493029752397e-06
epoch 40400  clean testing loss: 1.8568638563156128
epoch 40500  training loss: 1.8308491389618098e-09
epoch 40500  clean testing loss: 1.863244652748108
epoch 40600  training loss: 1.7690823597149574e-06

 41%|███████████████████████████████▎                                             | 40636/100000 [11:21<29:09, 33.94it/s]
epoch 40700  training loss: 1.2749727318350779e-07

 41%|███████████████████████████████▍                                             | 40762/100000 [11:23<15:24, 64.05it/s]
epoch 40800  training loss: 1.971765417607685e-09

 41%|███████████████████████████████▍                                             | 40797/100000 [11:23<15:13, 64.80it/s]
Validation loss variation < 1e-6, trained to interpolation, stop

 41%|███████████████████████████████▍                                             | 40800/100000 [11:23<16:31, 59.68it/s]