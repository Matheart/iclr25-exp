
  0%|▏                                                                                 | 698/300000 [00:01<09:25, 528.85it/s]
epoch 0  training loss: 10.444828033447266
epoch 0  clean testing loss: 0.500616729259491
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop0 ...
epoch 100  training loss: 10.118910789489746
epoch 100  clean testing loss: 0.31892743706703186
epoch 200  training loss: 9.945676803588867
epoch 200  clean testing loss: 0.18930286169052124
epoch 300  training loss: 9.846792221069336
epoch 300  clean testing loss: 0.22589118778705597
epoch 400  training loss: 9.734204292297363
epoch 400  clean testing loss: 0.31420785188674927
epoch 500  training loss: 9.61524486541748
epoch 500  clean testing loss: 0.40821534395217896
epoch 600  training loss: 9.476183891296387
epoch 600  clean testing loss: 0.5219691395759583
epoch 700  training loss: 9.310754776000977

  1%|▍                                                                                | 1721/300000 [00:03<10:14, 485.75it/s]
epoch 800  training loss: 9.101107597351074
epoch 800  clean testing loss: 0.7706317901611328
epoch 900  training loss: 8.863154411315918
epoch 900  clean testing loss: 0.9038758873939514
epoch 1000  training loss: 8.629779815673828
epoch 1000  clean testing loss: 1.0819041728973389
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop0 ...
epoch 1100  training loss: 8.413138389587402
epoch 1100  clean testing loss: 1.2167761325836182
epoch 1200  training loss: 8.21237850189209
epoch 1200  clean testing loss: 1.3608328104019165
epoch 1300  training loss: 8.024914741516113
epoch 1300  clean testing loss: 1.5030268430709839
epoch 1400  training loss: 7.8527679443359375
epoch 1400  clean testing loss: 1.6048941612243652
epoch 1500  training loss: 7.676555156707764
epoch 1500  clean testing loss: 1.741816759109497
epoch 1600  training loss: 7.522411346435547
epoch 1600  clean testing loss: 1.8995007276535034
epoch 1700  training loss: 7.369278907775879
epoch 1700  clean testing loss: 1.97356116771698
epoch 1800  training loss: 7.222456455230713

  1%|▊                                                                                | 2799/300000 [00:05<09:16, 534.47it/s]
epoch 1900  training loss: 7.097125053405762
epoch 1900  clean testing loss: 2.291884183883667
epoch 2000  training loss: 6.9618000984191895
epoch 2000  clean testing loss: 2.3575122356414795
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop0 ...
epoch 2100  training loss: 6.843167781829834
epoch 2100  clean testing loss: 2.500720739364624
epoch 2200  training loss: 6.734260082244873
epoch 2200  clean testing loss: 2.6471164226531982
epoch 2300  training loss: 6.617579460144043
epoch 2300  clean testing loss: 2.7065305709838867
epoch 2400  training loss: 6.5176472663879395
epoch 2400  clean testing loss: 2.8590753078460693
epoch 2500  training loss: 6.435615539550781
epoch 2500  clean testing loss: 2.8935651779174805
epoch 2600  training loss: 6.334583282470703
epoch 2600  clean testing loss: 3.0313892364501953
epoch 2700  training loss: 6.267115116119385
epoch 2700  clean testing loss: 3.195830821990967
epoch 2800  training loss: 6.195606708526611

  1%|█                                                                                | 3878/300000 [00:07<09:15, 532.98it/s]
epoch 2900  training loss: 6.11892032623291
epoch 2900  clean testing loss: 3.370820999145508
epoch 3000  training loss: 6.034521579742432
epoch 3000  clean testing loss: 3.4032442569732666
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop0 ...
epoch 3100  training loss: 5.9584174156188965
epoch 3100  clean testing loss: 3.4923360347747803
epoch 3200  training loss: 5.898220062255859
epoch 3200  clean testing loss: 3.5693414211273193
epoch 3300  training loss: 5.842574596405029
epoch 3300  clean testing loss: 3.6268560886383057
epoch 3400  training loss: 5.785811424255371
epoch 3400  clean testing loss: 3.754013776779175
epoch 3500  training loss: 5.736392021179199
epoch 3500  clean testing loss: 3.7767693996429443
epoch 3600  training loss: 5.687845706939697
epoch 3600  clean testing loss: 3.907628297805786
epoch 3700  training loss: 5.6349592208862305
epoch 3700  clean testing loss: 3.901075839996338
epoch 3800  training loss: 5.591939926147461
epoch 3800  clean testing loss: 4.050036907196045
epoch 3900  training loss: 5.542018890380859

  2%|█▎                                                                               | 4958/300000 [00:09<09:14, 532.52it/s]
epoch 4000  training loss: 5.488686561584473
epoch 4000  clean testing loss: 4.145711421966553
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop0 ...
epoch 4100  training loss: 5.447181701660156
epoch 4100  clean testing loss: 4.170350551605225
epoch 4200  training loss: 5.408430099487305
epoch 4200  clean testing loss: 4.230648517608643
epoch 4300  training loss: 5.376734256744385
epoch 4300  clean testing loss: 4.345281600952148
epoch 4400  training loss: 5.318531036376953
epoch 4400  clean testing loss: 4.3443427085876465
epoch 4500  training loss: 5.279524803161621
epoch 4500  clean testing loss: 4.406207084655762
epoch 4600  training loss: 5.249279499053955
epoch 4600  clean testing loss: 4.428074359893799
epoch 4700  training loss: 5.213555812835693
epoch 4700  clean testing loss: 4.5517659187316895
epoch 4800  training loss: 5.1610822677612305
epoch 4800  clean testing loss: 4.533499717712402
epoch 4900  training loss: 5.121911525726318
epoch 4900  clean testing loss: 4.627859592437744
epoch 5000  training loss: 5.0986809730529785
epoch 5000  clean testing loss: 4.62373161315918

  2%|█▌                                                                               | 6002/300000 [00:11<09:04, 539.87it/s]
epoch 5100  training loss: 5.056577205657959
epoch 5100  clean testing loss: 4.737395286560059
epoch 5200  training loss: 5.0200982093811035
epoch 5200  clean testing loss: 4.714538097381592
epoch 5300  training loss: 4.983257293701172
epoch 5300  clean testing loss: 4.829333305358887
epoch 5400  training loss: 4.948869228363037
epoch 5400  clean testing loss: 4.810147762298584
epoch 5500  training loss: 4.907031536102295
epoch 5500  clean testing loss: 4.88307523727417
epoch 5600  training loss: 4.874545097351074
epoch 5600  clean testing loss: 4.915860652923584
epoch 5700  training loss: 4.84441614151001
epoch 5700  clean testing loss: 4.960546493530273
epoch 5800  training loss: 4.814720153808594
epoch 5800  clean testing loss: 5.006772994995117
epoch 5900  training loss: 4.786505222320557
epoch 5900  clean testing loss: 5.030068874359131
epoch 6000  training loss: 4.753213405609131
epoch 6000  clean testing loss: 5.067152500152588
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop0 ...
epoch 6100  training loss: 4.7285614013671875

  2%|█▉                                                                               | 7102/300000 [00:13<09:02, 540.20it/s]
epoch 6200  training loss: 4.704321384429932
epoch 6200  clean testing loss: 5.118483543395996
epoch 6300  training loss: 4.677929401397705
epoch 6300  clean testing loss: 5.173835754394531
epoch 6400  training loss: 4.660041809082031
epoch 6400  clean testing loss: 5.171625137329102
epoch 6500  training loss: 4.631439208984375
epoch 6500  clean testing loss: 5.235418796539307
epoch 6600  training loss: 4.6046624183654785
epoch 6600  clean testing loss: 5.273233413696289
epoch 6700  training loss: 4.584432601928711
epoch 6700  clean testing loss: 5.331230640411377
epoch 6800  training loss: 4.544503211975098
epoch 6800  clean testing loss: 5.325934886932373
epoch 6900  training loss: 4.534358501434326
epoch 6900  clean testing loss: 5.3239617347717285
epoch 7000  training loss: 4.502983570098877
epoch 7000  clean testing loss: 5.394145965576172
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop0 ...
epoch 7100  training loss: 4.471607685089111

  2%|██                                                                               | 7481/300000 [00:14<09:09, 532.42it/s]
epoch 7200  training loss: 4.4543681144714355
epoch 7200  clean testing loss: 5.410438537597656
epoch 7300  training loss: 4.429515361785889
epoch 7300  clean testing loss: 5.464614391326904
epoch 7400  training loss: 4.399475574493408
epoch 7400  clean testing loss: 5.495320796966553
epoch 7500  training loss: 4.383153438568115

  3%|██▍                                                                              | 8837/300000 [00:21<09:33, 507.43it/s]
epoch 7600  training loss: 4.363008975982666
epoch 7600  clean testing loss: 5.573090076446533
epoch 7700  training loss: 4.338881015777588
epoch 7700  clean testing loss: 5.527379989624023
epoch 7800  training loss: 4.300352573394775
epoch 7800  clean testing loss: 5.570240497589111
epoch 7900  training loss: 4.294025421142578
epoch 7900  clean testing loss: 5.580250263214111
epoch 8000  training loss: 4.273525238037109
epoch 8000  clean testing loss: 5.675872325897217
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop0 ...
epoch 8100  training loss: 4.239677906036377
epoch 8100  clean testing loss: 5.628259181976318
epoch 8200  training loss: 4.212377548217773
epoch 8200  clean testing loss: 5.660423278808594
epoch 8300  training loss: 4.196769714355469
epoch 8300  clean testing loss: 5.669307708740234
epoch 8400  training loss: 4.192901611328125
epoch 8400  clean testing loss: 5.759740352630615
epoch 8500  training loss: 4.155210971832275
epoch 8500  clean testing loss: 5.722764015197754
epoch 8600  training loss: 4.132154941558838
epoch 8600  clean testing loss: 5.769561767578125
epoch 8700  training loss: 4.112806797027588
epoch 8700  clean testing loss: 5.779778480529785
epoch 8800  training loss: 4.087693691253662

  3%|██▋                                                                              | 9877/300000 [00:22<08:56, 540.59it/s]
epoch 8900  training loss: 4.075936317443848
epoch 8900  clean testing loss: 5.82428503036499
epoch 9000  training loss: 4.062871932983398
epoch 9000  clean testing loss: 5.877089500427246
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop0 ...
epoch 9100  training loss: 4.029413223266602
epoch 9100  clean testing loss: 5.876070499420166
epoch 9200  training loss: 4.012807369232178
epoch 9200  clean testing loss: 5.927850246429443
epoch 9300  training loss: 3.9960412979125977
epoch 9300  clean testing loss: 5.94187593460083
epoch 9400  training loss: 3.9843392372131348
epoch 9400  clean testing loss: 5.990600109100342
epoch 9500  training loss: 3.958160877227783
epoch 9500  clean testing loss: 5.9929327964782715
epoch 9600  training loss: 3.942660093307495
epoch 9600  clean testing loss: 6.007989406585693
epoch 9700  training loss: 3.9310250282287598
epoch 9700  clean testing loss: 6.073917388916016
epoch 9800  training loss: 3.9171218872070312
epoch 9800  clean testing loss: 6.056342124938965
epoch 9900  training loss: 3.9041025638580322

  4%|██▉                                                                             | 10962/300000 [00:25<08:56, 538.56it/s]
epoch 10000  training loss: 3.877589464187622
epoch 10000  clean testing loss: 6.116466999053955
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop0 ...
epoch 10100  training loss: 3.8566014766693115
epoch 10100  clean testing loss: 6.157275199890137
epoch 10200  training loss: 3.8442790508270264
epoch 10200  clean testing loss: 6.166598796844482
epoch 10300  training loss: 3.829906940460205
epoch 10300  clean testing loss: 6.226004600524902
epoch 10400  training loss: 3.8089723587036133
epoch 10400  clean testing loss: 6.224071502685547
epoch 10500  training loss: 3.788457155227661
epoch 10500  clean testing loss: 6.26684045791626
epoch 10600  training loss: 3.7728302478790283
epoch 10600  clean testing loss: 6.286269664764404
epoch 10700  training loss: 3.760132074356079
epoch 10700  clean testing loss: 6.334743976593018
epoch 10800  training loss: 3.742588520050049
epoch 10800  clean testing loss: 6.343120574951172
epoch 10900  training loss: 3.733407497406006
epoch 10900  clean testing loss: 6.366936683654785
epoch 11000  training loss: 3.727283000946045
epoch 11000  clean testing loss: 6.438617706298828

  4%|███▏                                                                            | 12054/300000 [00:27<08:59, 533.29it/s]
epoch 11100  training loss: 3.705697536468506
epoch 11100  clean testing loss: 6.424742698669434
epoch 11200  training loss: 3.6843698024749756
epoch 11200  clean testing loss: 6.466941833496094
epoch 11300  training loss: 3.6776578426361084
epoch 11300  clean testing loss: 6.513828277587891
epoch 11400  training loss: 3.6582186222076416
epoch 11400  clean testing loss: 6.509710311889648
epoch 11500  training loss: 3.646056890487671
epoch 11500  clean testing loss: 6.526970386505127
epoch 11600  training loss: 3.634485960006714
epoch 11600  clean testing loss: 6.547426223754883
epoch 11700  training loss: 3.625535726547241
epoch 11700  clean testing loss: 6.5809006690979
epoch 11800  training loss: 3.603215217590332
epoch 11800  clean testing loss: 6.609647750854492
epoch 11900  training loss: 3.5860910415649414
epoch 11900  clean testing loss: 6.647568702697754
epoch 12000  training loss: 3.5726826190948486
epoch 12000  clean testing loss: 6.677091598510742
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop0 ...
epoch 12100  training loss: 3.560974359512329

  4%|███▌                                                                            | 13144/300000 [00:29<08:54, 536.79it/s]
epoch 12200  training loss: 3.550015449523926
epoch 12200  clean testing loss: 6.715924263000488
epoch 12300  training loss: 3.5410571098327637
epoch 12300  clean testing loss: 6.734750270843506
epoch 12400  training loss: 3.5295796394348145
epoch 12400  clean testing loss: 6.775271415710449
epoch 12500  training loss: 3.5239219665527344
epoch 12500  clean testing loss: 6.775604724884033
epoch 12600  training loss: 3.502410888671875
epoch 12600  clean testing loss: 6.81384801864624
epoch 12700  training loss: 3.4999618530273438
epoch 12700  clean testing loss: 6.848440170288086
epoch 12800  training loss: 3.4873645305633545
epoch 12800  clean testing loss: 6.84263801574707
epoch 12900  training loss: 3.4662585258483887
epoch 12900  clean testing loss: 6.874997138977051
epoch 13000  training loss: 3.4631593227386475
epoch 13000  clean testing loss: 6.911665439605713
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop0 ...
epoch 13100  training loss: 3.448004961013794
epoch 13100  clean testing loss: 6.9215989112854
epoch 13200  training loss: 3.440227746963501

  5%|███▊                                                                            | 14236/300000 [00:31<08:51, 537.66it/s]
epoch 13300  training loss: 3.421372652053833
epoch 13300  clean testing loss: 6.9496870040893555
epoch 13400  training loss: 3.418672561645508
epoch 13400  clean testing loss: 7.010046482086182
epoch 13500  training loss: 3.3969874382019043
epoch 13500  clean testing loss: 6.998658657073975
epoch 13600  training loss: 3.3983354568481445
epoch 13600  clean testing loss: 7.025864601135254
epoch 13700  training loss: 3.380056858062744
epoch 13700  clean testing loss: 7.044950485229492
epoch 13800  training loss: 3.3746941089630127
epoch 13800  clean testing loss: 7.096695423126221
epoch 13900  training loss: 3.3560328483581543
epoch 13900  clean testing loss: 7.09931755065918
epoch 14000  training loss: 3.348255157470703
epoch 14000  clean testing loss: 7.122499465942383
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop0 ...
epoch 14100  training loss: 3.3356704711914062
epoch 14100  clean testing loss: 7.143049240112305
epoch 14200  training loss: 3.3218777179718018

  5%|████                                                                            | 15326/300000 [00:33<08:48, 538.44it/s]
epoch 14300  training loss: 3.3115453720092773
epoch 14300  clean testing loss: 7.192309856414795
epoch 14400  training loss: 3.306731700897217
epoch 14400  clean testing loss: 7.222458362579346
epoch 14500  training loss: 3.3008813858032227
epoch 14500  clean testing loss: 7.243048191070557
epoch 14600  training loss: 3.2798023223876953
epoch 14600  clean testing loss: 7.2303643226623535
epoch 14700  training loss: 3.278311252593994
epoch 14700  clean testing loss: 7.246774196624756
epoch 14800  training loss: 3.2609894275665283
epoch 14800  clean testing loss: 7.265469074249268
epoch 14900  training loss: 3.246979236602783
epoch 14900  clean testing loss: 7.294618606567383
epoch 15000  training loss: 3.2535457611083984
epoch 15000  clean testing loss: 7.327773571014404
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop0 ...
epoch 15100  training loss: 3.2282052040100098
epoch 15100  clean testing loss: 7.325279712677002
epoch 15200  training loss: 3.2200231552124023
epoch 15200  clean testing loss: 7.34663200378418
epoch 15300  training loss: 3.211704730987549

  5%|████▎                                                                           | 16359/300000 [00:35<08:46, 539.14it/s]
epoch 15400  training loss: 3.205383062362671
epoch 15400  clean testing loss: 7.381452560424805
epoch 15500  training loss: 3.1946475505828857
epoch 15500  clean testing loss: 7.403492450714111
epoch 15600  training loss: 3.186896562576294
epoch 15600  clean testing loss: 7.410015106201172
epoch 15700  training loss: 3.1748647689819336
epoch 15700  clean testing loss: 7.438676834106445
epoch 15800  training loss: 3.1679346561431885
epoch 15800  clean testing loss: 7.452056884765625
epoch 15900  training loss: 3.1674482822418213
epoch 15900  clean testing loss: 7.464261531829834
epoch 16000  training loss: 3.1512045860290527
epoch 16000  clean testing loss: 7.500096797943115
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop0 ...
epoch 16100  training loss: 3.13753080368042
epoch 16100  clean testing loss: 7.504728317260742
epoch 16200  training loss: 3.13608455657959
epoch 16200  clean testing loss: 7.531124591827393
epoch 16300  training loss: 3.1219239234924316
epoch 16300  clean testing loss: 7.5588178634643555
epoch 16400  training loss: 3.113997220993042

  6%|████▋                                                                           | 17452/300000 [00:37<08:42, 540.92it/s]
epoch 16500  training loss: 3.1090621948242188
epoch 16500  clean testing loss: 7.580770015716553
epoch 16600  training loss: 3.095613479614258
epoch 16600  clean testing loss: 7.616488933563232
epoch 16700  training loss: 3.084531545639038
epoch 16700  clean testing loss: 7.622563362121582
epoch 16800  training loss: 3.0812129974365234
epoch 16800  clean testing loss: 7.646121501922607
epoch 16900  training loss: 3.0722222328186035
epoch 16900  clean testing loss: 7.680797100067139
epoch 17000  training loss: 3.063854217529297
epoch 17000  clean testing loss: 7.692147254943848
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop0 ...
epoch 17100  training loss: 3.054295301437378
epoch 17100  clean testing loss: 7.692230701446533
epoch 17200  training loss: 3.0433263778686523
epoch 17200  clean testing loss: 7.7105393409729
epoch 17300  training loss: 3.0420548915863037
epoch 17300  clean testing loss: 7.747189998626709
epoch 17400  training loss: 3.0286364555358887
epoch 17400  clean testing loss: 7.743447303771973
epoch 17500  training loss: 3.0194969177246094

  6%|████▊                                                                           | 18055/300000 [00:38<08:44, 537.17it/s]
epoch 17600  training loss: 3.012273073196411
epoch 17600  clean testing loss: 7.783393383026123
epoch 17700  training loss: 3.0105831623077393
epoch 17700  clean testing loss: 7.819542407989502
epoch 17800  training loss: 2.99963116645813
epoch 17800  clean testing loss: 7.817251205444336
epoch 17900  training loss: 2.9869673252105713
epoch 17900  clean testing loss: 7.836019515991211
epoch 18000  training loss: 2.9788951873779297
epoch 18000  clean testing loss: 7.852304458618164
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop0 ...
epoch 18100  training loss: 2.9721901416778564

  7%|█████▎                                                                          | 20021/300000 [00:45<08:52, 525.66it/s]
epoch 18200  training loss: 2.9661855697631836
epoch 18200  clean testing loss: 7.882927894592285
epoch 18300  training loss: 2.960376024246216
epoch 18300  clean testing loss: 7.895811080932617
epoch 18400  training loss: 2.9543466567993164
epoch 18400  clean testing loss: 7.9117631912231445
epoch 18500  training loss: 2.9505231380462646
epoch 18500  clean testing loss: 7.941910266876221
epoch 18600  training loss: 2.9405758380889893
epoch 18600  clean testing loss: 7.949093818664551
epoch 18700  training loss: 2.931325912475586
epoch 18700  clean testing loss: 7.96341609954834
epoch 18800  training loss: 2.9327926635742188
epoch 18800  clean testing loss: 7.976510047912598
epoch 18900  training loss: 2.9220104217529297
epoch 18900  clean testing loss: 8.005438804626465
epoch 19000  training loss: 2.913922071456909
epoch 19000  clean testing loss: 8.006400108337402
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop0 ...
epoch 19100  training loss: 2.904827356338501
epoch 19100  clean testing loss: 8.028852462768555
epoch 19200  training loss: 2.9032535552978516
epoch 19200  clean testing loss: 8.042640686035156
epoch 19300  training loss: 2.89052152633667
epoch 19300  clean testing loss: 8.055654525756836
epoch 19400  training loss: 2.882585048675537
epoch 19400  clean testing loss: 8.079540252685547
epoch 19500  training loss: 2.8824613094329834
epoch 19500  clean testing loss: 8.092996597290039
epoch 19600  training loss: 2.869616746902466
epoch 19600  clean testing loss: 8.108123779296875
epoch 19700  training loss: 2.862515687942505
epoch 19700  clean testing loss: 8.124591827392578
epoch 19800  training loss: 2.8553004264831543
epoch 19800  clean testing loss: 8.13408088684082
epoch 19900  training loss: 2.850766181945801
epoch 19900  clean testing loss: 8.160615921020508
epoch 20000  training loss: 2.8419876098632812
epoch 20000  clean testing loss: 8.166666030883789

  7%|█████▌                                                                          | 21059/300000 [00:47<08:39, 536.70it/s]
epoch 20100  training loss: 2.8359858989715576
epoch 20100  clean testing loss: 8.190848350524902
epoch 20200  training loss: 2.832819938659668
epoch 20200  clean testing loss: 8.199621200561523
epoch 20300  training loss: 2.8248164653778076
epoch 20300  clean testing loss: 8.23206901550293
epoch 20400  training loss: 2.8168375492095947
epoch 20400  clean testing loss: 8.236742973327637
epoch 20500  training loss: 2.809400796890259
epoch 20500  clean testing loss: 8.255942344665527
epoch 20600  training loss: 2.803851842880249
epoch 20600  clean testing loss: 8.272377014160156
epoch 20700  training loss: 2.8019678592681885
epoch 20700  clean testing loss: 8.286147117614746
epoch 20800  training loss: 2.79190993309021
epoch 20800  clean testing loss: 8.310592651367188
epoch 20900  training loss: 2.7914063930511475
epoch 20900  clean testing loss: 8.335640907287598
epoch 21000  training loss: 2.77778697013855
epoch 21000  clean testing loss: 8.335042953491211
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop0 ...
epoch 21100  training loss: 2.7726826667785645

  7%|█████▉                                                                          | 22157/300000 [00:49<08:35, 538.52it/s]
epoch 21200  training loss: 2.7681212425231934
epoch 21200  clean testing loss: 8.36796760559082
epoch 21300  training loss: 2.762850284576416
epoch 21300  clean testing loss: 8.384699821472168
epoch 21400  training loss: 2.759277582168579
epoch 21400  clean testing loss: 8.40092658996582
epoch 21500  training loss: 2.753648281097412
epoch 21500  clean testing loss: 8.416268348693848
epoch 21600  training loss: 2.748565673828125
epoch 21600  clean testing loss: 8.439933776855469
epoch 21700  training loss: 2.739841938018799
epoch 21700  clean testing loss: 8.458420753479004
epoch 21800  training loss: 2.734295129776001
epoch 21800  clean testing loss: 8.476754188537598
epoch 21900  training loss: 2.732449531555176
epoch 21900  clean testing loss: 8.501620292663574
epoch 22000  training loss: 2.724433422088623
epoch 22000  clean testing loss: 8.51417350769043
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop0 ...
epoch 22100  training loss: 2.7195260524749756
epoch 22100  clean testing loss: 8.53371524810791
epoch 22200  training loss: 2.714484930038452

  8%|██████▏                                                                         | 23253/300000 [00:51<08:33, 538.71it/s]
epoch 22300  training loss: 2.70871639251709
epoch 22300  clean testing loss: 8.561467170715332
epoch 22400  training loss: 2.7024357318878174
epoch 22400  clean testing loss: 8.571314811706543
epoch 22500  training loss: 2.7002222537994385
epoch 22500  clean testing loss: 8.594246864318848
epoch 22600  training loss: 2.6913886070251465
epoch 22600  clean testing loss: 8.600247383117676
epoch 22700  training loss: 2.6864020824432373
epoch 22700  clean testing loss: 8.606695175170898
epoch 22800  training loss: 2.680539846420288
epoch 22800  clean testing loss: 8.62081527709961
epoch 22900  training loss: 2.6754469871520996
epoch 22900  clean testing loss: 8.628148078918457
epoch 23000  training loss: 2.66960072517395
epoch 23000  clean testing loss: 8.641098022460938
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop0 ...
epoch 23100  training loss: 2.6680519580841064
epoch 23100  clean testing loss: 8.660513877868652
epoch 23200  training loss: 2.6615359783172607
epoch 23200  clean testing loss: 8.670316696166992
epoch 23300  training loss: 2.659332513809204

  8%|██████▍                                                                         | 24345/300000 [00:53<08:30, 539.83it/s]
epoch 23400  training loss: 2.65334153175354
epoch 23400  clean testing loss: 8.706698417663574
epoch 23500  training loss: 2.6489527225494385
epoch 23500  clean testing loss: 8.720474243164062
epoch 23600  training loss: 2.6391520500183105
epoch 23600  clean testing loss: 8.731133460998535
epoch 23700  training loss: 2.635512351989746
epoch 23700  clean testing loss: 8.746590614318848
epoch 23800  training loss: 2.6308910846710205
epoch 23800  clean testing loss: 8.758681297302246
epoch 23900  training loss: 2.6281402111053467
epoch 23900  clean testing loss: 8.779838562011719
epoch 24000  training loss: 2.621701240539551
epoch 24000  clean testing loss: 8.78352165222168
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop0 ...
epoch 24100  training loss: 2.6148970127105713
epoch 24100  clean testing loss: 8.799262046813965
epoch 24200  training loss: 2.611344575881958
epoch 24200  clean testing loss: 8.811474800109863
epoch 24300  training loss: 2.6075356006622314

  8%|██████▊                                                                         | 25442/300000 [00:55<08:28, 539.84it/s]
epoch 24400  training loss: 2.6034772396087646
epoch 24400  clean testing loss: 8.84114933013916
epoch 24500  training loss: 2.598278045654297
epoch 24500  clean testing loss: 8.857367515563965
epoch 24600  training loss: 2.5967857837677
epoch 24600  clean testing loss: 8.873076438903809
epoch 24700  training loss: 2.5917012691497803
epoch 24700  clean testing loss: 8.885393142700195
epoch 24800  training loss: 2.5863161087036133
epoch 24800  clean testing loss: 8.903250694274902
epoch 24900  training loss: 2.5821852684020996
epoch 24900  clean testing loss: 8.917202949523926
epoch 25000  training loss: 2.5764265060424805
epoch 25000  clean testing loss: 8.930280685424805
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop0 ...
epoch 25100  training loss: 2.5744001865386963
epoch 25100  clean testing loss: 8.947969436645508
epoch 25200  training loss: 2.5675196647644043
epoch 25200  clean testing loss: 8.956727981567383
epoch 25300  training loss: 2.566227674484253
epoch 25300  clean testing loss: 8.973651885986328
epoch 25400  training loss: 2.5619564056396484

  9%|███████                                                                         | 26475/300000 [00:57<08:30, 535.77it/s]
epoch 25500  training loss: 2.5577731132507324
epoch 25500  clean testing loss: 9.000314712524414
epoch 25600  training loss: 2.5517196655273438
epoch 25600  clean testing loss: 9.014791488647461
epoch 25700  training loss: 2.549514055252075
epoch 25700  clean testing loss: 9.030596733093262
epoch 25800  training loss: 2.545929193496704
epoch 25800  clean testing loss: 9.045283317565918
epoch 25900  training loss: 2.539461374282837
epoch 25900  clean testing loss: 9.058878898620605
epoch 26000  training loss: 2.5358853340148926
epoch 26000  clean testing loss: 9.07485294342041
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop0 ...
epoch 26100  training loss: 2.5340025424957275
epoch 26100  clean testing loss: 9.082945823669434
epoch 26200  training loss: 2.5308761596679688
epoch 26200  clean testing loss: 9.100373268127441
epoch 26300  training loss: 2.5257408618927
epoch 26300  clean testing loss: 9.110209465026855
epoch 26400  training loss: 2.521812915802002
epoch 26400  clean testing loss: 9.11864948272705
epoch 26500  training loss: 2.5155766010284424

  9%|███████▎                                                                        | 27557/300000 [00:59<08:26, 537.96it/s]
epoch 26600  training loss: 2.5114073753356934
epoch 26600  clean testing loss: 9.144556999206543
epoch 26700  training loss: 2.5102486610412598
epoch 26700  clean testing loss: 9.16010570526123
epoch 26800  training loss: 2.5037853717803955
epoch 26800  clean testing loss: 9.167546272277832
epoch 26900  training loss: 2.50154447555542
epoch 26900  clean testing loss: 9.182724952697754
epoch 27000  training loss: 2.4997799396514893
epoch 27000  clean testing loss: 9.19189167022705
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop0 ...
epoch 27100  training loss: 2.4927761554718018
epoch 27100  clean testing loss: 9.201372146606445
epoch 27200  training loss: 2.4896833896636963
epoch 27200  clean testing loss: 9.21131706237793
epoch 27300  training loss: 2.4868128299713135
epoch 27300  clean testing loss: 9.22196102142334
epoch 27400  training loss: 2.4832208156585693
epoch 27400  clean testing loss: 9.234457015991211
epoch 27500  training loss: 2.4803879261016846
epoch 27500  clean testing loss: 9.245017051696777
epoch 27600  training loss: 2.4766087532043457

 10%|███████▋                                                                        | 28639/300000 [01:01<08:25, 536.49it/s]
epoch 27700  training loss: 2.472844362258911
epoch 27700  clean testing loss: 9.261947631835938
epoch 27800  training loss: 2.4688212871551514
epoch 27800  clean testing loss: 9.271890640258789
epoch 27900  training loss: 2.4651846885681152
epoch 27900  clean testing loss: 9.284770011901855
epoch 28000  training loss: 2.464109420776367
epoch 28000  clean testing loss: 9.29347038269043
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop0 ...
epoch 28100  training loss: 2.4583001136779785
epoch 28100  clean testing loss: 9.306163787841797
epoch 28200  training loss: 2.454598903656006
epoch 28200  clean testing loss: 9.316326141357422
epoch 28300  training loss: 2.452237367630005
epoch 28300  clean testing loss: 9.327498435974121
epoch 28400  training loss: 2.447732925415039
epoch 28400  clean testing loss: 9.3400297164917
epoch 28500  training loss: 2.4456944465637207
epoch 28500  clean testing loss: 9.348636627197266
epoch 28600  training loss: 2.4420745372772217

 10%|███████▉                                                                        | 29719/300000 [01:03<08:23, 536.66it/s]
epoch 28700  training loss: 2.437471389770508
epoch 28700  clean testing loss: 9.374493598937988
epoch 28800  training loss: 2.432176351547241
epoch 28800  clean testing loss: 9.365594863891602
epoch 28900  training loss: 2.4292337894439697
epoch 28900  clean testing loss: 9.379457473754883
epoch 29000  training loss: 2.423354148864746
epoch 29000  clean testing loss: 9.399041175842285
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop0 ...
epoch 29100  training loss: 2.4192023277282715
epoch 29100  clean testing loss: 9.417545318603516
epoch 29200  training loss: 2.417757511138916
epoch 29200  clean testing loss: 9.432936668395996
epoch 29300  training loss: 2.412459373474121
epoch 29300  clean testing loss: 9.446227073669434
epoch 29400  training loss: 2.410916805267334
epoch 29400  clean testing loss: 9.460338592529297
epoch 29500  training loss: 2.407311201095581
epoch 29500  clean testing loss: 9.46963119506836
epoch 29600  training loss: 2.4030141830444336
epoch 29600  clean testing loss: 9.48197078704834
epoch 29700  training loss: 2.399592399597168

 10%|████████▏                                                                       | 30801/300000 [01:05<08:21, 536.62it/s]
epoch 29800  training loss: 2.3952929973602295
epoch 29800  clean testing loss: 9.506917953491211
epoch 29900  training loss: 2.3943302631378174
epoch 29900  clean testing loss: 9.522050857543945
epoch 30000  training loss: 2.3900279998779297
epoch 30000  clean testing loss: 9.53005313873291
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop0 ...
epoch 30100  training loss: 2.3859376907348633
epoch 30100  clean testing loss: 9.540549278259277
epoch 30200  training loss: 2.3833696842193604
epoch 30200  clean testing loss: 9.54997730255127
epoch 30300  training loss: 2.380746841430664
epoch 30300  clean testing loss: 9.558067321777344
epoch 30400  training loss: 2.3780124187469482
epoch 30400  clean testing loss: 9.57031536102295
epoch 30500  training loss: 2.375715494155884
epoch 30500  clean testing loss: 9.577923774719238
epoch 30600  training loss: 2.3725075721740723
epoch 30600  clean testing loss: 9.587685585021973
epoch 30700  training loss: 2.3709473609924316
epoch 30700  clean testing loss: 9.596354484558105
epoch 30800  training loss: 2.3673629760742188

 11%|████████▌                                                                       | 31882/300000 [01:07<08:19, 536.86it/s]
epoch 30900  training loss: 2.3650689125061035
epoch 30900  clean testing loss: 9.620052337646484
epoch 31000  training loss: 2.3618874549865723
epoch 31000  clean testing loss: 9.62978458404541
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop0 ...
epoch 31100  training loss: 2.358656406402588
epoch 31100  clean testing loss: 9.636544227600098
epoch 31200  training loss: 2.356306791305542
epoch 31200  clean testing loss: 9.653351783752441
epoch 31300  training loss: 2.353006601333618
epoch 31300  clean testing loss: 9.661090850830078
epoch 31400  training loss: 2.350889205932617
epoch 31400  clean testing loss: 9.670374870300293
epoch 31500  training loss: 2.3484995365142822
epoch 31500  clean testing loss: 9.68016242980957
epoch 31600  training loss: 2.3455018997192383
epoch 31600  clean testing loss: 9.68911361694336
epoch 31700  training loss: 2.3417837619781494
epoch 31700  clean testing loss: 9.701266288757324
epoch 31800  training loss: 2.3394227027893066
epoch 31800  clean testing loss: 9.711164474487305
epoch 31900  training loss: 2.336153268814087

 11%|████████▊                                                                       | 32963/300000 [01:09<08:16, 538.00it/s]
epoch 32000  training loss: 2.3343749046325684
epoch 32000  clean testing loss: 9.734619140625
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop0 ...
epoch 32100  training loss: 2.3304474353790283
epoch 32100  clean testing loss: 9.73820972442627
epoch 32200  training loss: 2.3291358947753906
epoch 32200  clean testing loss: 9.743250846862793
epoch 32300  training loss: 2.3252062797546387
epoch 32300  clean testing loss: 9.753911972045898
epoch 32400  training loss: 2.3232789039611816
epoch 32400  clean testing loss: 9.76380729675293
epoch 32500  training loss: 2.3194029331207275
epoch 32500  clean testing loss: 9.7689847946167
epoch 32600  training loss: 2.316075325012207
epoch 32600  clean testing loss: 9.7711820602417
epoch 32700  training loss: 2.313117265701294
epoch 32700  clean testing loss: 9.779891014099121
epoch 32800  training loss: 2.308126211166382
epoch 32800  clean testing loss: 9.795817375183105
epoch 32900  training loss: 2.306171178817749

 11%|█████████                                                                       | 33989/300000 [01:11<08:14, 537.51it/s]
epoch 33000  training loss: 2.3032617568969727
epoch 33000  clean testing loss: 9.815986633300781
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop0 ...
epoch 33100  training loss: 2.300027370452881
epoch 33100  clean testing loss: 9.823558807373047
epoch 33200  training loss: 2.2978978157043457
epoch 33200  clean testing loss: 9.82839298248291
epoch 33300  training loss: 2.2959253787994385
epoch 33300  clean testing loss: 9.840668678283691
epoch 33400  training loss: 2.2934110164642334
epoch 33400  clean testing loss: 9.844171524047852
epoch 33500  training loss: 2.2912189960479736
epoch 33500  clean testing loss: 9.853611946105957
epoch 33600  training loss: 2.288687229156494
epoch 33600  clean testing loss: 9.856146812438965
epoch 33700  training loss: 2.286628484725952
epoch 33700  clean testing loss: 9.869852066040039
epoch 33800  training loss: 2.2843871116638184
epoch 33800  clean testing loss: 9.876821517944336
epoch 33900  training loss: 2.2814488410949707
epoch 33900  clean testing loss: 9.88052749633789
epoch 34000  training loss: 2.2788960933685303
epoch 34000  clean testing loss: 9.889934539794922

 12%|█████████▎                                                                      | 35072/300000 [01:13<08:15, 534.64it/s]
epoch 34100  training loss: 2.2767364978790283
epoch 34100  clean testing loss: 9.898401260375977
epoch 34200  training loss: 2.2747552394866943
epoch 34200  clean testing loss: 9.910639762878418
epoch 34300  training loss: 2.271848201751709
epoch 34300  clean testing loss: 9.913372039794922
epoch 34400  training loss: 2.2694740295410156
epoch 34400  clean testing loss: 9.925627708435059
epoch 34500  training loss: 2.2673377990722656
epoch 34500  clean testing loss: 9.928451538085938
epoch 34600  training loss: 2.264707088470459
epoch 34600  clean testing loss: 9.936651229858398
epoch 34700  training loss: 2.2624945640563965
epoch 34700  clean testing loss: 9.948062896728516
epoch 34800  training loss: 2.2604668140411377
epoch 34800  clean testing loss: 9.955076217651367
epoch 34900  training loss: 2.258298873901367
epoch 34900  clean testing loss: 9.96602725982666
epoch 35000  training loss: 2.255539655685425
epoch 35000  clean testing loss: 9.975239753723145
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop0 ...
epoch 35100  training loss: 2.2533559799194336

 12%|█████████▋                                                                      | 36098/300000 [01:15<08:15, 532.31it/s]
epoch 35200  training loss: 2.2506563663482666
epoch 35200  clean testing loss: 9.992414474487305
epoch 35300  training loss: 2.2490198612213135
epoch 35300  clean testing loss: 9.998747825622559
epoch 35400  training loss: 2.24603271484375
epoch 35400  clean testing loss: 10.009618759155273
epoch 35500  training loss: 2.2426183223724365
epoch 35500  clean testing loss: 10.014854431152344
epoch 35600  training loss: 2.2403225898742676
epoch 35600  clean testing loss: 10.019180297851562
epoch 35700  training loss: 2.237912178039551
epoch 35700  clean testing loss: 10.031725883483887
epoch 35800  training loss: 2.236375331878662
epoch 35800  clean testing loss: 10.040950775146484
epoch 35900  training loss: 2.2330172061920166
epoch 35900  clean testing loss: 10.042817115783691
epoch 36000  training loss: 2.230987071990967
epoch 36000  clean testing loss: 10.055244445800781
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop0 ...
epoch 36100  training loss: 2.2277119159698486

 13%|██████████                                                                      | 37505/300000 [01:19<09:37, 454.70it/s]
epoch 36200  training loss: 2.2259912490844727
epoch 36200  clean testing loss: 10.068879127502441
epoch 36300  training loss: 2.224058151245117
epoch 36300  clean testing loss: 10.07983112335205
epoch 36400  training loss: 2.221796751022339
epoch 36400  clean testing loss: 10.084010124206543
epoch 36500  training loss: 2.219938278198242
epoch 36500  clean testing loss: 10.090989112854004
epoch 36600  training loss: 2.218043804168701
epoch 36600  clean testing loss: 10.098609924316406
epoch 36700  training loss: 2.215956687927246
epoch 36700  clean testing loss: 10.104851722717285
epoch 36800  training loss: 2.2141177654266357
epoch 36800  clean testing loss: 10.114922523498535
epoch 36900  training loss: 2.2118277549743652
epoch 36900  clean testing loss: 10.122027397155762
epoch 37000  training loss: 2.2097456455230713
epoch 37000  clean testing loss: 10.129441261291504
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop0 ...
epoch 37100  training loss: 2.2077341079711914
epoch 37100  clean testing loss: 10.13349723815918
epoch 37200  training loss: 2.2057998180389404
epoch 37200  clean testing loss: 10.141213417053223
epoch 37300  training loss: 2.2041714191436768
epoch 37300  clean testing loss: 10.144256591796875
epoch 37400  training loss: 2.201658248901367
epoch 37400  clean testing loss: 10.155203819274902
epoch 37500  training loss: 2.199563503265381

 13%|██████████▎                                                                     | 38585/300000 [01:21<08:08, 535.24it/s]
epoch 37600  training loss: 2.197465181350708
epoch 37600  clean testing loss: 10.17141056060791
epoch 37700  training loss: 2.1960387229919434
epoch 37700  clean testing loss: 10.184931755065918
epoch 37800  training loss: 2.194075345993042
epoch 37800  clean testing loss: 10.185967445373535
epoch 37900  training loss: 2.191561698913574
epoch 37900  clean testing loss: 10.197084426879883
epoch 38000  training loss: 2.1892356872558594
epoch 38000  clean testing loss: 10.203815460205078
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop0 ...
epoch 38100  training loss: 2.1869544982910156
epoch 38100  clean testing loss: 10.206978797912598
epoch 38200  training loss: 2.185324192047119
epoch 38200  clean testing loss: 10.214672088623047
epoch 38300  training loss: 2.1833221912384033
epoch 38300  clean testing loss: 10.221477508544922
epoch 38400  training loss: 2.180903911590576
epoch 38400  clean testing loss: 10.225781440734863
epoch 38500  training loss: 2.1791675090789795
epoch 38500  clean testing loss: 10.233607292175293
epoch 38600  training loss: 2.176729679107666

 13%|██████████▌                                                                     | 39665/300000 [01:23<08:05, 535.95it/s]
epoch 38700  training loss: 2.1749107837677
epoch 38700  clean testing loss: 10.244064331054688
epoch 38800  training loss: 2.1733014583587646
epoch 38800  clean testing loss: 10.251492500305176
epoch 38900  training loss: 2.1705684661865234
epoch 38900  clean testing loss: 10.261945724487305
epoch 39000  training loss: 2.169058322906494
epoch 39000  clean testing loss: 10.266521453857422
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop0 ...
epoch 39100  training loss: 2.1669652462005615
epoch 39100  clean testing loss: 10.273053169250488
epoch 39200  training loss: 2.1652541160583496
epoch 39200  clean testing loss: 10.278295516967773
epoch 39300  training loss: 2.1636571884155273
epoch 39300  clean testing loss: 10.28455638885498
epoch 39400  training loss: 2.161961555480957
epoch 39400  clean testing loss: 10.290119171142578
epoch 39500  training loss: 2.1604819297790527
epoch 39500  clean testing loss: 10.300069808959961
epoch 39600  training loss: 2.1585006713867188

 14%|██████████▊                                                                     | 40746/300000 [01:25<08:04, 534.65it/s]
epoch 39700  training loss: 2.1573574542999268
epoch 39700  clean testing loss: 10.30894947052002
epoch 39800  training loss: 2.1555368900299072
epoch 39800  clean testing loss: 10.314637184143066
epoch 39900  training loss: 2.1535747051239014
epoch 39900  clean testing loss: 10.32494831085205
epoch 40000  training loss: 2.151779890060425
epoch 40000  clean testing loss: 10.332479476928711
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop0 ...
epoch 40100  training loss: 2.1499686241149902
epoch 40100  clean testing loss: 10.338820457458496
epoch 40200  training loss: 2.148341417312622
epoch 40200  clean testing loss: 10.344656944274902
epoch 40300  training loss: 2.1470465660095215
epoch 40300  clean testing loss: 10.350024223327637
epoch 40400  training loss: 2.1449673175811768
epoch 40400  clean testing loss: 10.359299659729004
epoch 40500  training loss: 2.143533229827881
epoch 40500  clean testing loss: 10.363198280334473
epoch 40600  training loss: 2.142022132873535
epoch 40600  clean testing loss: 10.37061595916748
epoch 40700  training loss: 2.140460968017578

 14%|███████████▏                                                                    | 41772/300000 [01:27<08:01, 536.60it/s]
epoch 40800  training loss: 2.1383023262023926
epoch 40800  clean testing loss: 10.388697624206543
epoch 40900  training loss: 2.136897563934326
epoch 40900  clean testing loss: 10.393994331359863
epoch 41000  training loss: 2.1350302696228027
epoch 41000  clean testing loss: 10.4000883102417
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop0 ...
epoch 41100  training loss: 2.133643627166748
epoch 41100  clean testing loss: 10.410198211669922
epoch 41200  training loss: 2.131941080093384
epoch 41200  clean testing loss: 10.414994239807129
epoch 41300  training loss: 2.1305367946624756
epoch 41300  clean testing loss: 10.421817779541016
epoch 41400  training loss: 2.1290442943573
epoch 41400  clean testing loss: 10.42701244354248
epoch 41500  training loss: 2.1274867057800293
epoch 41500  clean testing loss: 10.437405586242676
epoch 41600  training loss: 2.125304698944092
epoch 41600  clean testing loss: 10.442106246948242
epoch 41700  training loss: 2.1238648891448975
epoch 41700  clean testing loss: 10.447885513305664
epoch 41800  training loss: 2.122267246246338

 14%|███████████▍                                                                    | 42852/300000 [01:29<08:00, 535.20it/s]
epoch 41900  training loss: 2.120640754699707
epoch 41900  clean testing loss: 10.461329460144043
epoch 42000  training loss: 2.1188478469848633
epoch 42000  clean testing loss: 10.467316627502441
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop0 ...
epoch 42100  training loss: 2.1174681186676025
epoch 42100  clean testing loss: 10.473526954650879
epoch 42200  training loss: 2.116093873977661
epoch 42200  clean testing loss: 10.481364250183105
epoch 42300  training loss: 2.1146953105926514
epoch 42300  clean testing loss: 10.487152099609375
epoch 42400  training loss: 2.1132688522338867
epoch 42400  clean testing loss: 10.491666793823242
epoch 42500  training loss: 2.1118016242980957
epoch 42500  clean testing loss: 10.49651050567627
epoch 42600  training loss: 2.110426425933838
epoch 42600  clean testing loss: 10.501959800720215
epoch 42700  training loss: 2.1090383529663086
epoch 42700  clean testing loss: 10.510841369628906
epoch 42800  training loss: 2.107395887374878
epoch 42800  clean testing loss: 10.514836311340332
epoch 42900  training loss: 2.105921745300293

 15%|███████████▋                                                                    | 43932/300000 [01:31<07:58, 534.78it/s]
epoch 43000  training loss: 2.104773998260498
epoch 43000  clean testing loss: 10.5294189453125
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop0 ...
epoch 43100  training loss: 2.1032350063323975
epoch 43100  clean testing loss: 10.535280227661133
epoch 43200  training loss: 2.1016769409179688
epoch 43200  clean testing loss: 10.541471481323242
epoch 43300  training loss: 2.1001110076904297
epoch 43300  clean testing loss: 10.545365333557129
epoch 43400  training loss: 2.098719835281372
epoch 43400  clean testing loss: 10.551701545715332
epoch 43500  training loss: 2.0977072715759277
epoch 43500  clean testing loss: 10.555912017822266
epoch 43600  training loss: 2.0959556102752686
epoch 43600  clean testing loss: 10.563002586364746
epoch 43700  training loss: 2.094703197479248
epoch 43700  clean testing loss: 10.568657875061035
epoch 43800  training loss: 2.0932416915893555
epoch 43800  clean testing loss: 10.577906608581543
epoch 43900  training loss: 2.091657876968384

 15%|████████████                                                                    | 45012/300000 [01:33<08:09, 521.42it/s]
epoch 44000  training loss: 2.0904746055603027
epoch 44000  clean testing loss: 10.585196495056152
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop0 ...
epoch 44100  training loss: 2.0887691974639893
epoch 44100  clean testing loss: 10.592961311340332
epoch 44200  training loss: 2.0877702236175537
epoch 44200  clean testing loss: 10.596692085266113
epoch 44300  training loss: 2.086160182952881
epoch 44300  clean testing loss: 10.60096549987793
epoch 44400  training loss: 2.0846545696258545
epoch 44400  clean testing loss: 10.607244491577148
epoch 44500  training loss: 2.0829763412475586
epoch 44500  clean testing loss: 10.616312980651855
epoch 44600  training loss: 2.081498861312866
epoch 44600  clean testing loss: 10.621387481689453
epoch 44700  training loss: 2.0800538063049316
epoch 44700  clean testing loss: 10.624798774719238
epoch 44800  training loss: 2.0788729190826416
epoch 44800  clean testing loss: 10.6273775100708
epoch 44900  training loss: 2.077638864517212
epoch 44900  clean testing loss: 10.637214660644531
epoch 45000  training loss: 2.076167583465576
epoch 45000  clean testing loss: 10.640124320983887

 15%|████████████▎                                                                   | 46093/300000 [01:35<07:57, 531.49it/s]
epoch 45100  training loss: 2.0745368003845215
epoch 45100  clean testing loss: 10.644390106201172
epoch 45200  training loss: 2.073401927947998
epoch 45200  clean testing loss: 10.650347709655762
epoch 45300  training loss: 2.0721848011016846
epoch 45300  clean testing loss: 10.654439926147461
epoch 45400  training loss: 2.0710787773132324
epoch 45400  clean testing loss: 10.657268524169922
epoch 45500  training loss: 2.0698184967041016
epoch 45500  clean testing loss: 10.663049697875977
epoch 45600  training loss: 2.0685908794403076
epoch 45600  clean testing loss: 10.670461654663086
epoch 45700  training loss: 2.0674619674682617
epoch 45700  clean testing loss: 10.673199653625488
epoch 45800  training loss: 2.066051721572876
epoch 45800  clean testing loss: 10.677824974060059
epoch 45900  training loss: 2.0652477741241455
epoch 45900  clean testing loss: 10.68655776977539
epoch 46000  training loss: 2.0635569095611572
epoch 46000  clean testing loss: 10.686944007873535
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop0 ...
epoch 46100  training loss: 2.062333106994629

 16%|████████████▌                                                                   | 47173/300000 [01:37<07:53, 533.59it/s]
epoch 46200  training loss: 2.0611050128936768
epoch 46200  clean testing loss: 10.698022842407227
epoch 46300  training loss: 2.0597853660583496
epoch 46300  clean testing loss: 10.703401565551758
epoch 46400  training loss: 2.058587074279785
epoch 46400  clean testing loss: 10.709948539733887
epoch 46500  training loss: 2.0573790073394775
epoch 46500  clean testing loss: 10.715621948242188
epoch 46600  training loss: 2.0561165809631348
epoch 46600  clean testing loss: 10.719910621643066
epoch 46700  training loss: 2.0549304485321045
epoch 46700  clean testing loss: 10.727362632751465
epoch 46800  training loss: 2.0537145137786865
epoch 46800  clean testing loss: 10.732259750366211
epoch 46900  training loss: 2.052220582962036
epoch 46900  clean testing loss: 10.737655639648438
epoch 47000  training loss: 2.051131248474121
epoch 47000  clean testing loss: 10.741596221923828
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop0 ...
epoch 47100  training loss: 2.0497188568115234

 16%|████████████▊                                                                   | 48253/300000 [01:39<07:50, 535.32it/s]
epoch 47200  training loss: 2.0485832691192627
epoch 47200  clean testing loss: 10.756695747375488
epoch 47300  training loss: 2.0472872257232666
epoch 47300  clean testing loss: 10.757450103759766
epoch 47400  training loss: 2.0459020137786865
epoch 47400  clean testing loss: 10.762734413146973
epoch 47500  training loss: 2.0447890758514404
epoch 47500  clean testing loss: 10.772088050842285
epoch 47600  training loss: 2.0437021255493164
epoch 47600  clean testing loss: 10.777315139770508
epoch 47700  training loss: 2.042433261871338
epoch 47700  clean testing loss: 10.7830171585083
epoch 47800  training loss: 2.0410492420196533
epoch 47800  clean testing loss: 10.786810874938965
epoch 47900  training loss: 2.0398449897766113
epoch 47900  clean testing loss: 10.795851707458496
epoch 48000  training loss: 2.0387191772460938
epoch 48000  clean testing loss: 10.797004699707031
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop0 ...
epoch 48100  training loss: 2.037485122680664
epoch 48100  clean testing loss: 10.802254676818848
epoch 48200  training loss: 2.036475658416748

 16%|█████████████▏                                                                  | 49333/300000 [01:41<07:49, 533.58it/s]
epoch 48300  training loss: 2.0354814529418945
epoch 48300  clean testing loss: 10.811480522155762
epoch 48400  training loss: 2.0345232486724854
epoch 48400  clean testing loss: 10.817370414733887
epoch 48500  training loss: 2.0335495471954346
epoch 48500  clean testing loss: 10.821887016296387
epoch 48600  training loss: 2.0324525833129883
epoch 48600  clean testing loss: 10.827337265014648
epoch 48700  training loss: 2.031493663787842
epoch 48700  clean testing loss: 10.83191204071045
epoch 48800  training loss: 2.03039813041687
epoch 48800  clean testing loss: 10.839828491210938
epoch 48900  training loss: 2.0294084548950195
epoch 48900  clean testing loss: 10.84543228149414
epoch 49000  training loss: 2.0285420417785645
epoch 49000  clean testing loss: 10.850275993347168
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop0 ...
epoch 49100  training loss: 2.0274949073791504
epoch 49100  clean testing loss: 10.859846115112305
epoch 49200  training loss: 2.0264127254486084
epoch 49200  clean testing loss: 10.865339279174805
epoch 49300  training loss: 2.02545428276062

 17%|█████████████▍                                                                  | 50359/300000 [01:43<07:45, 536.19it/s]
epoch 49400  training loss: 2.024390935897827
epoch 49400  clean testing loss: 10.873838424682617
epoch 49500  training loss: 2.0233421325683594
epoch 49500  clean testing loss: 10.881650924682617
epoch 49600  training loss: 2.022386074066162
epoch 49600  clean testing loss: 10.886621475219727
epoch 49700  training loss: 2.0213913917541504
epoch 49700  clean testing loss: 10.891976356506348
epoch 49800  training loss: 2.020477533340454
epoch 49800  clean testing loss: 10.898528099060059
epoch 49900  training loss: 2.019397020339966
epoch 49900  clean testing loss: 10.905484199523926
epoch 50000  training loss: 2.0183756351470947
epoch 50000  clean testing loss: 10.910735130310059
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop0 ...
epoch 50100  training loss: 2.0174272060394287
epoch 50100  clean testing loss: 10.917770385742188
epoch 50200  training loss: 2.016458034515381
epoch 50200  clean testing loss: 10.923359870910645
epoch 50300  training loss: 2.0153467655181885
epoch 50300  clean testing loss: 10.929515838623047
epoch 50400  training loss: 2.014234781265259

 17%|█████████████▍                                                                  | 50467/300000 [01:43<07:45, 536.21it/s]
epoch 50500  training loss: 2.013320207595825

 17%|█████████████▊                                                                  | 51980/300000 [01:53<08:04, 512.22it/s]
epoch 50600  training loss: 2.012345552444458
epoch 50600  clean testing loss: 10.943997383117676
epoch 50700  training loss: 2.011326789855957
epoch 50700  clean testing loss: 10.950155258178711
epoch 50800  training loss: 2.0102975368499756
epoch 50800  clean testing loss: 10.959166526794434
epoch 50900  training loss: 2.009394645690918
epoch 50900  clean testing loss: 10.961636543273926
epoch 51000  training loss: 2.008293867111206
epoch 51000  clean testing loss: 10.971457481384277
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop0 ...
epoch 51100  training loss: 2.0073320865631104
epoch 51100  clean testing loss: 10.974353790283203
epoch 51200  training loss: 2.0064921379089355
epoch 51200  clean testing loss: 10.978581428527832
epoch 51300  training loss: 2.00570011138916
epoch 51300  clean testing loss: 10.9834566116333
epoch 51400  training loss: 2.004777193069458
epoch 51400  clean testing loss: 10.989991188049316
epoch 51500  training loss: 2.0035886764526367
epoch 51500  clean testing loss: 10.9981689453125
epoch 51600  training loss: 2.002352237701416
epoch 51600  clean testing loss: 11.002893447875977
epoch 51700  training loss: 2.0013320446014404
epoch 51700  clean testing loss: 11.00421142578125
epoch 51800  training loss: 2.000243902206421
epoch 51800  clean testing loss: 11.008684158325195
epoch 51900  training loss: 1.9993085861206055
epoch 51900  clean testing loss: 11.015422821044922
epoch 52000  training loss: 1.9983973503112793
epoch 52000  clean testing loss: 11.020344734191895
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop0 ...
epoch 52100  training loss: 1.9974671602249146
epoch 52100  clean testing loss: 11.023036003112793
epoch 52200  training loss: 1.9965320825576782
epoch 52200  clean testing loss: 11.030144691467285
epoch 52300  training loss: 1.9955037832260132
epoch 52300  clean testing loss: 11.034260749816895
epoch 52400  training loss: 1.9945824146270752
epoch 52400  clean testing loss: 11.038302421569824
epoch 52500  training loss: 1.993650197982788
epoch 52500  clean testing loss: 11.042923927307129
epoch 52600  training loss: 1.992785930633545
epoch 52600  clean testing loss: 11.048434257507324
epoch 52700  training loss: 1.991789698600769
epoch 52700  clean testing loss: 11.054082870483398
epoch 52800  training loss: 1.9908792972564697
epoch 52800  clean testing loss: 11.059409141540527
epoch 52900  training loss: 1.990103006362915
epoch 52900  clean testing loss: 11.066226959228516
epoch 53000  training loss: 1.9891834259033203
epoch 53000  clean testing loss: 11.067618370056152

 18%|██████████████▏                                                                 | 53069/300000 [01:55<07:40, 536.11it/s]
epoch 53100  training loss: 1.9882124662399292
epoch 53100  clean testing loss: 11.07337760925293
epoch 53200  training loss: 1.9871876239776611
epoch 53200  clean testing loss: 11.080193519592285
epoch 53300  training loss: 1.9863226413726807
epoch 53300  clean testing loss: 11.0836181640625
epoch 53400  training loss: 1.9853613376617432
epoch 53400  clean testing loss: 11.089986801147461
epoch 53500  training loss: 1.9843146800994873
epoch 53500  clean testing loss: 11.094051361083984
epoch 53600  training loss: 1.9834048748016357
epoch 53600  clean testing loss: 11.099034309387207
epoch 53700  training loss: 1.9824755191802979
epoch 53700  clean testing loss: 11.104059219360352
epoch 53800  training loss: 1.9815759658813477
epoch 53800  clean testing loss: 11.10899829864502
epoch 53900  training loss: 1.9806416034698486
epoch 53900  clean testing loss: 11.113439559936523
epoch 54000  training loss: 1.9798002243041992
epoch 54000  clean testing loss: 11.116640090942383
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop0 ...
epoch 54100  training loss: 1.9790105819702148

 18%|██████████████▍                                                                 | 54110/300000 [01:57<07:37, 537.47it/s]
epoch 54200  training loss: 1.9782822132110596
epoch 54200  clean testing loss: 11.124104499816895
epoch 54300  training loss: 1.9775158166885376
epoch 54300  clean testing loss: 11.129185676574707
epoch 54400  training loss: 1.9767476320266724
epoch 54400  clean testing loss: 11.133407592773438
epoch 54500  training loss: 1.9760013818740845
epoch 54500  clean testing loss: 11.137544631958008
epoch 54600  training loss: 1.975258469581604
epoch 54600  clean testing loss: 11.142082214355469
epoch 54700  training loss: 1.9745640754699707
epoch 54700  clean testing loss: 11.146381378173828
epoch 54800  training loss: 1.9738563299179077
epoch 54800  clean testing loss: 11.149109840393066
epoch 54900  training loss: 1.9730254411697388
epoch 54900  clean testing loss: 11.154332160949707
epoch 55000  training loss: 1.9722877740859985
epoch 55000  clean testing loss: 11.157920837402344
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop0 ...
epoch 55100  training loss: 1.971524953842163
epoch 55100  clean testing loss: 11.165059089660645
epoch 55200  training loss: 1.9708400964736938

 18%|██████████████▋                                                                 | 55208/300000 [01:59<07:34, 538.58it/s]
epoch 55300  training loss: 1.9701122045516968
epoch 55300  clean testing loss: 11.17430591583252
epoch 55400  training loss: 1.9693634510040283
epoch 55400  clean testing loss: 11.178210258483887
epoch 55500  training loss: 1.9686360359191895
epoch 55500  clean testing loss: 11.182182312011719
epoch 55600  training loss: 1.9678095579147339
epoch 55600  clean testing loss: 11.186934471130371
epoch 55700  training loss: 1.9671478271484375
epoch 55700  clean testing loss: 11.192614555358887
epoch 55800  training loss: 1.9663704633712769
epoch 55800  clean testing loss: 11.197369575500488
epoch 55900  training loss: 1.965601921081543
epoch 55900  clean testing loss: 11.199507713317871
epoch 56000  training loss: 1.964869737625122
epoch 56000  clean testing loss: 11.202539443969727
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop0 ...
epoch 56100  training loss: 1.964097023010254
epoch 56100  clean testing loss: 11.209354400634766
epoch 56200  training loss: 1.9633811712265015
epoch 56200  clean testing loss: 11.211424827575684
epoch 56300  training loss: 1.962638258934021


 19%|███████████████▎                                                                | 57404/300000 [02:03<07:29, 539.92it/s]
epoch 56400  training loss: 1.9618464708328247
epoch 56400  clean testing loss: 11.221235275268555
epoch 56500  training loss: 1.9611144065856934
epoch 56500  clean testing loss: 11.22662353515625
epoch 56600  training loss: 1.960427165031433
epoch 56600  clean testing loss: 11.229844093322754
epoch 56700  training loss: 1.959660291671753
epoch 56700  clean testing loss: 11.23524284362793
epoch 56800  training loss: 1.958857536315918
epoch 56800  clean testing loss: 11.240255355834961
epoch 56900  training loss: 1.9581546783447266
epoch 56900  clean testing loss: 11.243912696838379
epoch 57000  training loss: 1.9574332237243652
epoch 57000  clean testing loss: 11.25017261505127
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop0 ...
epoch 57100  training loss: 1.95679771900177
epoch 57100  clean testing loss: 11.253238677978516
epoch 57200  training loss: 1.9562193155288696
epoch 57200  clean testing loss: 11.25629711151123
epoch 57300  training loss: 1.9555833339691162
epoch 57300  clean testing loss: 11.26046371459961
epoch 57400  training loss: 1.954987645149231

 19%|███████████████▌                                                                | 58448/300000 [02:05<07:26, 541.57it/s]
epoch 57500  training loss: 1.9544553756713867
epoch 57500  clean testing loss: 11.269885063171387
epoch 57600  training loss: 1.9537962675094604
epoch 57600  clean testing loss: 11.273456573486328
epoch 57700  training loss: 1.9531935453414917
epoch 57700  clean testing loss: 11.277104377746582
epoch 57800  training loss: 1.9526071548461914
epoch 57800  clean testing loss: 11.279664993286133
epoch 57900  training loss: 1.9519473314285278
epoch 57900  clean testing loss: 11.284635543823242
epoch 58000  training loss: 1.9512903690338135
epoch 58000  clean testing loss: 11.28875732421875
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop0 ...
epoch 58100  training loss: 1.9506689310073853
epoch 58100  clean testing loss: 11.291674613952637
epoch 58200  training loss: 1.950007438659668
epoch 58200  clean testing loss: 11.296631813049316
epoch 58300  training loss: 1.9493404626846313
epoch 58300  clean testing loss: 11.300806999206543
epoch 58400  training loss: 1.9486864805221558

 20%|███████████████▉                                                                | 59548/300000 [02:07<07:22, 543.44it/s]
epoch 58500  training loss: 1.9480363130569458
epoch 58500  clean testing loss: 11.31058406829834
epoch 58600  training loss: 1.947321891784668
epoch 58600  clean testing loss: 11.314105987548828
epoch 58700  training loss: 1.9466887712478638
epoch 58700  clean testing loss: 11.316170692443848
epoch 58800  training loss: 1.9460848569869995
epoch 58800  clean testing loss: 11.319796562194824
epoch 58900  training loss: 1.945432186126709
epoch 58900  clean testing loss: 11.325146675109863
epoch 59000  training loss: 1.9447598457336426
epoch 59000  clean testing loss: 11.32918643951416
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop0 ...
epoch 59100  training loss: 1.9441651105880737
epoch 59100  clean testing loss: 11.334434509277344
epoch 59200  training loss: 1.9434727430343628
epoch 59200  clean testing loss: 11.33742618560791
epoch 59300  training loss: 1.9428588151931763
epoch 59300  clean testing loss: 11.34162712097168
epoch 59400  training loss: 1.9422051906585693
epoch 59400  clean testing loss: 11.345518112182617
epoch 59500  training loss: 1.941577672958374

 20%|████████████████▏                                                               | 60647/300000 [02:09<07:21, 542.46it/s]
epoch 59600  training loss: 1.940943956375122
epoch 59600  clean testing loss: 11.35421371459961
epoch 59700  training loss: 1.9403607845306396
epoch 59700  clean testing loss: 11.359469413757324
epoch 59800  training loss: 1.9396692514419556
epoch 59800  clean testing loss: 11.362523078918457
epoch 59900  training loss: 1.9390630722045898
epoch 59900  clean testing loss: 11.368282318115234
epoch 60000  training loss: 1.9384589195251465
epoch 60000  clean testing loss: 11.371210098266602
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop0 ...
epoch 60100  training loss: 1.9379128217697144
epoch 60100  clean testing loss: 11.374506950378418
epoch 60200  training loss: 1.9374263286590576
epoch 60200  clean testing loss: 11.378470420837402
epoch 60300  training loss: 1.9368984699249268
epoch 60300  clean testing loss: 11.381805419921875
epoch 60400  training loss: 1.9363739490509033
epoch 60400  clean testing loss: 11.384246826171875
epoch 60500  training loss: 1.935843586921692
epoch 60500  clean testing loss: 11.388588905334473
epoch 60600  training loss: 1.9353440999984741

 21%|████████████████▍                                                               | 61747/300000 [02:11<07:19, 542.44it/s]
epoch 60700  training loss: 1.9348225593566895
epoch 60700  clean testing loss: 11.396605491638184
epoch 60800  training loss: 1.9342767000198364
epoch 60800  clean testing loss: 11.398780822753906
epoch 60900  training loss: 1.933728814125061
epoch 60900  clean testing loss: 11.402534484863281
epoch 61000  training loss: 1.9332058429718018
epoch 61000  clean testing loss: 11.40693187713623
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop0 ...
epoch 61100  training loss: 1.932699203491211
epoch 61100  clean testing loss: 11.410977363586426
epoch 61200  training loss: 1.9321295022964478
epoch 61200  clean testing loss: 11.414711952209473
epoch 61300  training loss: 1.9315909147262573
epoch 61300  clean testing loss: 11.417806625366211
epoch 61400  training loss: 1.9310396909713745
epoch 61400  clean testing loss: 11.4205961227417
epoch 61500  training loss: 1.9304498434066772
epoch 61500  clean testing loss: 11.425622940063477
epoch 61600  training loss: 1.9299075603485107
epoch 61600  clean testing loss: 11.429238319396973
epoch 61700  training loss: 1.9293421506881714

 21%|████████████████▋                                                               | 62792/300000 [02:13<07:16, 543.29it/s]
epoch 61800  training loss: 1.9288026094436646
epoch 61800  clean testing loss: 11.434640884399414
epoch 61900  training loss: 1.9283015727996826
epoch 61900  clean testing loss: 11.438854217529297
epoch 62000  training loss: 1.9277149438858032
epoch 62000  clean testing loss: 11.441651344299316
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop0 ...
epoch 62100  training loss: 1.9271798133850098
epoch 62100  clean testing loss: 11.446210861206055
epoch 62200  training loss: 1.9266680479049683
epoch 62200  clean testing loss: 11.450397491455078
epoch 62300  training loss: 1.9261314868927002
epoch 62300  clean testing loss: 11.454072952270508
epoch 62400  training loss: 1.92559814453125
epoch 62400  clean testing loss: 11.45769214630127
epoch 62500  training loss: 1.9250812530517578
epoch 62500  clean testing loss: 11.459074974060059
epoch 62600  training loss: 1.9245421886444092
epoch 62600  clean testing loss: 11.46377182006836
epoch 62700  training loss: 1.9240243434906006
epoch 62700  clean testing loss: 11.467314720153809
epoch 62800  training loss: 1.9234931468963623

 21%|█████████████████                                                               | 63892/300000 [02:15<07:13, 544.71it/s]
epoch 62900  training loss: 1.922978401184082
epoch 62900  clean testing loss: 11.47415828704834
epoch 63000  training loss: 1.9224342107772827
epoch 63000  clean testing loss: 11.478822708129883
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop0 ...
epoch 63100  training loss: 1.9219528436660767
epoch 63100  clean testing loss: 11.482122421264648
epoch 63200  training loss: 1.9215171337127686
epoch 63200  clean testing loss: 11.484983444213867
epoch 63300  training loss: 1.9210816621780396
epoch 63300  clean testing loss: 11.489084243774414
epoch 63400  training loss: 1.920636534690857
epoch 63400  clean testing loss: 11.492024421691895
epoch 63500  training loss: 1.9201856851577759
epoch 63500  clean testing loss: 11.495595932006836
epoch 63600  training loss: 1.9197337627410889
epoch 63600  clean testing loss: 11.498540878295898
epoch 63700  training loss: 1.9192875623703003
epoch 63700  clean testing loss: 11.501840591430664
epoch 63800  training loss: 1.9188450574874878
epoch 63800  clean testing loss: 11.504060745239258
epoch 63900  training loss: 1.9184110164642334

 22%|█████████████████▎                                                              | 64989/300000 [02:17<07:13, 541.65it/s]
epoch 64000  training loss: 1.9179625511169434
epoch 64000  clean testing loss: 11.511600494384766
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop0 ...
epoch 64100  training loss: 1.9174765348434448
epoch 64100  clean testing loss: 11.514022827148438
epoch 64200  training loss: 1.9170212745666504
epoch 64200  clean testing loss: 11.517067909240723
epoch 64300  training loss: 1.9165632724761963
epoch 64300  clean testing loss: 11.520383834838867
epoch 64400  training loss: 1.9161224365234375
epoch 64400  clean testing loss: 11.52242374420166
epoch 64500  training loss: 1.9156495332717896
epoch 64500  clean testing loss: 11.525893211364746
epoch 64600  training loss: 1.915198802947998
epoch 64600  clean testing loss: 11.52912712097168
epoch 64700  training loss: 1.914747714996338
epoch 64700  clean testing loss: 11.5319242477417
epoch 64800  training loss: 1.914311408996582
epoch 64800  clean testing loss: 11.535148620605469
epoch 64900  training loss: 1.913867712020874
epoch 64900  clean testing loss: 11.537116050720215
epoch 65000  training loss: 1.9134072065353394
epoch 65000  clean testing loss: 11.541044235229492

 22%|█████████████████▌                                                              | 66089/300000 [02:19<07:13, 539.80it/s]
epoch 65100  training loss: 1.9129819869995117
epoch 65100  clean testing loss: 11.543135643005371
epoch 65200  training loss: 1.9125514030456543
epoch 65200  clean testing loss: 11.545897483825684
epoch 65300  training loss: 1.9120850563049316
epoch 65300  clean testing loss: 11.549893379211426
epoch 65400  training loss: 1.911649227142334
epoch 65400  clean testing loss: 11.552769660949707
epoch 65500  training loss: 1.911210060119629
epoch 65500  clean testing loss: 11.555031776428223
epoch 65600  training loss: 1.910752296447754
epoch 65600  clean testing loss: 11.558572769165039
epoch 65700  training loss: 1.9103033542633057
epoch 65700  clean testing loss: 11.562033653259277
epoch 65800  training loss: 1.9098902940750122
epoch 65800  clean testing loss: 11.565343856811523
epoch 65900  training loss: 1.9094246625900269
epoch 65900  clean testing loss: 11.56675910949707
epoch 66000  training loss: 1.9090064764022827
epoch 66000  clean testing loss: 11.570356369018555
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop0 ...

epoch 66100  training loss: 1.9086133241653442
epoch 66100  clean testing loss: 11.572437286376953
epoch 66200  training loss: 1.9082491397857666
epoch 66200  clean testing loss: 11.574738502502441
epoch 66300  training loss: 1.907895803451538
epoch 66300  clean testing loss: 11.577397346496582
epoch 66400  training loss: 1.9075382947921753
epoch 66400  clean testing loss: 11.580082893371582
epoch 66500  training loss: 1.907153606414795
epoch 66500  clean testing loss: 11.582656860351562
epoch 66600  training loss: 1.9067747592926025
epoch 66600  clean testing loss: 11.58543586730957
epoch 66700  training loss: 1.9064050912857056
epoch 66700  clean testing loss: 11.586877822875977
epoch 66800  training loss: 1.906031608581543
epoch 66800  clean testing loss: 11.589789390563965
epoch 66900  training loss: 1.9056499004364014
epoch 66900  clean testing loss: 11.592288970947266
epoch 67000  training loss: 1.9052705764770508
epoch 67000  clean testing loss: 11.595098495483398
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop0 ...
epoch 67100  training loss: 1.9048906564712524

 23%|██████████████████▏                                                             | 68233/300000 [02:23<07:07, 541.68it/s]
epoch 67200  training loss: 1.9045237302780151
epoch 67200  clean testing loss: 11.600289344787598
epoch 67300  training loss: 1.9041392803192139
epoch 67300  clean testing loss: 11.602995872497559
epoch 67400  training loss: 1.9037576913833618
epoch 67400  clean testing loss: 11.605950355529785
epoch 67500  training loss: 1.9033807516098022
epoch 67500  clean testing loss: 11.608532905578613
epoch 67600  training loss: 1.9030040502548218
epoch 67600  clean testing loss: 11.610608100891113
epoch 67700  training loss: 1.902625560760498
epoch 67700  clean testing loss: 11.61410140991211
epoch 67800  training loss: 1.9022772312164307
epoch 67800  clean testing loss: 11.6161527633667
epoch 67900  training loss: 1.9018808603286743
epoch 67900  clean testing loss: 11.618646621704102
epoch 68000  training loss: 1.9015032052993774
epoch 68000  clean testing loss: 11.620397567749023
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop0 ...
epoch 68100  training loss: 1.9011178016662598
epoch 68100  clean testing loss: 11.624204635620117
epoch 68200  training loss: 1.9007328748703003

 23%|██████████████████▍                                                             | 69332/300000 [02:25<07:05, 542.23it/s]
epoch 68300  training loss: 1.9003738164901733
epoch 68300  clean testing loss: 11.62876033782959
epoch 68400  training loss: 1.900010585784912
epoch 68400  clean testing loss: 11.632081031799316
epoch 68500  training loss: 1.899613857269287
epoch 68500  clean testing loss: 11.63390064239502
epoch 68600  training loss: 1.8992286920547485
epoch 68600  clean testing loss: 11.636195182800293
epoch 68700  training loss: 1.8988776206970215
epoch 68700  clean testing loss: 11.63935661315918
epoch 68800  training loss: 1.898503303527832
epoch 68800  clean testing loss: 11.640990257263184
epoch 68900  training loss: 1.8981066942214966
epoch 68900  clean testing loss: 11.644083976745605
epoch 69000  training loss: 1.8977423906326294
epoch 69000  clean testing loss: 11.64695930480957
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop0 ...
epoch 69100  training loss: 1.8974097967147827
epoch 69100  clean testing loss: 11.649078369140625
epoch 69200  training loss: 1.8971102237701416
epoch 69200  clean testing loss: 11.65093994140625
epoch 69300  training loss: 1.8967996835708618

 23%|██████████████████▊                                                             | 70429/300000 [02:27<07:05, 539.81it/s]
epoch 69400  training loss: 1.896488904953003
epoch 69400  clean testing loss: 11.654401779174805
epoch 69500  training loss: 1.896188497543335
epoch 69500  clean testing loss: 11.656871795654297
epoch 69600  training loss: 1.8958715200424194
epoch 69600  clean testing loss: 11.65842056274414
epoch 69700  training loss: 1.8955579996109009
epoch 69700  clean testing loss: 11.660665512084961
epoch 69800  training loss: 1.8952292203903198
epoch 69800  clean testing loss: 11.663331031799316
epoch 69900  training loss: 1.8949005603790283
epoch 69900  clean testing loss: 11.665739059448242
epoch 70000  training loss: 1.8945882320404053
epoch 70000  clean testing loss: 11.667829513549805
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop0 ...
epoch 70100  training loss: 1.8942466974258423
epoch 70100  clean testing loss: 11.670571327209473
epoch 70200  training loss: 1.8939155340194702
epoch 70200  clean testing loss: 11.673356056213379
epoch 70300  training loss: 1.8935959339141846
epoch 70300  clean testing loss: 11.675605773925781
epoch 70400  training loss: 1.8932669162750244

 24%|███████████████████                                                             | 71528/300000 [02:29<07:01, 541.43it/s]
epoch 70500  training loss: 1.8929463624954224
epoch 70500  clean testing loss: 11.679976463317871
epoch 70600  training loss: 1.8926169872283936
epoch 70600  clean testing loss: 11.681685447692871
epoch 70700  training loss: 1.892306923866272
epoch 70700  clean testing loss: 11.684976577758789
epoch 70800  training loss: 1.8919605016708374
epoch 70800  clean testing loss: 11.68617057800293
epoch 70900  training loss: 1.8916219472885132
epoch 70900  clean testing loss: 11.688047409057617
epoch 71000  training loss: 1.8912991285324097
epoch 71000  clean testing loss: 11.689455032348633
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop0 ...
epoch 71100  training loss: 1.8909752368927002
epoch 71100  clean testing loss: 11.692279815673828
epoch 71200  training loss: 1.8906514644622803
epoch 71200  clean testing loss: 11.69398021697998
epoch 71300  training loss: 1.8903249502182007
epoch 71300  clean testing loss: 11.697283744812012
epoch 71400  training loss: 1.8900171518325806
epoch 71400  clean testing loss: 11.698030471801758
epoch 71500  training loss: 1.889670729637146

 24%|███████████████████▎                                                            | 72567/300000 [02:31<07:01, 539.96it/s]
epoch 71600  training loss: 1.8893436193466187
epoch 71600  clean testing loss: 11.702630996704102
epoch 71700  training loss: 1.889035701751709
epoch 71700  clean testing loss: 11.7049560546875
epoch 71800  training loss: 1.8887234926223755
epoch 71800  clean testing loss: 11.706709861755371
epoch 71900  training loss: 1.888395071029663
epoch 71900  clean testing loss: 11.70994758605957
epoch 72000  training loss: 1.8880689144134521
epoch 72000  clean testing loss: 11.712023735046387
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop0 ...
epoch 72100  training loss: 1.8877896070480347
epoch 72100  clean testing loss: 11.714465141296387
epoch 72200  training loss: 1.8875343799591064
epoch 72200  clean testing loss: 11.715482711791992
epoch 72300  training loss: 1.887272834777832
epoch 72300  clean testing loss: 11.717779159545898
epoch 72400  training loss: 1.8870149850845337
epoch 72400  clean testing loss: 11.719489097595215
epoch 72500  training loss: 1.8867466449737549
epoch 72500  clean testing loss: 11.722000122070312
epoch 72600  training loss: 1.8864787817001343

 25%|███████████████████▋                                                            | 73656/300000 [02:33<06:58, 540.59it/s]
epoch 72700  training loss: 1.8862210512161255
epoch 72700  clean testing loss: 11.725268363952637
epoch 72800  training loss: 1.8859496116638184
epoch 72800  clean testing loss: 11.727143287658691
epoch 72900  training loss: 1.8856770992279053
epoch 72900  clean testing loss: 11.729547500610352
epoch 73000  training loss: 1.8854080438613892
epoch 73000  clean testing loss: 11.73154067993164
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop0 ...
epoch 73100  training loss: 1.8851463794708252
epoch 73100  clean testing loss: 11.733684539794922
epoch 73200  training loss: 1.8848800659179688
epoch 73200  clean testing loss: 11.734977722167969
epoch 73300  training loss: 1.8846049308776855
epoch 73300  clean testing loss: 11.737526893615723
epoch 73400  training loss: 1.8843435049057007
epoch 73400  clean testing loss: 11.739043235778809
epoch 73500  training loss: 1.8840794563293457
epoch 73500  clean testing loss: 11.740729331970215
epoch 73600  training loss: 1.8837947845458984

 25%|███████████████████▋                                                            | 73876/300000 [02:33<06:57, 541.77it/s]
epoch 73700  training loss: 1.8835301399230957
epoch 73700  clean testing loss: 11.744563102722168
epoch 73800  training loss: 1.883280634880066
epoch 73800  clean testing loss: 11.746723175048828
epoch 73900  training loss: 1.882995843887329

 25%|███████████████████▊                                                            | 74204/300000 [02:37<28:20, 132.81it/s]
epoch 74000  training loss: 1.8827306032180786
epoch 74000  clean testing loss: 11.750319480895996
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop0 ...
epoch 74100  training loss: 1.8824621438980103
epoch 74100  clean testing loss: 11.752036094665527
epoch 74200  training loss: 1.8821979761123657

 25%|████████████████████                                                            | 75297/300000 [02:39<06:56, 538.87it/s]
epoch 74300  training loss: 1.881930947303772
epoch 74300  clean testing loss: 11.755573272705078
epoch 74400  training loss: 1.8816636800765991
epoch 74400  clean testing loss: 11.758159637451172
epoch 74500  training loss: 1.8813884258270264
epoch 74500  clean testing loss: 11.759933471679688
epoch 74600  training loss: 1.88112473487854
epoch 74600  clean testing loss: 11.76119613647461
epoch 74700  training loss: 1.8808436393737793
epoch 74700  clean testing loss: 11.763082504272461
epoch 74800  training loss: 1.8805712461471558
epoch 74800  clean testing loss: 11.765080451965332
epoch 74900  training loss: 1.8802964687347412
epoch 74900  clean testing loss: 11.766793251037598
epoch 75000  training loss: 1.8800241947174072
epoch 75000  clean testing loss: 11.769179344177246
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop0 ...
epoch 75100  training loss: 1.8798044919967651
epoch 75100  clean testing loss: 11.77029037475586
epoch 75200  training loss: 1.8795862197875977
epoch 75200  clean testing loss: 11.7716646194458
epoch 75300  training loss: 1.8793694972991943

 25%|████████████████████▎                                                           | 76394/300000 [02:41<06:52, 542.31it/s]
epoch 75400  training loss: 1.8791531324386597
epoch 75400  clean testing loss: 11.774369239807129
epoch 75500  training loss: 1.8789358139038086
epoch 75500  clean testing loss: 11.776277542114258
epoch 75600  training loss: 1.8787163496017456
epoch 75600  clean testing loss: 11.777970314025879
epoch 75700  training loss: 1.878501296043396
epoch 75700  clean testing loss: 11.779794692993164
epoch 75800  training loss: 1.8782827854156494
epoch 75800  clean testing loss: 11.78076457977295
epoch 75900  training loss: 1.8780584335327148
epoch 75900  clean testing loss: 11.782428741455078
epoch 76000  training loss: 1.877851963043213
epoch 76000  clean testing loss: 11.784063339233398
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop0 ...
epoch 76100  training loss: 1.8776228427886963
epoch 76100  clean testing loss: 11.78575611114502
epoch 76200  training loss: 1.8774080276489258
epoch 76200  clean testing loss: 11.78743839263916
epoch 76300  training loss: 1.8771840333938599
epoch 76300  clean testing loss: 11.78882884979248

epoch 76400  training loss: 1.8769617080688477
epoch 76400  clean testing loss: 11.790532112121582
epoch 76500  training loss: 1.8767473697662354
epoch 76500  clean testing loss: 11.792012214660645
epoch 76600  training loss: 1.8765288591384888
epoch 76600  clean testing loss: 11.793668746948242
epoch 76700  training loss: 1.8762972354888916
epoch 76700  clean testing loss: 11.794889450073242
epoch 76800  training loss: 1.8760846853256226
epoch 76800  clean testing loss: 11.796648025512695
epoch 76900  training loss: 1.8758642673492432
epoch 76900  clean testing loss: 11.798169136047363
epoch 77000  training loss: 1.8756531476974487
epoch 77000  clean testing loss: 11.799936294555664
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop0 ...
epoch 77100  training loss: 1.8754169940948486
epoch 77100  clean testing loss: 11.801313400268555
epoch 77200  training loss: 1.8752002716064453
epoch 77200  clean testing loss: 11.802594184875488
epoch 77300  training loss: 1.874982476234436
epoch 77300  clean testing loss: 11.804383277893066
epoch 77400  training loss: 1.8747608661651611

 26%|████████████████████▉                                                           | 78529/300000 [02:45<06:51, 538.78it/s]
epoch 77500  training loss: 1.874539852142334
epoch 77500  clean testing loss: 11.807622909545898
epoch 77600  training loss: 1.8743135929107666
epoch 77600  clean testing loss: 11.80938720703125
epoch 77700  training loss: 1.8741000890731812
epoch 77700  clean testing loss: 11.81063461303711
epoch 77800  training loss: 1.8738784790039062
epoch 77800  clean testing loss: 11.812297821044922
epoch 77900  training loss: 1.8736618757247925
epoch 77900  clean testing loss: 11.813912391662598
epoch 78000  training loss: 1.8734450340270996
epoch 78000  clean testing loss: 11.81523609161377
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop0 ...
epoch 78100  training loss: 1.8732564449310303
epoch 78100  clean testing loss: 11.816635131835938
epoch 78200  training loss: 1.8730744123458862
epoch 78200  clean testing loss: 11.817950248718262
epoch 78300  training loss: 1.872891902923584
epoch 78300  clean testing loss: 11.819111824035645
epoch 78400  training loss: 1.8727139234542847
epoch 78400  clean testing loss: 11.820549011230469
epoch 78500  training loss: 1.8725298643112183

 27%|█████████████████████▏                                                          | 79616/300000 [02:47<06:54, 531.70it/s]
epoch 78600  training loss: 1.8723546266555786
epoch 78600  clean testing loss: 11.82263469696045
epoch 78700  training loss: 1.8721694946289062
epoch 78700  clean testing loss: 11.823963165283203
epoch 78800  training loss: 1.8719828128814697
epoch 78800  clean testing loss: 11.825676918029785
epoch 78900  training loss: 1.8717999458312988
epoch 78900  clean testing loss: 11.826698303222656
epoch 79000  training loss: 1.8716219663619995
epoch 79000  clean testing loss: 11.828262329101562
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop0 ...
epoch 79100  training loss: 1.8714323043823242
epoch 79100  clean testing loss: 11.829740524291992
epoch 79200  training loss: 1.871250867843628
epoch 79200  clean testing loss: 11.831018447875977
epoch 79300  training loss: 1.8710649013519287
epoch 79300  clean testing loss: 11.83250617980957
epoch 79400  training loss: 1.8708823919296265
epoch 79400  clean testing loss: 11.833691596984863
epoch 79500  training loss: 1.8706996440887451
epoch 79500  clean testing loss: 11.835246086120605
epoch 79600  training loss: 1.8705189228057861

 27%|█████████████████████▌                                                          | 80696/300000 [02:49<06:50, 534.17it/s]
epoch 79700  training loss: 1.8703304529190063
epoch 79700  clean testing loss: 11.837775230407715
epoch 79800  training loss: 1.870148777961731
epoch 79800  clean testing loss: 11.839155197143555
epoch 79900  training loss: 1.8699629306793213
epoch 79900  clean testing loss: 11.840336799621582
epoch 80000  training loss: 1.869780421257019
epoch 80000  clean testing loss: 11.841865539550781
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop0 ...
epoch 80100  training loss: 1.8695932626724243
epoch 80100  clean testing loss: 11.843449592590332
epoch 80200  training loss: 1.8694063425064087
epoch 80200  clean testing loss: 11.844498634338379
epoch 80300  training loss: 1.869217872619629
epoch 80300  clean testing loss: 11.845672607421875
epoch 80400  training loss: 1.869034767150879
epoch 80400  clean testing loss: 11.847113609313965
epoch 80500  training loss: 1.8688526153564453
epoch 80500  clean testing loss: 11.848479270935059
epoch 80600  training loss: 1.8686718940734863
epoch 80600  clean testing loss: 11.849870681762695
epoch 80700  training loss: 1.8684864044189453

 27%|█████████████████████▊                                                          | 81739/300000 [02:51<06:42, 541.60it/s]
epoch 80800  training loss: 1.8683035373687744
epoch 80800  clean testing loss: 11.852590560913086
epoch 80900  training loss: 1.8681284189224243
epoch 80900  clean testing loss: 11.854284286499023
epoch 81000  training loss: 1.8679395914077759
epoch 81000  clean testing loss: 11.855690956115723
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop0 ...
epoch 81100  training loss: 1.8677901029586792
epoch 81100  clean testing loss: 11.856690406799316
epoch 81200  training loss: 1.867644190788269
epoch 81200  clean testing loss: 11.858055114746094
epoch 81300  training loss: 1.8674979209899902
epoch 81300  clean testing loss: 11.859282493591309
epoch 81400  training loss: 1.8673526048660278
epoch 81400  clean testing loss: 11.860153198242188
epoch 81500  training loss: 1.8672034740447998
epoch 81500  clean testing loss: 11.861215591430664
epoch 81600  training loss: 1.8670527935028076
epoch 81600  clean testing loss: 11.862760543823242
epoch 81700  training loss: 1.8669037818908691

 28%|██████████████████████                                                          | 82834/300000 [02:53<06:41, 540.99it/s]
epoch 81800  training loss: 1.8667553663253784
epoch 81800  clean testing loss: 11.865386962890625
epoch 81900  training loss: 1.866604208946228
epoch 81900  clean testing loss: 11.866230010986328
epoch 82000  training loss: 1.866456389427185
epoch 82000  clean testing loss: 11.867500305175781
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop0 ...
epoch 82100  training loss: 1.8663127422332764
epoch 82100  clean testing loss: 11.868721008300781
epoch 82200  training loss: 1.8661551475524902
epoch 82200  clean testing loss: 11.870284080505371
epoch 82300  training loss: 1.8660067319869995
epoch 82300  clean testing loss: 11.87154483795166
epoch 82400  training loss: 1.865860939025879
epoch 82400  clean testing loss: 11.872340202331543
epoch 82500  training loss: 1.865703821182251
epoch 82500  clean testing loss: 11.873871803283691
epoch 82600  training loss: 1.8655526638031006
epoch 82600  clean testing loss: 11.87509536743164
epoch 82700  training loss: 1.86540687084198
epoch 82700  clean testing loss: 11.876502990722656
epoch 82800  training loss: 1.8652571439743042

 28%|██████████████████████▎                                                         | 83654/300000 [02:55<06:39, 541.97it/s]
epoch 82900  training loss: 1.8651058673858643
epoch 82900  clean testing loss: 11.878744125366211
epoch 83000  training loss: 1.8649570941925049
epoch 83000  clean testing loss: 11.880056381225586
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop0 ...
epoch 83100  training loss: 1.8648045063018799
epoch 83100  clean testing loss: 11.881174087524414
epoch 83200  training loss: 1.8646551370620728
epoch 83200  clean testing loss: 11.88277816772461
epoch 83300  training loss: 1.864508867263794
epoch 83300  clean testing loss: 11.883975982666016
epoch 83400  training loss: 1.8643567562103271
epoch 83400  clean testing loss: 11.885353088378906
epoch 83500  training loss: 1.8642044067382812
epoch 83500  clean testing loss: 11.886391639709473
epoch 83600  training loss: 1.8640592098236084
epoch 83600  clean testing loss: 11.887591361999512
epoch 83700  training loss: 1.8639092445373535
epoch 83700  clean testing loss: 11.888848304748535
epoch 83800  training loss: 1.8637598752975464
epoch 83800  clean testing loss: 11.890219688415527
epoch 83900  training loss: 1.863613486289978

 28%|██████████████████████▌                                                         | 84753/300000 [02:57<06:35, 543.86it/s]
epoch 84000  training loss: 1.863459587097168
epoch 84000  clean testing loss: 11.892348289489746
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop0 ...
epoch 84100  training loss: 1.8633311986923218
epoch 84100  clean testing loss: 11.893411636352539
epoch 84200  training loss: 1.8632173538208008
epoch 84200  clean testing loss: 11.894203186035156
epoch 84300  training loss: 1.8630975484848022
epoch 84300  clean testing loss: 11.895364761352539
epoch 84400  training loss: 1.86297607421875
epoch 84400  clean testing loss: 11.89647388458252
epoch 84500  training loss: 1.8628525733947754
epoch 84500  clean testing loss: 11.897120475769043
epoch 84600  training loss: 1.862734317779541
epoch 84600  clean testing loss: 11.898452758789062
epoch 84700  training loss: 1.8626092672348022
epoch 84700  clean testing loss: 11.899325370788574
epoch 84800  training loss: 1.8624907732009888
epoch 84800  clean testing loss: 11.900106430053711
epoch 84900  training loss: 1.8623658418655396
epoch 84900  clean testing loss: 11.901147842407227
epoch 85000  training loss: 1.862241268157959
epoch 85000  clean testing loss: 11.902169227600098

 29%|██████████████████████▉                                                         | 85853/300000 [02:59<06:34, 543.40it/s]
epoch 85100  training loss: 1.862116813659668
epoch 85100  clean testing loss: 11.903419494628906
epoch 85200  training loss: 1.8619866371154785
epoch 85200  clean testing loss: 11.904242515563965
epoch 85300  training loss: 1.8618617057800293
epoch 85300  clean testing loss: 11.905354499816895
epoch 85400  training loss: 1.8617331981658936
epoch 85400  clean testing loss: 11.906170845031738
epoch 85500  training loss: 1.8616057634353638
epoch 85500  clean testing loss: 11.907421112060547
epoch 85600  training loss: 1.861478328704834
epoch 85600  clean testing loss: 11.908336639404297
epoch 85700  training loss: 1.8613492250442505
epoch 85700  clean testing loss: 11.909214973449707
epoch 85800  training loss: 1.8612271547317505
epoch 85800  clean testing loss: 11.909873962402344
epoch 85900  training loss: 1.861100435256958
epoch 85900  clean testing loss: 11.911181449890137
epoch 86000  training loss: 1.8609697818756104
epoch 86000  clean testing loss: 11.911916732788086
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop0 ...
epoch 86100  training loss: 1.8608406782150269

 29%|███████████████████████▏                                                        | 86898/300000 [03:01<06:32, 543.44it/s]
epoch 86200  training loss: 1.8607114553451538
epoch 86200  clean testing loss: 11.913822174072266
epoch 86300  training loss: 1.8605868816375732
epoch 86300  clean testing loss: 11.914936065673828
epoch 86400  training loss: 1.8604614734649658
epoch 86400  clean testing loss: 11.91610050201416
epoch 86500  training loss: 1.8603280782699585
epoch 86500  clean testing loss: 11.916996955871582
epoch 86600  training loss: 1.860199213027954
epoch 86600  clean testing loss: 11.917556762695312
epoch 86700  training loss: 1.8600703477859497
epoch 86700  clean testing loss: 11.918856620788574
epoch 86800  training loss: 1.8599406480789185
epoch 86800  clean testing loss: 11.919817924499512
epoch 86900  training loss: 1.8598129749298096
epoch 86900  clean testing loss: 11.920891761779785
epoch 87000  training loss: 1.859687328338623
epoch 87000  clean testing loss: 11.921899795532227
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop0 ...
epoch 87100  training loss: 1.859580397605896
epoch 87100  clean testing loss: 11.922953605651855
epoch 87200  training loss: 1.8594733476638794

 29%|███████████████████████▍                                                        | 87998/300000 [03:03<06:29, 544.10it/s]
epoch 87300  training loss: 1.8593690395355225
epoch 87300  clean testing loss: 11.92470932006836
epoch 87400  training loss: 1.8592628240585327
epoch 87400  clean testing loss: 11.925517082214355
epoch 87500  training loss: 1.8591604232788086
epoch 87500  clean testing loss: 11.926531791687012
epoch 87600  training loss: 1.8590574264526367
epoch 87600  clean testing loss: 11.927184104919434
epoch 87700  training loss: 1.8589556217193604
epoch 87700  clean testing loss: 11.928206443786621
epoch 87800  training loss: 1.8588436841964722
epoch 87800  clean testing loss: 11.92893123626709
epoch 87900  training loss: 1.8587366342544556
epoch 87900  clean testing loss: 11.929883003234863
epoch 88000  training loss: 1.8586331605911255
epoch 88000  clean testing loss: 11.930806159973145
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop0 ...
epoch 88100  training loss: 1.8585255146026611
epoch 88100  clean testing loss: 11.931807518005371
epoch 88200  training loss: 1.8584197759628296
epoch 88200  clean testing loss: 11.932793617248535
epoch 88300  training loss: 1.858317494392395

 30%|███████████████████████▊                                                        | 89093/300000 [03:05<06:32, 537.90it/s]
epoch 88400  training loss: 1.858209252357483
epoch 88400  clean testing loss: 11.93466854095459
epoch 88500  training loss: 1.8581026792526245
epoch 88500  clean testing loss: 11.935792922973633
epoch 88600  training loss: 1.8579946756362915
epoch 88600  clean testing loss: 11.936633110046387
epoch 88700  training loss: 1.8578773736953735
epoch 88700  clean testing loss: 11.93770980834961
epoch 88800  training loss: 1.8577594757080078
epoch 88800  clean testing loss: 11.938672065734863
epoch 88900  training loss: 1.8576390743255615
epoch 88900  clean testing loss: 11.939523696899414
epoch 89000  training loss: 1.8575220108032227
epoch 89000  clean testing loss: 11.940635681152344
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop0 ...
epoch 89100  training loss: 1.8574098348617554
epoch 89100  clean testing loss: 11.941740989685059
epoch 89200  training loss: 1.8572967052459717
epoch 89200  clean testing loss: 11.942662239074707
epoch 89300  training loss: 1.8571839332580566

 30%|████████████████████████                                                        | 90193/300000 [03:07<06:26, 543.42it/s]
epoch 89400  training loss: 1.857068419456482
epoch 89400  clean testing loss: 11.944439888000488
epoch 89500  training loss: 1.8569574356079102
epoch 89500  clean testing loss: 11.94564151763916
epoch 89600  training loss: 1.8568466901779175
epoch 89600  clean testing loss: 11.94651985168457
epoch 89700  training loss: 1.8567334413528442
epoch 89700  clean testing loss: 11.94731330871582
epoch 89800  training loss: 1.8566229343414307
epoch 89800  clean testing loss: 11.948395729064941
epoch 89900  training loss: 1.8565119504928589
epoch 89900  clean testing loss: 11.949110984802246
epoch 90000  training loss: 1.856398105621338
epoch 90000  clean testing loss: 11.950215339660645
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop0 ...
epoch 90100  training loss: 1.8563047647476196
epoch 90100  clean testing loss: 11.950957298278809
epoch 90200  training loss: 1.8562220335006714

 30%|████████████████████████▎                                                       | 91293/300000 [03:09<06:23, 543.54it/s]
epoch 90300  training loss: 1.8561323881149292
epoch 90300  clean testing loss: 11.952438354492188
epoch 90400  training loss: 1.856046199798584
epoch 90400  clean testing loss: 11.9531831741333
epoch 90500  training loss: 1.855954647064209
epoch 90500  clean testing loss: 11.953790664672852
epoch 90600  training loss: 1.8558635711669922
epoch 90600  clean testing loss: 11.954721450805664
epoch 90700  training loss: 1.8557761907577515
epoch 90700  clean testing loss: 11.955254554748535
epoch 90800  training loss: 1.8556885719299316
epoch 90800  clean testing loss: 11.956060409545898
epoch 90900  training loss: 1.855599045753479
epoch 90900  clean testing loss: 11.956975936889648
epoch 91000  training loss: 1.85551118850708
epoch 91000  clean testing loss: 11.957383155822754
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop0 ...
epoch 91100  training loss: 1.8554236888885498
epoch 91100  clean testing loss: 11.958292007446289
epoch 91200  training loss: 1.8553338050842285
epoch 91200  clean testing loss: 11.95897388458252
epoch 91300  training loss: 1.855248212814331

 31%|████████████████████████▌                                                       | 92337/300000 [03:11<06:30, 531.85it/s]
epoch 91400  training loss: 1.855155348777771
epoch 91400  clean testing loss: 11.960453033447266
epoch 91500  training loss: 1.8550705909729004
epoch 91500  clean testing loss: 11.961175918579102
epoch 91600  training loss: 1.8549822568893433
epoch 91600  clean testing loss: 11.96202564239502
epoch 91700  training loss: 1.854889154434204
epoch 91700  clean testing loss: 11.962754249572754
epoch 91800  training loss: 1.8547998666763306
epoch 91800  clean testing loss: 11.963400840759277
epoch 91900  training loss: 1.854713797569275
epoch 91900  clean testing loss: 11.96413803100586
epoch 92000  training loss: 1.8546234369277954
epoch 92000  clean testing loss: 11.964838027954102
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop0 ...
epoch 92100  training loss: 1.8545336723327637
epoch 92100  clean testing loss: 11.96572494506836
epoch 92200  training loss: 1.854446530342102
epoch 92200  clean testing loss: 11.966283798217773
epoch 92300  training loss: 1.8543546199798584

 31%|████████████████████████▉                                                       | 93423/300000 [03:13<06:26, 534.89it/s]
epoch 92400  training loss: 1.8542706966400146
epoch 92400  clean testing loss: 11.967832565307617
epoch 92500  training loss: 1.8541779518127441
epoch 92500  clean testing loss: 11.96875
epoch 92600  training loss: 1.8540894985198975
epoch 92600  clean testing loss: 11.969748497009277
epoch 92700  training loss: 1.8539996147155762
epoch 92700  clean testing loss: 11.970219612121582
epoch 92800  training loss: 1.8539124727249146
epoch 92800  clean testing loss: 11.971062660217285
epoch 92900  training loss: 1.8538223505020142
epoch 92900  clean testing loss: 11.97172737121582
epoch 93000  training loss: 1.8537330627441406
epoch 93000  clean testing loss: 11.972404479980469
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop0 ...
epoch 93100  training loss: 1.853659987449646
epoch 93100  clean testing loss: 11.973097801208496
epoch 93200  training loss: 1.8535876274108887
epoch 93200  clean testing loss: 11.97373104095459
epoch 93300  training loss: 1.8535130023956299
epoch 93300  clean testing loss: 11.974529266357422
epoch 93400  training loss: 1.8534386157989502

 31%|█████████████████████████                                                       | 94071/300000 [03:14<06:28, 530.56it/s]
epoch 93500  training loss: 1.853367805480957
epoch 93500  clean testing loss: 11.975935935974121
epoch 93600  training loss: 1.8532911539077759
epoch 93600  clean testing loss: 11.976469039916992
epoch 93700  training loss: 1.8532215356826782
epoch 93700  clean testing loss: 11.977222442626953
epoch 93800  training loss: 1.8531460762023926
epoch 93800  clean testing loss: 11.977801322937012
epoch 93900  training loss: 1.85306978225708
epoch 93900  clean testing loss: 11.978519439697266
epoch 94000  training loss: 1.8529987335205078
epoch 94000  clean testing loss: 11.979254722595215
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop0 ...
epoch 94100  training loss: 1.8529226779937744

 32%|█████████████████████████▌                                                      | 95689/300000 [03:19<06:51, 496.95it/s]
epoch 94200  training loss: 1.8528486490249634
epoch 94200  clean testing loss: 11.980408668518066
epoch 94300  training loss: 1.8527734279632568
epoch 94300  clean testing loss: 11.981281280517578
epoch 94400  training loss: 1.8526999950408936
epoch 94400  clean testing loss: 11.981792449951172
epoch 94500  training loss: 1.8526250123977661
epoch 94500  clean testing loss: 11.98261833190918
epoch 94600  training loss: 1.8525514602661133
epoch 94600  clean testing loss: 11.983267784118652
epoch 94700  training loss: 1.8524787425994873
epoch 94700  clean testing loss: 11.983938217163086
epoch 94800  training loss: 1.8524038791656494
epoch 94800  clean testing loss: 11.984548568725586
epoch 94900  training loss: 1.8523304462432861
epoch 94900  clean testing loss: 11.98524284362793
epoch 95000  training loss: 1.852253794670105
epoch 95000  clean testing loss: 11.98588752746582
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop0 ...
epoch 95100  training loss: 1.8521820306777954
epoch 95100  clean testing loss: 11.986608505249023
epoch 95200  training loss: 1.8521028757095337
epoch 95200  clean testing loss: 11.98727798461914
epoch 95300  training loss: 1.8520302772521973
epoch 95300  clean testing loss: 11.98793888092041
epoch 95400  training loss: 1.8519580364227295
epoch 95400  clean testing loss: 11.98873519897461
epoch 95500  training loss: 1.8518824577331543
epoch 95500  clean testing loss: 11.98918342590332
epoch 95600  training loss: 1.8518062829971313
epoch 95600  clean testing loss: 11.990044593811035
epoch 95700  training loss: 1.8517347574234009

 32%|█████████████████████████▊                                                      | 96778/300000 [03:21<06:16, 540.35it/s]
epoch 95800  training loss: 1.8516626358032227
epoch 95800  clean testing loss: 11.991287231445312
epoch 95900  training loss: 1.8515836000442505
epoch 95900  clean testing loss: 11.992144584655762
epoch 96000  training loss: 1.8515123128890991
epoch 96000  clean testing loss: 11.992704391479492
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop0 ...
epoch 96100  training loss: 1.851447582244873
epoch 96100  clean testing loss: 11.99314022064209
epoch 96200  training loss: 1.8513944149017334
epoch 96200  clean testing loss: 11.99375057220459
epoch 96300  training loss: 1.851337194442749
epoch 96300  clean testing loss: 11.994140625
epoch 96400  training loss: 1.851280927658081
epoch 96400  clean testing loss: 11.994775772094727
epoch 96500  training loss: 1.8512253761291504
epoch 96500  clean testing loss: 11.995173454284668
epoch 96600  training loss: 1.851165771484375
epoch 96600  clean testing loss: 11.995705604553223
epoch 96700  training loss: 1.8511059284210205
epoch 96700  clean testing loss: 11.996237754821777
epoch 96800  training loss: 1.8510507345199585

 33%|██████████████████████████                                                      | 97875/300000 [03:23<06:12, 542.36it/s]
epoch 96900  training loss: 1.8509905338287354
epoch 96900  clean testing loss: 11.997330665588379
epoch 97000  training loss: 1.850934386253357
epoch 97000  clean testing loss: 11.997673988342285
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop0 ...
epoch 97100  training loss: 1.8508789539337158
epoch 97100  clean testing loss: 11.998327255249023
epoch 97200  training loss: 1.8508189916610718
epoch 97200  clean testing loss: 11.998682022094727
epoch 97300  training loss: 1.8507630825042725
epoch 97300  clean testing loss: 11.999153137207031
epoch 97400  training loss: 1.8507000207901
epoch 97400  clean testing loss: 11.999683380126953
epoch 97500  training loss: 1.8506453037261963
epoch 97500  clean testing loss: 12.0001859664917
epoch 97600  training loss: 1.8505833148956299
epoch 97600  clean testing loss: 12.0006742477417
epoch 97700  training loss: 1.8505271673202515
epoch 97700  clean testing loss: 12.00119400024414
epoch 97800  training loss: 1.8504693508148193

 33%|██████████████████████████▍                                                     | 98970/300000 [03:25<06:11, 540.94it/s]
epoch 97900  training loss: 1.8504127264022827
epoch 97900  clean testing loss: 12.002202033996582
epoch 98000  training loss: 1.8503572940826416
epoch 98000  clean testing loss: 12.002723693847656
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop0 ...
epoch 98100  training loss: 1.850297212600708
epoch 98100  clean testing loss: 12.003311157226562
epoch 98200  training loss: 1.850241780281067
epoch 98200  clean testing loss: 12.003705978393555
epoch 98300  training loss: 1.8501818180084229
epoch 98300  clean testing loss: 12.00426959991455
epoch 98400  training loss: 1.8501250743865967
epoch 98400  clean testing loss: 12.004852294921875
epoch 98500  training loss: 1.8500633239746094
epoch 98500  clean testing loss: 12.005210876464844
epoch 98600  training loss: 1.8500093221664429
epoch 98600  clean testing loss: 12.005778312683105
epoch 98700  training loss: 1.8499499559402466
epoch 98700  clean testing loss: 12.006183624267578
epoch 98800  training loss: 1.8498916625976562
epoch 98800  clean testing loss: 12.00680160522461
epoch 98900  training loss: 1.8498340845108032

 33%|██████████████████████████▌                                                     | 99700/300000 [03:26<06:54, 482.66it/s]
epoch 99000  training loss: 1.849777340888977
epoch 99000  clean testing loss: 12.007683753967285
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop0 ...
epoch 99100  training loss: 1.849727749824524
epoch 99100  clean testing loss: 12.008035659790039
epoch 99200  training loss: 1.8496806621551514
epoch 99200  clean testing loss: 12.00853157043457
epoch 99300  training loss: 1.8496326208114624
epoch 99300  clean testing loss: 12.008840560913086
epoch 99400  training loss: 1.8495848178863525
epoch 99400  clean testing loss: 12.00937557220459
epoch 99500  training loss: 1.8495396375656128
epoch 99500  clean testing loss: 12.009860038757324
epoch 99600  training loss: 1.8494927883148193
epoch 99600  clean testing loss: 12.010238647460938
epoch 99700  training loss: 1.849442720413208
epoch 99700  clean testing loss: 12.010713577270508
Validation loss variation < 1e-6, trained to interpolation, stop
Saving the clean test loss file into log/new_plot/output_dim2_width512_layers3_relu_size500_noise1.00e+01_invop0 ...