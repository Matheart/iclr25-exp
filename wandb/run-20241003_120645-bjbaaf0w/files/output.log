
  1%|▊                                                                                                                      | 724/100000 [00:01<03:00, 548.62it/s]
epoch 0  training loss: inf
epoch 0  clean testing loss: nan
epoch 100  training loss: nan
epoch 100  clean testing loss: nan
epoch 200  training loss: nan
epoch 200  clean testing loss: nan
epoch 300  training loss: nan
epoch 300  clean testing loss: nan
epoch 400  training loss: nan
epoch 400  clean testing loss: nan
epoch 500  training loss: nan
epoch 500  clean testing loss: nan
epoch 600  training loss: nan
epoch 600  clean testing loss: nan
epoch 700  training loss: nan
epoch 700  clean testing loss: nan
epoch 800  training loss: nan

  2%|██▏                                                                                                                   | 1843/100000 [00:03<02:56, 554.68it/s]
epoch 900  training loss: nan
epoch 900  clean testing loss: nan
epoch 1000  training loss: nan
epoch 1000  clean testing loss: nan
epoch 1100  training loss: nan
epoch 1100  clean testing loss: nan
epoch 1200  training loss: nan
epoch 1200  clean testing loss: nan
epoch 1300  training loss: nan
epoch 1300  clean testing loss: nan
epoch 1400  training loss: nan
epoch 1400  clean testing loss: nan
epoch 1500  training loss: nan
epoch 1500  clean testing loss: nan
epoch 1600  training loss: nan
epoch 1600  clean testing loss: nan
epoch 1700  training loss: nan
epoch 1700  clean testing loss: nan
epoch 1800  training loss: nan
epoch 1800  clean testing loss: nan
epoch 1900  training loss: nan

  3%|███▍                                                                                                                  | 2963/100000 [00:05<02:55, 552.78it/s]
epoch 2000  training loss: nan
epoch 2000  clean testing loss: nan
epoch 2100  training loss: nan
epoch 2100  clean testing loss: nan
epoch 2200  training loss: nan
epoch 2200  clean testing loss: nan
epoch 2300  training loss: nan
epoch 2300  clean testing loss: nan
epoch 2400  training loss: nan
epoch 2400  clean testing loss: nan
epoch 2500  training loss: nan
epoch 2500  clean testing loss: nan
epoch 2600  training loss: nan
epoch 2600  clean testing loss: nan
epoch 2700  training loss: nan
epoch 2700  clean testing loss: nan
epoch 2800  training loss: nan
epoch 2800  clean testing loss: nan
epoch 2900  training loss: nan
epoch 2900  clean testing loss: nan
epoch 3000  training loss: nan
  3%|███▉                                                                                                                  | 3333/100000 [00:06<02:58, 541.24it/s]
Traceback (most recent call last):
  File "/home/howon/aistats25-exp/nn_exp.py", line 242, in <module>
    losses.backward()
  File "/home/howon/.conda/envs/cs552/lib/python3.10/site-packages/torch/_tensor.py", line 525, in backward
    torch.autograd.backward(
  File "/home/howon/.conda/envs/cs552/lib/python3.10/site-packages/torch/autograd/__init__.py", line 267, in backward
    _engine_run_backward(
  File "/home/howon/.conda/envs/cs552/lib/python3.10/site-packages/torch/autograd/graph.py", line 744, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt
epoch 3100  training loss: nan
epoch 3100  clean testing loss: nan
epoch 3200  training loss: nan
epoch 3200  clean testing loss: nan
epoch 3300  training loss: nan
epoch 3300  clean testing loss: nan